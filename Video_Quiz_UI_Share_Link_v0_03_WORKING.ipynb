{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxTPUgaDDB+rmye8BV0TaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donkeytonk/DIB-R/blob/master/Video_Quiz_UI_Share_Link_v0_03_WORKING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.03 NOTES\n",
        "-- Fixed the issue related to difficult questions being too long (too long not suitable for videos)\n"
      ],
      "metadata": {
        "id": "o-vsu_NZs5Bj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW3gdQglzInu",
        "outputId": "c947e28a-6d68-4bf8-c7fd-c101928867e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e363bfe9a919d53253.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e363bfe9a919d53253.gradio.live\n"
          ]
        }
      ],
      "source": [
        "# === ONE-CELL: MyAI101 Quiz Video Maker (1080x1920, Gemini-powered via REST, No MoviePy) ===\n",
        "# - Vertical HD (1080x1920 @ 24fps), ffmpeg only (threads=1)\n",
        "# - Gradio UI: Topic, Difficulty(1-10), Count(1-10), Gemini API Key, Model\n",
        "# - Uses Gemini **v1 REST** (no SDK). Button to list models your key actually has.\n",
        "# - Randomizes correct answer placement; editable preview table\n",
        "# - Renders MP4s to /content/out/videos; prints Local/Public URLs\n",
        "\n",
        "import os, sys, subprocess, random, string, time, gc, warnings, re, json\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Headless safety ---\n",
        "os.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"\n",
        "os.environ[\"XDG_RUNTIME_DIR\"]  = \"/tmp/runtime\"\n",
        "os.makedirs(\"/tmp/runtime\", exist_ok=True)\n",
        "\n",
        "# --- Deps ---\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                       \"pillow>=10.0.0\", \"gradio>=4.0.0\", \"pandas>=1.3\", \"requests>=2.31\"])\n",
        "subprocess.call([\"apt-get\", \"-y\", \"install\", \"-qq\",\n",
        "                 \"ffmpeg\", \"fonts-dejavu-core\", \"fonts-liberation\"])\n",
        "\n",
        "\n",
        "import requests\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# ---------- RENDER SETTINGS ----------\n",
        "W, H, FPS = 1080, 1920, 24\n",
        "BITRATE = \"3500k\"\n",
        "\n",
        "GREEN   = (16, 163, 74)\n",
        "MINT    = (209, 250, 229)\n",
        "DARK    = (15, 23, 42)\n",
        "MID     = (51, 65, 85)\n",
        "BG      = (255, 255, 255)\n",
        "CHIP_BG = (243, 244, 246)\n",
        "\n",
        "SAFE_X   = int(W * 0.09)\n",
        "SAFE_TOP = int(H * 0.08)\n",
        "SAFE_BOT = int(H * 0.10)\n",
        "\n",
        "TITLE_BOX_H         = int(H * 0.36)\n",
        "GAP_BELOW_TITLE_MIN = int(H * 0.10)\n",
        "\n",
        "CHIP_H      = 108\n",
        "CHIP_TX_H   = 80\n",
        "CHIP_TX_PAD = 90\n",
        "CHIP_GAP    = int(H * 0.085)\n",
        "\n",
        "CTA_BUTTON_W = 640\n",
        "CTA_BUTTON_H = 120\n",
        "\n",
        "# ------- Readability budgets (approx. 6s Q, 4.3s reveal) -------\n",
        "Q_MAX = 130        # question max chars\n",
        "OPT_MAX = 38       # per option max chars\n",
        "EXP_MAX = 110      # explanation max chars\n",
        "\n",
        "# Flags we’ll append if we had to clip (helps you spot them in the table)\n",
        "CLIP_FLAG = \" ·clipped\"\n",
        "\n",
        "# Output markers to bound JSON\n",
        "BEGIN_JSON = \"<<<JSON>>>\"\n",
        "END_JSON   = \"<<<END>>>\"\n",
        "\n",
        "import re as _re\n",
        "\n",
        "# Light, safe compaction for common bloat\n",
        "_FILLER = [\n",
        "    r\"\\bthat\\b\", r\"\\bvery\\b\", r\"\\bactually\\b\", r\"\\breally\\b\",\n",
        "    r\"\\bjust\\b\", r\"\\bkind of\\b\", r\"\\bsort of\\b\", r\"\\bin order to\\b\",\n",
        "]\n",
        "\n",
        "def _squash_spaces(s: str) -> str:\n",
        "    return _re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
        "\n",
        "def _kill_filler(s: str) -> str:\n",
        "    for pat in _FILLER:\n",
        "        s = _re.sub(pat, \"\", s, flags=_re.IGNORECASE)\n",
        "    return _squash_spaces(s)\n",
        "\n",
        "def _tighten_punct(s: str) -> str:\n",
        "    if not s: return s\n",
        "    s = _re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)        # no space before punctuation\n",
        "    s = _re.sub(r\"\\(([^)]{0,20})\\)\", r\"\\1\", s)     # drop short parentheticals\n",
        "    s = _re.sub(r\"--+\", \"–\", s)                    # normalize dashes\n",
        "    return _squash_spaces(s)\n",
        "\n",
        "def _smart_clip(s: str, n: int, add_ellipsis: bool = True) -> Tuple[str, bool]:\n",
        "    \"\"\"Clip at a natural boundary ≤ n; return (text, clipped?).\"\"\"\n",
        "    s = (s or \"\").strip()\n",
        "    if len(s) <= n:\n",
        "        return s, False\n",
        "    cut = s[:n+1]\n",
        "    m = _re.search(r\"[.;:!?]\\s+\\S*$\", cut)\n",
        "    if m:\n",
        "        out = cut[:m.start()].rstrip()\n",
        "        return (out + \"…\", True) if add_ellipsis else (out, True)\n",
        "    m = _re.search(r\"\\s+\\S*$\", cut)\n",
        "    if m:\n",
        "        out = cut[:m.start()].rstrip()\n",
        "        return (out + \"…\", True) if add_ellipsis else (out, True)\n",
        "    out = s[:n].rstrip()\n",
        "    return (out + \"…\", True) if add_ellipsis else (out, True)\n",
        "\n",
        "def compact_to(s: str, n: int, add_ellipsis: bool = True, *, prune_mode: str = \"generic\") -> Tuple[str, bool]:\n",
        "    \"\"\"\n",
        "    Heuristic squeeze -> abbreviate -> prune clauses (mode) -> (last resort) clip.\n",
        "    prune_mode: \"generic\" | \"question\" | \"none\"\n",
        "    \"\"\"\n",
        "    before = s\n",
        "    t = _squash_spaces(s)\n",
        "    t = _tighten_punct(t)\n",
        "    t = _kill_filler(t)\n",
        "    if len(t) <= n:\n",
        "        return t, len(t) != len(before)\n",
        "\n",
        "    t2 = _abbr_pass(t)\n",
        "    if len(t2) <= n:\n",
        "        return t2, True\n",
        "\n",
        "    # choose pruner\n",
        "    if prune_mode == \"question\":\n",
        "        t3 = _prune_clauses_question(t2)\n",
        "    elif prune_mode == \"generic\":\n",
        "        t3 = _prune_clauses(t2)\n",
        "    else:\n",
        "        t3 = t2  # no clause pruning\n",
        "\n",
        "    if len(t3) <= n:\n",
        "        return t3, True\n",
        "\n",
        "    # last resort: natural clip\n",
        "    return _smart_clip(t3, n, add_ellipsis=add_ellipsis)\n",
        "\n",
        "\n",
        "def lint_item_lengths(q: str, options: List[str], exp: str, *, add_ellipsis: bool = False) -> Tuple[str, List[str], str, dict]:\n",
        "    flags = {\"Q\": False, \"A\": False, \"B\": False, \"C\": False, \"D\": False, \"EXP\": False}\n",
        "\n",
        "    # keep original for salvage\n",
        "    q_raw = q or \"\"\n",
        "\n",
        "    # Strip verbose stems\n",
        "    q = _re.sub(r\"^\\s*(Which of the following|What of the following|Which statement).*?:\\s*\", \"\", q_raw, flags=_re.IGNORECASE)\n",
        "    # (remove the old 'In <word>,' stripper — it caused harm in some cases)\n",
        "\n",
        "    # compact with question-safe pruning\n",
        "    q1, qc = compact_to(q, Q_MAX, add_ellipsis=add_ellipsis, prune_mode=\"question\")\n",
        "    flags[\"Q\"] = qc\n",
        "    q1 = _finish_sentence(q1, is_question=True)\n",
        "\n",
        "    # SALVAGE: if pruning produced a stubby opener, recompute without comma-prune\n",
        "    looks_stub = (len(q1) < 24) and _re.match(r\"^(in|when|while|during|where)\\b\", q1.strip().lower())\n",
        "    if looks_stub:\n",
        "        q2, _ = compact_to(q_raw, Q_MAX, add_ellipsis=add_ellipsis, prune_mode=\"none\")\n",
        "        q2 = _finish_sentence(q2, is_question=True)\n",
        "        # prefer the longer, clearer question if it fits budget\n",
        "        if len(q2) >= len(q1):\n",
        "            q1 = q2\n",
        "\n",
        "    q = q1\n",
        "\n",
        "    outs, letters = [], [\"A\",\"B\",\"C\",\"D\"]\n",
        "    for i, o in enumerate((options + [\"\"]*4)[:4]):\n",
        "        o2, oc = compact_to(o or \"\", OPT_MAX, add_ellipsis=add_ellipsis, prune_mode=\"generic\")\n",
        "        outs.append(o2); flags[letters[i]] = oc\n",
        "\n",
        "    exp1, ec = compact_to(exp or \"\", EXP_MAX, add_ellipsis=add_ellipsis, prune_mode=\"generic\")\n",
        "    flags[\"EXP\"] = ec\n",
        "    exp = _finish_sentence(exp1, is_question=False)\n",
        "\n",
        "    return q, outs, exp, flags\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------- JSON-ish cleanup + robust extraction ---------\n",
        "def _strip_json_comments(s: str) -> str:\n",
        "    s = re.sub(r\"(^|\\s)//.*?$\", r\"\\1\", s, flags=re.MULTILINE)\n",
        "    s = re.sub(r\"/\\*.*?\\*/\", \"\", s, flags=re.DOTALL)\n",
        "    return s\n",
        "\n",
        "def _normalize_quotes(s: str) -> str:\n",
        "    return (s or \"\").replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"').replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\")\n",
        "\n",
        "def _remove_trailing_commas(s: str) -> str:\n",
        "    s = re.sub(r\",\\s*([}\\]])\", r\"\\1\", s)\n",
        "    return s\n",
        "\n",
        "def _clean_jsonish(s: str) -> str:\n",
        "    s = _strip_code_fences(s)\n",
        "    s = _normalize_quotes(s)\n",
        "    s = _strip_json_comments(s)\n",
        "    s = _remove_trailing_commas(s)\n",
        "    return s.strip()\n",
        "\n",
        "def _extract_between_markers(s: str):\n",
        "    i = s.find(BEGIN_JSON); j = s.rfind(END_JSON)\n",
        "    if i != -1 and j != -1 and j > i:\n",
        "        return s[i+len(BEGIN_JSON):j]\n",
        "    return None\n",
        "\n",
        "def _split_objects_by_brace(text: str) -> list:\n",
        "    body, objs, depth, in_str, esc, start = text, [], 0, False, False, None\n",
        "    for idx, ch in enumerate(body):\n",
        "        if in_str:\n",
        "            if esc: esc = False\n",
        "            elif ch == \"\\\\\": esc = True\n",
        "            elif ch == '\"': in_str = False\n",
        "        else:\n",
        "            if ch == '\"': in_str = True\n",
        "            elif ch == \"{\":\n",
        "                if depth == 0: start = idx\n",
        "                depth += 1\n",
        "            elif ch == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0 and start is not None:\n",
        "                    objs.append(body[start:idx+1]); start = None\n",
        "    return objs\n",
        "\n",
        "def _first_json_dict(s: str):\n",
        "    s = _clean_jsonish(s)\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        # Try coercing if it looks like bare key/value lines\n",
        "        s2 = _coerce_objectish(s)\n",
        "        if s2 != s:\n",
        "            try:\n",
        "                return json.loads(_clean_jsonish(s2))\n",
        "            except Exception:\n",
        "                pass\n",
        "        if \"{\" in s and \"}\" in s:\n",
        "            chunk = s[s.find(\"{\"): s.rfind(\"}\")+1]\n",
        "            try:\n",
        "                return json.loads(_clean_jsonish(chunk))\n",
        "            except Exception:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_questions_from_model(s: str) -> list:\n",
        "    \"\"\"\n",
        "    Robustly recover the list of question dicts from many shapes:\n",
        "    - Proper object with \"questions\"\n",
        "    - Object with keys like \"items\"/\"mcqs\"/\"data\"/\"list\"\n",
        "    - Bare array [...]\n",
        "    - JSON-ish text with adjacent {} blocks and missing commas\n",
        "    - Prefer content between BEGIN/END markers if present\n",
        "    \"\"\"\n",
        "    between = _extract_between_markers(s)\n",
        "    if between is not None:\n",
        "      s = between\n",
        "\n",
        "    # NEW: if the slice looks like key/value lines without braces, wrap it\n",
        "    s = _coerce_objectish(s)\n",
        "\n",
        "    s_clean = _clean_jsonish(s)\n",
        "\n",
        "\n",
        "\n",
        "    # 1) whole-doc parse\n",
        "    data = _first_json_dict(s_clean)\n",
        "    if isinstance(data, dict):\n",
        "        if isinstance(data.get(\"questions\"), list):\n",
        "            return data[\"questions\"]\n",
        "        for k in (\"items\", \"mcqs\", \"data\", \"list\", \"entries\"):\n",
        "            v = data.get(k)\n",
        "            if isinstance(v, list):\n",
        "                return v\n",
        "    try:\n",
        "        arr = json.loads(s_clean)\n",
        "        if isinstance(arr, list):\n",
        "            return arr\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) last resort: split all objects and pick MCQ-like ones\n",
        "    objs = _split_objects_by_brace(s_clean)\n",
        "    out = []\n",
        "    for obj in objs:\n",
        "        try:\n",
        "            d = json.loads(_clean_jsonish(obj))\n",
        "        except Exception:\n",
        "            try:\n",
        "                d = json.loads(_clean_jsonish(re.sub(r\"'\", '\"', obj)))\n",
        "            except Exception:\n",
        "                continue\n",
        "        if not isinstance(d, dict):\n",
        "            continue\n",
        "        if \"question\" in d and (\"correct_answer\" in d or \"answer\" in d or \"correct\" in d or \"options\" in d):\n",
        "            if \"correct_answer\" not in d:\n",
        "                if \"answer\" in d: d[\"correct_answer\"] = d[\"answer\"]\n",
        "                elif \"correct\" in d: d[\"correct_answer\"] = d[\"correct\"]\n",
        "                elif \"options\" in d and isinstance(d.get(\"correct_index\"), int):\n",
        "                    opts = d.get(\"options\") or []\n",
        "                    ci = d.get(\"correct_index\")\n",
        "                    if 0 <= ci < len(opts):\n",
        "                        d[\"correct_answer\"] = opts[ci]\n",
        "                        d[\"distractors\"] = [o for i, o in enumerate(opts) if i != ci][:3]\n",
        "            out.append(d)\n",
        "    return out\n",
        "\n",
        "# ---------- Font + text helpers ----------\n",
        "# Robust font resolver so we never fall back to the tiny bitmap font\n",
        "FONT_CANDIDATES = [\n",
        "    \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\",\n",
        "    \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
        "    \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\",\n",
        "    \"/usr/share/fonts/truetype/liberation2/LiberationSans-Bold.ttf\",\n",
        "    \"/usr/share/fonts/truetype/freefont/FreeSansBold.ttf\",\n",
        "]\n",
        "\n",
        "def _resolve_font_path() -> str:\n",
        "    for p in FONT_CANDIDATES:\n",
        "        if Path(p).exists():\n",
        "            return p\n",
        "    try:\n",
        "        for p in Path(\"/usr/share/fonts\").rglob(\"*.ttf\"):\n",
        "            return str(p)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"  # will trigger bitmap fallback\n",
        "\n",
        "DEFAULT_FONT_PATH = _resolve_font_path()\n",
        "DEFAULT_FONT_EXISTS = bool(DEFAULT_FONT_PATH)\n",
        "\n",
        "OUT_DIR = Path(\"/content/out/videos\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TMP_DIR = Path(\"/content/out/tmp\"); TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _font(pt: int) -> ImageFont.FreeTypeFont:\n",
        "    if DEFAULT_FONT_EXISTS:\n",
        "        return ImageFont.truetype(DEFAULT_FONT_PATH, pt)\n",
        "    return ImageFont.load_default()\n",
        "\n",
        "def _finish_sentence(t: str, *, is_question: bool) -> str:\n",
        "    t = (t or \"\").strip()\n",
        "    # remove dangling tails like \", which\", \"—\", \"including\", etc.\n",
        "    t = re.sub(r\"(,\\s*(which|that|who|when|where)\\b.*)$\", \"\", t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r\"[\\s:;,\\-–—]+$\", \"\", t)  # trim trailing punctuation fragments\n",
        "    t = re.sub(r\"\\b(such as|including|like|for example|e\\.g\\.)\\s*$\", \"\", t, flags=re.IGNORECASE)\n",
        "    # normalize end mark\n",
        "    if is_question:\n",
        "        t = t.rstrip(\".\")  # don't end a question with '.'\n",
        "        if not t.endswith(\"?\"):\n",
        "            t += \"?\"\n",
        "    else:\n",
        "        if not re.search(r\"[.!?]$\", t):\n",
        "            t += \".\"\n",
        "    return t\n",
        "\n",
        "\n",
        "\n",
        "def _text_wrap(draw: ImageDraw.ImageDraw, text: str, font: ImageFont.FreeTypeFont, max_w: int):\n",
        "    words = text.split()\n",
        "    lines, cur = [], \"\"\n",
        "    for w in words:\n",
        "        t = (cur + \" \" + w).strip()\n",
        "        bbox = draw.textbbox((0,0), t, font=font)\n",
        "        if bbox[2]-bbox[0] <= max_w or not cur:\n",
        "            cur = t\n",
        "        else:\n",
        "            lines.append(cur); cur = w\n",
        "    if cur: lines.append(cur)\n",
        "    if len(lines) >= 2 and len(lines[-1].split()) <= 2 and len(lines[-2].split()) > 2:\n",
        "        prev = lines[-2].split()\n",
        "        moved = prev.pop()\n",
        "        lines[-2] = \" \".join(prev)\n",
        "        lines[-1] = (moved + \" \" + lines[-1]).strip()\n",
        "    return lines\n",
        "\n",
        "def _draw_text_block(img, box, text, color, max_pt, min_pt, leading_ratio=0.30, stroke=0, stroke_color=(255,255,255), align=\"center\"):\n",
        "    x, y, w, h = box\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for pt in range(max_pt, min_pt-1, -2):\n",
        "        f = _font(pt)\n",
        "        lines = _text_wrap(draw, text, f, w)\n",
        "        line_heights = [draw.textbbox((0,0), ln, font=f)[3] - draw.textbbox((0,0), ln, font=f)[1] for ln in lines]\n",
        "        total_h = sum(line_heights) + int(pt * leading_ratio) * (len(lines)-1)\n",
        "        if total_h <= h:\n",
        "            cur_y = y + (h - total_h)//2\n",
        "            for ln in lines:\n",
        "                bbox = draw.textbbox((0,0), ln, font=f)\n",
        "                tw = bbox[2]-bbox[0]; th = bbox[3]-bbox[1]\n",
        "                if align == \"center\":\n",
        "                    tx = x + (w - tw)//2\n",
        "                elif align == \"left\":\n",
        "                    tx = x\n",
        "                else:\n",
        "                    tx = x + (w - tw)\n",
        "                if stroke > 0:\n",
        "                    draw.text((tx, cur_y), ln, font=f, fill=stroke_color, stroke_width=stroke, stroke_fill=stroke_color)\n",
        "                draw.text((tx, cur_y), ln, font=f, fill=color, stroke_width=stroke, stroke_fill=stroke_color)\n",
        "                cur_y += th + int(pt * leading_ratio)\n",
        "            return\n",
        "    f = _font(min_pt)\n",
        "    bbox = draw.textbbox((0,0), text, font=f); tw = bbox[2]-bbox[0]; th = bbox[3]-bbox[1]\n",
        "    tx = x + (w - tw)//2; ty = y + (h - th)//2\n",
        "    draw.text((tx, ty), text, font=f, fill=color, stroke_width=stroke, stroke_fill=stroke_color)\n",
        "\n",
        "# ---------- Slide renderers ----------\n",
        "def _badge(img: Image.Image):\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.rectangle([0,0,W,8], fill=GREEN)\n",
        "    pill_w, pill_h = 300, 72\n",
        "    px, py = W - pill_w - 36, 36\n",
        "    draw.rounded_rectangle([px, py, px+pill_w, py+pill_h], radius=20, fill=MINT)\n",
        "    _draw_text_block(img, (px+22, py+14, pill_w-44, pill_h-28), \"MyAI101\", DARK, 56, 32, align=\"left\")\n",
        "\n",
        "def _chip(img: Image.Image, y_center: int, text: str):\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    chip_w = W - 2*SAFE_X\n",
        "    x0, y0 = (W - chip_w)//2, y_center - CHIP_H//2\n",
        "    draw.rounded_rectangle([x0, y0, x0+chip_w, y0+CHIP_H], radius=18, fill=CHIP_BG)\n",
        "    draw.rectangle([x0, y0, x0+chip_w, y0+4], fill=GREEN)\n",
        "    _draw_text_block(img, (x0+CHIP_TX_PAD//2, y0+(CHIP_H-CHIP_TX_H)//2, chip_w-CHIP_TX_PAD, CHIP_TX_H),\n",
        "                     text, DARK, max_pt=54, min_pt=30, align=\"left\")\n",
        "\n",
        "def _question_slide(question: str, options: List[str]) -> Image.Image:\n",
        "    img = Image.new(\"RGB\", (W, H), BG)\n",
        "    _badge(img)\n",
        "    title_box = (SAFE_X, SAFE_TOP, W-2*SAFE_X, TITLE_BOX_H)\n",
        "    _draw_text_block(img, title_box, question, DARK, 84, 34, stroke=2, stroke_color=(255,255,255))\n",
        "    title_bottom = SAFE_TOP + TITLE_BOX_H\n",
        "    band_top = max(title_bottom + GAP_BELOW_TITLE_MIN, SAFE_TOP + int(H*0.34))\n",
        "    band_bot = min(H - SAFE_BOT, int(H*0.86))\n",
        "    n = len(options)\n",
        "    if n > 0:\n",
        "        band_h = band_bot - band_top\n",
        "        preferred = n*CHIP_H + (n-1)*CHIP_GAP\n",
        "        if preferred <= band_h:\n",
        "            ys = [band_top + CHIP_H//2 + i*(CHIP_H+CHIP_GAP) for i in range(n)]\n",
        "        else:\n",
        "            gap = max(18, int((band_h - n*CHIP_H) / max(1, n-1)))\n",
        "            ys = [band_top + CHIP_H//2 + i*(CHIP_H + gap) for i in range(n)]\n",
        "        for i, (opt, yc) in enumerate(zip(options, ys)):\n",
        "            _chip(img, yc, f\"{chr(65+i)}. {opt}\")\n",
        "    return img\n",
        "\n",
        "def _reveal_slide(correct: str, explanation: str) -> Image.Image:\n",
        "    img = Image.new(\"RGB\", (W, H), BG)\n",
        "    _badge(img)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    card_w, card_h = W - 2*SAFE_X, 140\n",
        "    x0, y0 = SAFE_X, int(H*0.34)\n",
        "    draw.rounded_rectangle([x0, y0, x0+card_w, y0+card_h], radius=20, fill=MINT)\n",
        "    _draw_text_block(img, (x0+20, y0+16, card_w-40, card_h-32), f\"Answer: {correct}\", DARK, 80, 40)\n",
        "    _draw_text_block(img, (SAFE_X, int(H*0.54), W-2*SAFE_X, int(H*0.28)), explanation, MID, 62, 32)\n",
        "    return img\n",
        "\n",
        "def _cta_slide() -> Image.Image:\n",
        "    img = Image.new(\"RGB\", (W, H), BG)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    _badge(img)\n",
        "    _draw_text_block(img, (SAFE_X, int(H*0.36), W-2*SAFE_X, int(H*0.22)), \"MyAI101\", DARK, 160, 84)\n",
        "    _draw_text_block(img, (SAFE_X, int(H*0.54), W-2*SAFE_X, int(H*0.16)),\n",
        "                     \"Daily AI literacy in 60 seconds\", MID, 72, 38)\n",
        "    bx, by = (W-CTA_BUTTON_W)//2, int(H*0.74)\n",
        "    draw.rounded_rectangle([bx, by, bx+CTA_BUTTON_W, by+CTA_BUTTON_H], radius=22, fill=GREEN)\n",
        "    _draw_text_block(img, (bx+18, by+10, CTA_BUTTON_W-36, CTA_BUTTON_H-20), \"Start Learning for Free\", (255,255,255), 54, 28)\n",
        "    return img\n",
        "\n",
        "# ---------- Local fallback generator ----------\n",
        "@dataclass\n",
        "class QuizItem:\n",
        "    topic: str\n",
        "    question: str\n",
        "    options: List[str]\n",
        "    answer_index: int\n",
        "    explanation: str\n",
        "\n",
        "EASY_TEMPLATES = [\n",
        "    \"Which of these is an example of {topic}?\",\n",
        "    \"What is {topic} mainly used for?\",\n",
        "    \"Which choice best matches {topic}?\",\n",
        "]\n",
        "MEDIUM_TEMPLATES = [\n",
        "    \"Which scenario best illustrates {topic} in practice?\",\n",
        "    \"Which statement about {topic} is correct?\",\n",
        "    \"What is a common use case of {topic}?\",\n",
        "]\n",
        "HARD_TEMPLATES = [\n",
        "    \"Which of the following is most accurate regarding {topic}?\",\n",
        "    \"In applied settings, which describes {topic} most precisely?\",\n",
        "    \"Which statement about {topic} reflects best practice?\",\n",
        "]\n",
        "EASY_DISTRACTORS = [\"Something unrelated\",\"A wrong idea\",\"Not quite right\",\"Another choice\",\"Sounds similar but isn't\"]\n",
        "MEDIUM_DISTRACTORS = [\"A partially correct statement\",\"A common misconception\",\"An unrelated technique\",\"A vague description\"]\n",
        "HARD_DISTRACTORS = [\"A subtle misconception\",\"A related but incorrect method\",\"An imprecise definition\",\"A misleading best practice\"]\n",
        "\n",
        "def _pick_template(difficulty: int) -> str:\n",
        "    return (random.choice(EASY_TEMPLATES) if difficulty<=3\n",
        "            else random.choice(MEDIUM_TEMPLATES) if difficulty<=7\n",
        "            else random.choice(HARD_TEMPLATES))\n",
        "\n",
        "def _generate_options_local(topic: str, difficulty: int) -> Tuple[List[str], int, str]:\n",
        "    topic_clean = topic.strip().rstrip(\"?.!\")\n",
        "    if difficulty <= 3:\n",
        "        correct, pool = f\"A simple example of {topic_clean}\", EASY_DISTRACTORS\n",
        "    elif difficulty <= 7:\n",
        "        correct, pool = f\"A practical use case of {topic_clean}\", MEDIUM_DISTRACTORS\n",
        "    else:\n",
        "        correct, pool = f\"A precise description of {topic_clean}\", HARD_DISTRACTORS\n",
        "    distractors = random.sample(pool, k=3)\n",
        "    options = distractors + [correct]\n",
        "    random.shuffle(options)\n",
        "    answer_index = options.index(correct)\n",
        "    explanation = (f\"The correct option describes {topic_clean} more appropriately than the others.\"\n",
        "                   if difficulty >= 4 else f\"It's the best match for {topic_clean}.\")\n",
        "    return options, answer_index, explanation\n",
        "\n",
        "def make_quiz_item_local(topic: str, difficulty: int) -> QuizItem:\n",
        "    q = _pick_template(difficulty).format(topic=topic)\n",
        "    options, answer_index, explanation = _generate_options_local(topic, difficulty)\n",
        "    return QuizItem(topic=topic, question=q, options=options, answer_index=answer_index, explanation=explanation)\n",
        "\n",
        "# ---------- Gemini prompt ----------\n",
        "DIFFICULTY_GUIDE = \"\"\"\n",
        "Map difficulty 1–10 to these constraints:\n",
        "1–2: kid-simple; one sentence; no jargon; obvious distractors.\n",
        "3–4: basic recognition; short phrasing; simple plausible distractors.\n",
        "5–6: intermediate conceptual; 1–2 sentences; plausible/related distractors.\n",
        "7–8: advanced application or edge cases; 2–3 sentences; subtle distractors.\n",
        "9–10: professional nuance; 2–3 concise sentences; highly plausible distractors with subtle traps.\n",
        "\"\"\"\n",
        "\n",
        "GEMINI_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert quiz writer for 1080×1920 SHORT videos.\n",
        "\n",
        "Produce {count} distinct multiple-choice questions on the topic.\n",
        "\n",
        "Topic: \"{topic}\"\n",
        "Difficulty (1-10): {difficulty}\n",
        "\n",
        "{difficulty_guide}\n",
        "\n",
        "HARD LENGTH LIMITS (NEVER EXCEED):\n",
        "- question <= 130 characters\n",
        "- each option <= 38 characters\n",
        "- rationale <= 110 characters\n",
        "\n",
        "STYLE:\n",
        "- No preambles like \"Which of the following\". Write the stem directly.\n",
        "- Everyday words. No parentheticals, citations, footnotes, or emojis.\n",
        "- Difficulty comes from idea and distractor plausibility, not length.\n",
        "- Exactly 1 correct answer, 3 plausible distractors. No \"All/None of the above\".\n",
        "\n",
        "Return ONLY a single JSON object BETWEEN the markers below.\n",
        "Start your first character with the JSON after {BEGIN_JSON} and end before {END_JSON}.\n",
        "Do not include any text outside the markers. No markdown fences.\n",
        "\n",
        "{BEGIN_JSON}\n",
        "{{\n",
        "  \"topic\": \"string\",\n",
        "  \"difficulty\": {difficulty},\n",
        "  \"questions\": [\n",
        "    {{\n",
        "      \"question\": \"string (<=130 chars)\",\n",
        "      \"correct_answer\": \"string (<=38 chars)\",\n",
        "      \"distractors\": [\"string (<=38)\",\"string (<=38)\",\"string (<=38)\"],\n",
        "      \"rationale\": \"string (<=110 chars)\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "{END_JSON}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def _strip_code_fences(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    if s.startswith(\"```\"):\n",
        "        s = re.sub(r\"^```(?:json)?\", \"\", s.strip(), flags=re.IGNORECASE).strip()\n",
        "        s = re.sub(r\"```$\", \"\", s.strip()).strip()\n",
        "    return s\n",
        "\n",
        "def _ensure_3_distractors(distractors: List[str], correct: str) -> List[str]:\n",
        "    seen, out = set(), []\n",
        "    for d in distractors:\n",
        "        d = (d or \"\").strip()\n",
        "        if not d: continue\n",
        "        if d.lower() == (correct or \"\").strip().lower(): continue\n",
        "        if d.lower() in seen: continue\n",
        "        out.append(d); seen.add(d.lower())\n",
        "    while len(out) < 3:\n",
        "        out.append(f\"Alternative {len(out)+1}\")\n",
        "    return out[:3]\n",
        "\n",
        "def _shuffle_with_correct(correct: str, distractors: List[str]) -> Tuple[List[str], int]:\n",
        "    opts = distractors[:3] + [correct]\n",
        "    random.shuffle(opts)\n",
        "    idx = opts.index(correct)\n",
        "    return opts, idx\n",
        "\n",
        "\n",
        "def _coerce_objectish(s: str) -> str:\n",
        "    \"\"\"\n",
        "    If the text between markers looks like top-level key/value lines but lacks\n",
        "    outer braces, wrap it into {...}. Also trims leading/trailing junk.\n",
        "    \"\"\"\n",
        "    t = (s or \"\").strip()\n",
        "    if not t:\n",
        "        return t\n",
        "    # Already an object/array? leave it.\n",
        "    if t.startswith(\"{\") or t.startswith(\"[\"):\n",
        "        return t\n",
        "    # Common case we see: starts with \"topic\": ... or \"questions\": ...\n",
        "    if t.startswith('\"topic\"') or t.startswith('\"difficulty\"') or t.startswith('\"questions\"'):\n",
        "        # Avoid double closing brace if user/model already placed a stray \"}\"\n",
        "        t = t.rstrip()\n",
        "        if not t.endswith(\"}\"):\n",
        "            t = t + \"}\"\n",
        "        return \"{\" + t\n",
        "    # Fallback: return as-is\n",
        "    return t\n",
        "\n",
        "# --- extra shrink helpers ---\n",
        "_ABBR_REPL = [\n",
        "    (r\"\\bapproximately\\b\", \"~\"),\n",
        "    (r\"\\babout\\b\", \"~\"),\n",
        "    (r\"\\baround\\b\", \"~\"),\n",
        "    (r\"\\bversus\\b\", \"vs\"),\n",
        "    (r\"\\band\\b\", \"&\"),\n",
        "    (r\"\\bpercent\\b\", \"%\"),\n",
        "    (r\"\\bper\\s+cent\\b\", \"%\"),\n",
        "    (r\"\\byears?\\b\", \"yrs\"),\n",
        "    (r\"\\bminutes?\\b\", \"min\"),\n",
        "    (r\"\\bhours?\\b\", \"h\"),\n",
        "    (r\"\\bseconds?\\b\", \"s\"),\n",
        "    (r\"\\bmillion\\b\", \"M\"),\n",
        "    (r\"\\bbillion\\b\", \"B\"),\n",
        "    (r\"\\bUnited States\\b\", \"US\"),\n",
        "    (r\"\\bUnited Kingdom\\b\", \"UK\"),\n",
        "    (r\"\\bkilometers per hour\\b\", \"km/h\"),\n",
        "    (r\"\\bkilometres per hour\\b\", \"km/h\"),\n",
        "    (r\"\\bmiles per hour\\b\", \"mph\"),\n",
        "]\n",
        "_MONTHS = {\n",
        "    \"January\":\"Jan\",\"February\":\"Feb\",\"March\":\"Mar\",\"April\":\"Apr\",\"June\":\"Jun\",\n",
        "    \"July\":\"Jul\",\"August\":\"Aug\",\"September\":\"Sep\",\"October\":\"Oct\",\"November\":\"Nov\",\"December\":\"Dec\",\n",
        "    \"May\":\"May\",\n",
        "}\n",
        "\n",
        "def _abbr_pass(s: str) -> str:\n",
        "    t = s\n",
        "    for pat, rep in _ABBR_REPL:\n",
        "        t = _re.sub(pat, rep, t, flags=_re.IGNORECASE)\n",
        "    for long, short in _MONTHS.items():\n",
        "        t = _re.sub(rf\"\\b{long}\\b\", short, t)\n",
        "    # compact number + unit (e.g., \"10 years\" -> \"10 yrs\")\n",
        "    t = _re.sub(r\"\\b(\\d+)\\s+yrs?\\b\", r\"\\1 yrs\", t)\n",
        "    t = _re.sub(r\"\\b(\\d+)\\s+minutes?\\b\", r\"\\1 min\", t)\n",
        "    t = _re.sub(r\"\\b(\\d+)\\s+hours?\\b\", r\"\\1 h\", t)\n",
        "    t = _squash_spaces(t)\n",
        "    return t\n",
        "\n",
        "def _prune_clauses(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Drop tailing descriptive clauses that usually aren't essential for short-form:\n",
        "    comma 'which/that/when/where/who', em/en-dash chunks, colon tails.\n",
        "    \"\"\"\n",
        "    t = s\n",
        "    t = _re.sub(r\",\\s+(which|that|who|when|where)\\b.*$\", \"\", t, flags=_re.IGNORECASE)\n",
        "    t = _re.sub(r\"\\s+[–—-]\\s+.*$\", \"\", t)     # after dash\n",
        "    t = _re.sub(r\":\\s+.*$\", \"\", t)            # after colon\n",
        "    return _squash_spaces(t)\n",
        "\n",
        "\n",
        "def _prune_clauses_question(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Safer pruning for questions:\n",
        "    - Only drop comma-tails that look descriptive (e.g., ', which is ...')\n",
        "    - Keep dash/colon tails trimmed.\n",
        "    \"\"\"\n",
        "    t = s\n",
        "    # descriptive comma tails\n",
        "    t = _re.sub(r\",\\s+(which|that|who)\\s+(is|are|was|were)\\b.*$\", \"\", t, flags=_re.IGNORECASE)\n",
        "    t = _re.sub(r\",\\s+(when|where)\\s+(it|they)\\s+(is|are|was|were)\\b.*$\", \"\", t, flags=_re.IGNORECASE)\n",
        "    # dash/colon tails\n",
        "    t = _re.sub(r\"\\s+[–—-]\\s+.*$\", \"\", t)\n",
        "    t = _re.sub(r\":\\s+.*$\", \"\", t)\n",
        "    return _squash_spaces(t)\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Gemini REST helpers ----------\n",
        "API_BASE = \"https://generativelanguage.googleapis.com/v1\"\n",
        "\n",
        "def list_models_v1(api_key: str) -> list:\n",
        "\n",
        "# print(prompt[:300])\n",
        "# assert BEGIN_JSON not in prompt and END_JSON not in prompt\n",
        "# assert \"<<<JSON>>>\\n{\" in prompt\n",
        "\n",
        "    r = requests.get(f\"{API_BASE}/models\", params={\"key\": api_key}, timeout=30)\n",
        "    if r.status_code >= 400:\n",
        "        raise RuntimeError(f\"REST {r.status_code}: {r.text[:200]}\")\n",
        "    return r.json().get(\"models\", []) or []\n",
        "\n",
        "def filter_generate_content_models(models: list) -> list:\n",
        "    out = []\n",
        "    for m in models:\n",
        "        methods = (\n",
        "            m.get(\"supportedGenerationMethods\")\n",
        "            or m.get(\"supported_generation_methods\")\n",
        "            or []\n",
        "        )\n",
        "        if \"generateContent\" in methods:\n",
        "            name = m.get(\"name\", \"\")\n",
        "            if name:\n",
        "                out.append(name.split(\"/\")[-1])\n",
        "    return out\n",
        "\n",
        "def gemini_generate_v1(api_key: str, model: str, prompt: str,\n",
        "                       temperature: float, max_output_tokens: int) -> str:\n",
        "    url = f\"{API_BASE}/models/{model}:generateContent\"\n",
        "    params = {\"key\": api_key}\n",
        "    payload = {\n",
        "        \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": float(temperature),\n",
        "            \"topP\": 0.9,\n",
        "            \"topK\": 40,\n",
        "            \"maxOutputTokens\": int(max_output_tokens),\n",
        "            \"candidateCount\": 1,\n",
        "        },\n",
        "    }\n",
        "    r = requests.post(url, params=params, json=payload, timeout=60)\n",
        "    if r.status_code >= 400:\n",
        "        raise RuntimeError(f\"REST {r.status_code}: {r.text[:200]}\")\n",
        "    data = r.json()\n",
        "\n",
        "    cands = data.get(\"candidates\", [])\n",
        "    if not cands:\n",
        "        raise RuntimeError(f\"Empty candidates: {data}\")\n",
        "\n",
        "    cand0 = cands[0]\n",
        "    finish = cand0.get(\"finishReason\")\n",
        "    parts = (cand0.get(\"content\") or {}).get(\"parts\") or []\n",
        "    text = \"\".join(p.get(\"text\", \"\") for p in parts if isinstance(p, dict))\n",
        "\n",
        "    if not text:\n",
        "        raise RuntimeError(f\"No text in response (finishReason={finish}). Raw: {json.dumps(data)[:400]}\")\n",
        "\n",
        "    return text\n",
        "\n",
        "def _candidate_models_ordered(requested: str, available: list) -> list:\n",
        "    req = (requested or \"\").strip()\n",
        "    if req.endswith(\"-latest\"):\n",
        "        req = req[:-7] + \"-001\"\n",
        "    order = []\n",
        "    if req:\n",
        "        order.append(req)\n",
        "    for m in [\"gemini-2.5-flash\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-2.0-flash-exp\", \"gemini-pro\", \"gemini-1.0-pro\"]:\n",
        "        if m not in order:\n",
        "            order.append(m)\n",
        "    avail = set(available)\n",
        "    return [m for m in order if m in avail]\n",
        "\n",
        "# ---------- AI question generator (REST) ----------\n",
        "def generate_questions_gemini(api_key: str, model_name: str, topic: str, difficulty: int, count: int,\n",
        "                              temperature: float = 0.6, max_output_tokens: int = 1600):\n",
        "    topic = (topic or \"\").strip()\n",
        "    difficulty = int(max(1, min(10, difficulty)))\n",
        "    count = int(max(1, min(10, count)))\n",
        "    items: List[QuizItem] = []\n",
        "    info_msgs: List[str] = []\n",
        "\n",
        "    if not api_key:\n",
        "        info_msgs.append(\"No Gemini API key provided — using local generator.\")\n",
        "        for _ in range(count):\n",
        "            items.append(make_quiz_item_local(topic, difficulty))\n",
        "        return items, \" \".join(info_msgs)\n",
        "\n",
        "    # Discover models\n",
        "    try:\n",
        "        models_raw = list_models_v1(api_key)\n",
        "        available = filter_generate_content_models(models_raw)\n",
        "    except Exception as e:\n",
        "        available = []\n",
        "        info_msgs.append(f\"Model listing failed ({e}); attempting common defaults.\")\n",
        "\n",
        "    to_try = _candidate_models_ordered(\n",
        "        model_name,\n",
        "        available if available else [\"gemini-2.5-flash\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-pro\", \"gemini-1.0-pro\"]\n",
        "    ) or [\"gemini-2.5-flash\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-pro\", \"gemini-1.0-pro\"]\n",
        "\n",
        "    last_err = None\n",
        "    for m in to_try:\n",
        "        try:\n",
        "            remaining = count\n",
        "            # Difficulty-aware chunking + token headroom\n",
        "            if difficulty >= 9:\n",
        "                chunk = 1\n",
        "            elif difficulty >= 7:\n",
        "                chunk = min(2, remaining)\n",
        "            else:\n",
        "                chunk = min(3, remaining)\n",
        "\n",
        "            tok = max(max_output_tokens if max_output_tokens else 1600,\n",
        "                      1400 if difficulty >= 7 else 1200)\n",
        "\n",
        "            while remaining > 0:\n",
        "                take = min(chunk, remaining)\n",
        "                prompt = GEMINI_PROMPT_TEMPLATE.format(\n",
        "                    count=take,\n",
        "                    topic=topic,\n",
        "                    difficulty=difficulty,\n",
        "                    difficulty_guide=DIFFICULTY_GUIDE,\n",
        "                    BEGIN_JSON=BEGIN_JSON,\n",
        "                    END_JSON=END_JSON,\n",
        "                )\n",
        "\n",
        "                # Try call, back off on MAX_TOKENS/no-text\n",
        "                try:\n",
        "                    text = gemini_generate_v1(api_key, m, prompt, temperature, tok)\n",
        "                except Exception as e:\n",
        "                    msg = str(e)\n",
        "                    if \"finishReason=MAX_TOKENS\" in msg or \"No text in response\" in msg:\n",
        "                        if take > 1:\n",
        "                            chunk = 1\n",
        "                            continue\n",
        "                        else:\n",
        "                            tok = min(tok + 300, 4096)\n",
        "                            text = gemini_generate_v1(api_key, m, prompt, temperature, tok)\n",
        "                    else:\n",
        "                        raise\n",
        "\n",
        "                qlist = parse_questions_from_model(text)\n",
        "                if not qlist:\n",
        "                    # backoff parse once more\n",
        "                    if take > 1:\n",
        "                        chunk = 1\n",
        "                        continue\n",
        "                    tok = min(tok + 300, 4096)\n",
        "                    text = gemini_generate_v1(api_key, m, prompt, temperature, tok)\n",
        "                    qlist = parse_questions_from_model(text)\n",
        "                    if not qlist:\n",
        "                        raise ValueError(\"Gemini returned no questions (parse).\")\n",
        "\n",
        "                if m != model_name and f\"Using available model: {m}\" not in info_msgs:\n",
        "                    info_msgs.append(f\"Using available model: {m}\")\n",
        "\n",
        "                use_n = min(len(qlist), remaining)\n",
        "                for q in qlist[:use_n]:\n",
        "                    qtext = (q.get(\"question\") or \"\").strip()\n",
        "                    correct = (q.get(\"correct_answer\") or q.get(\"answer\") or q.get(\"correct\") or \"\").strip()\n",
        "                    distractors = q.get(\"distractors\") or []\n",
        "                    rationale = (q.get(\"rationale\") or q.get(\"explanation\") or \"\").strip()\n",
        "                    if not qtext or not correct:\n",
        "                        continue\n",
        "                    distractors = _ensure_3_distractors(distractors, correct)\n",
        "                    options, answer_index = _shuffle_with_correct(correct, distractors)\n",
        "                    qtext, options, rationale, _ = lint_item_lengths(qtext, options, rationale)\n",
        "                    items.append(QuizItem(topic=topic, question=qtext, options=options,\n",
        "                                          answer_index=answer_index, explanation=rationale))\n",
        "\n",
        "                remaining -= use_n\n",
        "                chunk = min(chunk, remaining) if remaining > 0 else chunk\n",
        "\n",
        "            return items, (\" \".join(info_msgs) if info_msgs else f\"Generated {len(items)} item(s) via Gemini.\")\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            continue\n",
        "\n",
        "    info_msgs.append(f\"Gemini error: {last_err}. Falling back to local generator.\")\n",
        "    items = [make_quiz_item_local(topic, difficulty) for _ in range(count)]\n",
        "    return items, \" \".join(info_msgs)\n",
        "\n",
        "# ---------- FFmpeg assembly ----------\n",
        "def _save_png(img: Image.Image, path: Path):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    img.save(str(path), format=\"PNG\", optimize=True)\n",
        "\n",
        "def _ffmpeg_build(out_path: Path, question_png: Path, reveal_png: Path, cta_png: Path,\n",
        "                  q_sec=6.0, r_sec=4.3, c_sec=2.9):\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\"-y\",\n",
        "        \"-loglevel\",\"error\",\n",
        "        \"-threads\",\"1\",\n",
        "        \"-loop\",\"1\",\"-t\",f\"{q_sec}\",\"-i\",str(question_png),\n",
        "        \"-loop\",\"1\",\"-t\",f\"{r_sec}\",\"-i\",str(reveal_png),\n",
        "        \"-loop\",\"1\",\"-t\",f\"{c_sec}\",\"-i\",str(cta_png),\n",
        "        \"-filter_complex\",\n",
        "        (\n",
        "          f\"[0:v]scale={W}:{H},setsar=1[v0];\"\n",
        "          f\"[1:v]scale={W}:{H},setsar=1[v1];\"\n",
        "          f\"[2:v]scale={W}:{H},setsar=1[v2];\"\n",
        "          f\"[v0][v1][v2]concat=n=3:v=1:a=0,format=yuv420p[v]\"\n",
        "        ),\n",
        "        \"-map\",\"[v]\",\"-r\", str(FPS),\n",
        "        \"-c:v\",\"libx264\",\"-preset\",\"veryfast\",\n",
        "        \"-b:v\", BITRATE,\"-maxrate\", BITRATE,\"-bufsize\", BITRATE,\n",
        "        \"-movflags\",\"+faststart\",\"-g\", str(FPS*2),\n",
        "        str(out_path)\n",
        "    ]\n",
        "    subprocess.check_call(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "\n",
        "def build_quiz_video(item: QuizItem, out_path: Path) -> Path:\n",
        "    q_img = _question_slide(item.question, item.options[:4])\n",
        "    r_img = _reveal_slide(item.options[item.answer_index], item.explanation)\n",
        "    c_img = _cta_slide()\n",
        "    base = TMP_DIR / f\"vid_{int(time.time()*1000)}_{random.randint(1000,9999)}\"\n",
        "    q_png, r_png, c_png = base.with_suffix(\".q.png\"), base.with_suffix(\".r.png\"), base.with_suffix(\".c.png\")\n",
        "    _save_png(q_img, q_png); _save_png(r_img, r_png); _save_png(c_img, c_png)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    _ffmpeg_build(out_path, q_png, r_png, c_png)\n",
        "    for p in [q_png, r_png, c_png]:\n",
        "        try: p.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "    return out_path\n",
        "\n",
        "def _slugify(text: str) -> str:\n",
        "    s = text.lower().strip().replace(\" \", \"-\")\n",
        "    allowed = set(string.ascii_lowercase + string.digits + \"-\")\n",
        "    s = \"\".join(ch for ch in s if ch in allowed)\n",
        "    return s or f\"item-{int(time.time())}\"\n",
        "\n",
        "# ---------- Gradio callbacks ----------\n",
        "def _seed_rng(): random.seed(time.time_ns() % (2**32 - 1))\n",
        "\n",
        "def generate_preview_ai(topic: str, difficulty: int, count: int, api_key: str, model_name: str,\n",
        "                        temperature: float = 0.6, max_output_tokens: int = 1600):\n",
        "    topic = (topic or \"\").strip()\n",
        "    if not topic:\n",
        "        return pd.DataFrame([]), \"Please enter a topic.\", [], \"\"\n",
        "    difficulty = int(max(1, min(10, difficulty)))\n",
        "    count = int(max(1, min(10, count)))\n",
        "    _seed_rng()\n",
        "\n",
        "    items, info_msg = generate_questions_gemini(api_key, model_name, topic, difficulty, count,\n",
        "                                                temperature=temperature, max_output_tokens=max_output_tokens)\n",
        "\n",
        "    rows = []\n",
        "    for i, it in enumerate(items, 1):\n",
        "        q2, opts2, exp2, flags = lint_item_lengths(it.question, it.options[:4], it.explanation, add_ellipsis=False)\n",
        "        q_show  = q2 + (CLIP_FLAG if flags[\"Q\"] else \"\")\n",
        "        a_show  = opts2[0] + (CLIP_FLAG if flags[\"A\"] else \"\")\n",
        "        b_show  = opts2[1] + (CLIP_FLAG if flags[\"B\"] else \"\")\n",
        "        c_show  = opts2[2] + (CLIP_FLAG if flags[\"C\"] else \"\")\n",
        "        d_show  = opts2[3] + (CLIP_FLAG if flags[\"D\"] else \"\")\n",
        "        exp_show= exp2 + (CLIP_FLAG if flags[\"EXP\"] else \"\")\n",
        "\n",
        "        rows.append({\n",
        "            \"#\": i, \"Question\": q_show,\n",
        "            \"A\": a_show, \"B\": b_show, \"C\": c_show, \"D\": d_show,\n",
        "            \"Correct\": [\"A\",\"B\",\"C\",\"D\"][it.answer_index],\n",
        "            \"Explanation\": exp_show,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    status = info_msg or f\"Generated {len(rows)} quiz item(s). Review below.\"\n",
        "    return df, status, rows, topic\n",
        "\n",
        "def confirm_and_produce(items_table, topic: str):\n",
        "    if items_table is None:\n",
        "        return \"No items to render. Please generate a preview first.\", []\n",
        "    if hasattr(items_table, \"to_dict\"):\n",
        "        rows = items_table.to_dict(orient=\"records\")\n",
        "    elif isinstance(items_table, list):\n",
        "        rows = items_table\n",
        "    else:\n",
        "        rows = []\n",
        "    if not rows:\n",
        "        return \"No items to render. Please generate a preview first.\", []\n",
        "\n",
        "    saved_paths = []\n",
        "    for idx, raw in enumerate(rows, 1):\n",
        "        def _clean(s):\n",
        "            return (s or \"\").replace(CLIP_FLAG, \"\").strip()\n",
        "\n",
        "        options = [_clean(raw.get(\"A\",\"\")), _clean(raw.get(\"B\",\"\")),\n",
        "                   _clean(raw.get(\"C\",\"\")), _clean(raw.get(\"D\",\"\"))]\n",
        "        correct_letter = str(raw.get(\"Correct\",\"A\")).strip().upper()\n",
        "        answer_index = {\"A\":0,\"B\":1,\"C\":2,\"D\":3}.get(correct_letter, 0)\n",
        "\n",
        "        q = _clean(raw.get(\"Question\",\"\"))\n",
        "        exp = _clean(raw.get(\"Explanation\",\"\"))\n",
        "\n",
        "        q, options, exp, _ = lint_item_lengths(q, options, exp, add_ellipsis=True)\n",
        "\n",
        "\n",
        "        qi = QuizItem(topic=topic or \"\", question=q, options=options,\n",
        "                      answer_index=answer_index, explanation=exp)\n",
        "\n",
        "        slug = _slugify(f\"{topic}-{idx}\")\n",
        "        out_path = OUT_DIR / f\"myai101_{slug}.mp4\"\n",
        "        try:\n",
        "            build_quiz_video(qi, out_path)\n",
        "            saved_paths.append(str(out_path))\n",
        "        except Exception as e:\n",
        "            saved_paths.append(f\"ERROR: {e}\")\n",
        "        gc.collect()\n",
        "\n",
        "    msg = f\"Done. Produced {len(saved_paths)} video(s).\"\n",
        "    files = [p for p in saved_paths if Path(p).suffix.lower()==\".mp4\" and Path(p).exists()]\n",
        "    return msg, files\n",
        "\n",
        "_empty_df = pd.DataFrame(columns=[\"#\", \"Question\", \"A\", \"B\", \"C\", \"D\", \"Correct\", \"Explanation\"])\n",
        "def _make_table():\n",
        "    try:\n",
        "        return gr.Dataframe(\n",
        "            headers=list(_empty_df.columns),\n",
        "            datatype=[\"number\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\"],\n",
        "            row_count=(1, \"dynamic\"),\n",
        "            col_count=(8, \"fixed\"),\n",
        "            wrap=True,\n",
        "            label=\"You can edit cells before confirming to tweak wording.\",\n",
        "            interactive=True,\n",
        "        )\n",
        "    except TypeError:\n",
        "        return gr.Dataframe(\n",
        "            value=_empty_df,\n",
        "            headers=list(_empty_df.columns),\n",
        "            wrap=True,\n",
        "            label=\"You can edit cells before confirming to tweak wording.\",\n",
        "            interactive=True,\n",
        "        )\n",
        "\n",
        "# ---------- UI ----------\n",
        "with gr.Blocks(title=\"MyAI101 — Quiz Video Maker (Gemini REST, No MoviePy, 1080x1920)\") as demo:\n",
        "    gr.Markdown(\"# MyAI101 — Quiz Video Maker (Gemini, **REST**, no MoviePy, 1080×1920)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        topic_inp = gr.Textbox(label=\"Topic\", placeholder=\"e.g. Backpropagation, SSL certificates, Photosynthesis\")\n",
        "    with gr.Row():\n",
        "        diff_inp  = gr.Slider(1, 10, value=5, step=1, label=\"Difficulty (1 = child <10, 10 = professional)\")\n",
        "        count_inp = gr.Slider(1, 10, value=3, step=1, label=\"How many questions / videos to create\")\n",
        "\n",
        "    gr.Markdown(\"### AI Generation Settings (Gemini)\")\n",
        "    with gr.Row():\n",
        "        api_key_inp = gr.Textbox(label=\"Gemini API Key\", placeholder=\"Paste your Google AI Studio API key\", type=\"password\")\n",
        "        model_inp   = gr.Dropdown(choices=[\"gemini-2.5-flash\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\"], value=\"gemini-2.5-flash\", label=\"Model\")\n",
        "        list_btn    = gr.Button(\"🔎 List Available Models\")\n",
        "\n",
        "    with gr.Row():\n",
        "        temp_inp    = gr.Slider(0.0, 1.0, value=0.6, step=0.1, label=\"Temperature (creativity)\")\n",
        "        max_tok_inp = gr.Slider(200, 4096, value=1600, step=100, label=\"Max output tokens\")\n",
        "\n",
        "    with gr.Row():\n",
        "        preview_btn = gr.Button(\"🧠 Generate Preview (AI)\")\n",
        "        regen_btn   = gr.Button(\"↻ Regenerate\")\n",
        "\n",
        "    gr.Markdown(\"### Preview: Questions & Answers\")\n",
        "    preview_state = gr.State([])   # Python list (for safety across Gradio versions)\n",
        "    topic_state   = gr.State(\"\")   # store topic used for preview\n",
        "\n",
        "    table  = _make_table()\n",
        "    status = gr.Markdown(visible=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        confirm_btn = gr.Button(\"✅ Confirm & Produce Videos\", variant=\"primary\")\n",
        "    out_msg   = gr.Markdown()\n",
        "    out_files = gr.Files(label=\"Rendered MP4s\")\n",
        "\n",
        "    # Callbacks\n",
        "    def _on_list_models(api_key):\n",
        "        if not api_key:\n",
        "            return gr.update(), \"Enter API key first.\"\n",
        "        try:\n",
        "            mods = list_models_v1(api_key)\n",
        "            choices = filter_generate_content_models(mods)\n",
        "            if not choices:\n",
        "                return gr.update(choices=[], value=None), \"No generateContent-capable models found for this key.\"\n",
        "            # Prefer newer flash if present\n",
        "            default = (\"gemini-2.5-flash\" if \"gemini-2.5-flash\" in choices\n",
        "                       else \"gemini-1.5-flash-001\" if \"gemini-1.5-flash-001\" in choices\n",
        "                       else choices[0])\n",
        "            return gr.update(choices=choices, value=default), f\"Found {len(choices)} model(s).\"\n",
        "        except Exception as e:\n",
        "            return gr.update(), f\"Listing failed: {e}\"\n",
        "\n",
        "    def _on_preview(topic, diff, count, api_key, model_name, temperature, max_tokens):\n",
        "        df, msg, rows, used_topic = generate_preview_ai(topic, int(diff), int(count), api_key, model_name, float(temperature), int(max_tokens))\n",
        "        if not hasattr(df, \"to_dict\"):\n",
        "            df = pd.DataFrame(df)\n",
        "        return df, msg, rows, used_topic\n",
        "\n",
        "    def _on_confirm(current_table, topic_used):\n",
        "        return confirm_and_produce(current_table, topic_used)\n",
        "\n",
        "    list_btn.click(_on_list_models, inputs=[api_key_inp], outputs=[model_inp, status])\n",
        "    preview_btn.click(_on_preview, inputs=[topic_inp, diff_inp, count_inp, api_key_inp, model_inp, temp_inp, max_tok_inp], outputs=[table, status, preview_state, topic_state])\n",
        "    regen_btn.click(_on_preview,   inputs=[topic_inp, diff_inp, count_inp, api_key_inp, model_inp, temp_inp, max_tok_inp], outputs=[table, status, preview_state, topic_state])\n",
        "    confirm_btn.click(_on_confirm, inputs=[table, topic_state], outputs=[out_msg, out_files])\n",
        "\n",
        "# --- Launch (print URLs) ---\n",
        "gr.close_all(); gc.collect()\n",
        "res = demo.launch(share=True, inbrowser=False, inline=False, show_error=True, debug=True, prevent_thread_lock=True)\n",
        "try:\n",
        "    print(\"Local URL:\", getattr(res, \"local_url\", None) or res.local_url)\n",
        "    print(\"Public URL:\", getattr(res, \"share_url\", None) or res.share_url)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    }
  ]
}