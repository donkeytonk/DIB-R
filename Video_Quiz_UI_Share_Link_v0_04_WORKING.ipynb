{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlnjp61fvFkZihmQMXgcZw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donkeytonk/DIB-R/blob/master/Video_Quiz_UI_Share_Link_v0_04_WORKING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 0.04 Updates:\n",
        "- Standards & Topic options for Standards Mode"
      ],
      "metadata": {
        "id": "BQy6taxmLo9r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dW3gdQglzInu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a690249-3cbf-4137-b220-74f423d43808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a22cfb1ab7ea14c30e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a22cfb1ab7ea14c30e.gradio.live\n"
          ]
        }
      ],
      "source": [
        "# === ONE-CELL: MyAI101 Quiz Video Maker (1080x1920, Gemini-powered via REST, No MoviePy) ===\n",
        "# - Vertical HD (1080x1920 @ 24fps), ffmpeg only (threads=1)\n",
        "# - Gradio UI: Topic, Difficulty(1-10), Count(1-10), Gemini API Key, Model\n",
        "# - Uses Gemini **v1 REST** (no SDK). Button to list models your key actually has.\n",
        "# - Randomizes correct answer placement; editable preview table\n",
        "# - Renders MP4s to /content/out/videos; prints Local/Public URLs\n",
        "\n",
        "import os, sys, subprocess, random, string, time, gc, warnings, re, json\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Headless safety ---\n",
        "os.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"\n",
        "os.environ[\"XDG_RUNTIME_DIR\"]  = \"/tmp/runtime\"\n",
        "os.makedirs(\"/tmp/runtime\", exist_ok=True)\n",
        "\n",
        "# --- Deps ---\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                       \"pillow>=10.0.0\", \"gradio>=4.0.0\", \"pandas>=1.3\", \"requests>=2.31\"])\n",
        "subprocess.call([\"apt-get\", \"-y\", \"install\", \"-qq\",\n",
        "                 \"ffmpeg\", \"fonts-dejavu-core\", \"fonts-liberation\"])\n",
        "\n",
        "import requests\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# ---------- RENDER SETTINGS ----------\n",
        "W, H, FPS = 1080, 1920, 24\n",
        "BITRATE = \"3500k\"\n",
        "\n",
        "GREEN   = (16, 163, 74)\n",
        "MINT    = (209, 250, 229)\n",
        "DARK    = (15, 23, 42)\n",
        "MID     = (51, 65, 85)\n",
        "BG      = (255, 255, 255)\n",
        "CHIP_BG = (243, 244, 246)\n",
        "\n",
        "SAFE_X   = int(W * 0.09)\n",
        "SAFE_TOP = int(H * 0.08)\n",
        "SAFE_BOT = int(H * 0.10)\n",
        "\n",
        "TITLE_BOX_H         = int(H * 0.36)\n",
        "GAP_BELOW_TITLE_MIN = int(H * 0.10)\n",
        "\n",
        "CHIP_H      = 108\n",
        "CHIP_TX_H   = 80\n",
        "CHIP_TX_PAD = 90\n",
        "CHIP_GAP    = int(H * 0.085)\n",
        "\n",
        "CTA_BUTTON_W = 640\n",
        "CTA_BUTTON_H = 120\n",
        "\n",
        "# ------- Readability budgets -------\n",
        "Q_MAX = 130\n",
        "OPT_MAX = 38\n",
        "EXP_MAX = 110\n",
        "\n",
        "CLIP_FLAG = \" ·clipped\"\n",
        "\n",
        "BEGIN_JSON = \"<<<JSON>>>\"\n",
        "END_JSON   = \"<<<END>>>\"\n",
        "\n",
        "import re as _re\n",
        "\n",
        "# ---------- Packs / Blueprints (starter set) ----------\n",
        "PACKS: Dict[str, dict] = {\n",
        "    \"llm_101\": {\n",
        "        \"id\": \"llm_101\",\n",
        "        \"name\": \"LLM Fundamentals\",\n",
        "        \"domain\": \"ai\",\n",
        "        \"locale\": \"ANY\",\n",
        "        \"tags\": [\"llm\",\"tokens\",\"context\",\"embeddings\",\"finetune\",\"tools\"],\n",
        "        \"banned_phrases\": [\"All of the above\", \"None of the above\", \"It depends\"],\n",
        "        \"prompt_extra\": \"Prefer concrete, non-marketing language. No trivia about product brand names.\",\n",
        "        \"categories\": {\"Tokens & Context\":2, \"Embeddings\":2, \"Pretrain vs Finetune\":2, \"Tool Use\":2},\n",
        "        \"facts\": [\n",
        "            {\"id\":\"LLM-TOK-001\",\"text\":\"Tokens are units of text the model reads and generates.\"},\n",
        "            {\"id\":\"LLM-CTX-002\",\"text\":\"A model’s context window caps how many tokens it can consider at once.\"},\n",
        "            {\"id\":\"LLM-EMB-003\",\"text\":\"Embeddings map text to vectors that capture semantic similarity.\"},\n",
        "            {\"id\":\"LLM-FIN-004\",\"text\":\"Finetuning adjusts model weights; prompting does not change weights.\"},\n",
        "            {\"id\":\"LLM-TOOL-005\",\"text\":\"Tool use lets a model call functions or APIs to get external results.\"},\n",
        "            {\"id\":\"LLM-HAL-006\",\"text\":\"Hallucination is confident output not grounded in facts or tools.\"},\n",
        "            {\"id\":\"LLM-SYS-007\",\"text\":\"System prompts steer model behavior but are not guaranteed to be followed.\"},\n",
        "            {\"id\":\"LLM-TMP-008\",\"text\":\"Higher temperature increases randomness in token sampling.\"},\n",
        "            {\"id\":\"LLM-TOP-009\",\"text\":\"Top-p limits sampling to the smallest set of tokens whose mass ≥ p.\"},\n",
        "            {\"id\":\"LLM-COT-010\",\"text\":\"Chain-of-thought reveals intermediate steps but can leak private info if logged.\"},\n",
        "        ],\n",
        "    },\n",
        "    \"rag_essentials\": {\n",
        "        \"id\": \"rag_essentials\",\n",
        "        \"name\": \"RAG Essentials\",\n",
        "        \"domain\": \"ai\",\n",
        "        \"locale\": \"ANY\",\n",
        "        \"tags\": [\"rag\",\"retrieval\",\"rerank\",\"chunking\",\"eval\"],\n",
        "        \"banned_phrases\": [\"All of the above\", \"None of the above\"],\n",
        "        \"prompt_extra\": \"Focus on retrieval quality, reranking precision, chunking trade-offs, and evaluation signals.\",\n",
        "        \"categories\": {\"Chunking\":2, \"Retrieval\":2, \"Reranking\":2, \"Evaluation\":2},\n",
        "        \"facts\": [\n",
        "            {\"id\":\"RAG-CH-001\",\"text\":\"Overlapping chunks improve recall at the cost of more tokens.\"},\n",
        "            {\"id\":\"RAG-RET-002\",\"text\":\"BM25 is lexical; dense retrieval uses embeddings for semantic matches.\"},\n",
        "            {\"id\":\"RAG-RR-003\",\"text\":\"Rerankers rescore top-k candidates to improve precision.\"},\n",
        "            {\"id\":\"RAG-HYB-004\",\"text\":\"Hybrid retrieval combines lexical and dense signals for robustness.\"},\n",
        "            {\"id\":\"RAG-EVL-005\",\"text\":\"Answer-grounded evals compare the answer to the retrieved context.\"},\n",
        "            {\"id\":\"RAG-K-006\",\"text\":\"Raising k increases recall but may add noise and latency.\"},\n",
        "            {\"id\":\"RAG-CH-007\",\"text\":\"Smaller chunks reduce drift; larger chunks preserve cross-sentence context.\"},\n",
        "            {\"id\":\"RAG-CIT-008\",\"text\":\"Citations help users verify and trust answers.\"},\n",
        "            {\"id\":\"RAG-FMT-009\",\"text\":\"Structured chunks (headings, lists) often retrieve better than raw text.\"},\n",
        "            {\"id\":\"RAG-DUP-010\",\"text\":\"Deduping near-identical chunks reduces wasted tokens and bias.\"},\n",
        "        ],\n",
        "    },\n",
        "    \"prompt_patterns\": {\n",
        "        \"id\":\"prompt_patterns\",\n",
        "        \"name\":\"Prompt Patterns\",\n",
        "        \"domain\":\"ai\",\n",
        "        \"locale\":\"ANY\",\n",
        "        \"tags\":[\"prompting\",\"few-shot\",\"structured\",\"guardrails\"],\n",
        "        \"banned_phrases\":[\"All of the above\",\"None of the above\",\"very\",\"actually\",\"really\"],\n",
        "        \"prompt_extra\":\"Write stems that test specific patterns (zero-shot, few-shot, JSON structuring, tool calls).\",\n",
        "        \"categories\":{\"Zero/Few-shot\":2,\"Structure\":2,\"Guardrails\":2,\"Self-consistency\":2},\n",
        "        \"facts\":[\n",
        "            {\"id\":\"PRM-ZS-001\",\"text\":\"Zero-shot prompts give no examples; rely on clear instructions.\"},\n",
        "            {\"id\":\"PRM-FS-002\",\"text\":\"Few-shot prompts provide examples to steer the style and format.\"},\n",
        "            {\"id\":\"PRM-STR-003\",\"text\":\"Structured outputs reduce parsing errors and improve reliability.\"},\n",
        "            {\"id\":\"PRM-GRD-004\",\"text\":\"Guardrails are constraints that restrict style or content.\"},\n",
        "            {\"id\":\"PRM-SC-005\",\"text\":\"Self-consistency samples multiple chains and picks the most common answer.\"},\n",
        "            {\"id\":\"PRM-ERR-006\",\"text\":\"Explicit error handling prompts the model to retry on invalid output.\"},\n",
        "            {\"id\":\"PRM-ROLE-007\",\"text\":\"Role priming changes tone but cannot guarantee truthfulness.\"},\n",
        "            {\"id\":\"PRM-CMP-008\",\"text\":\"Compression prompts ask the model to summarize before answering.\"},\n",
        "        ],\n",
        "    },\n",
        "    \"llmops_evals\": {\n",
        "        \"id\":\"llmops_evals\",\n",
        "        \"name\":\"LLMOps & Evaluation\",\n",
        "        \"domain\":\"ai\",\n",
        "        \"locale\":\"ANY\",\n",
        "        \"tags\":[\"evals\",\"observability\",\"regression\",\"canary\"],\n",
        "        \"banned_phrases\":[\"All of the above\",\"None of the above\"],\n",
        "        \"prompt_extra\":\"Prefer practice-ready items: regression tests, offline evals, golden sets, canary traffic.\",\n",
        "        \"categories\":{\"Offline eval\":2,\"Golden sets\":2,\"Observability\":2,\"Guardrails\":2},\n",
        "        \"facts\":[\n",
        "            {\"id\":\"OPS-REG-001\",\"text\":\"Regression tests catch quality drops from model or prompt changes.\"},\n",
        "            {\"id\":\"OPS-GS-002\",\"text\":\"Golden sets are labeled examples used for repeatable evaluation.\"},\n",
        "            {\"id\":\"OPS-CAN-003\",\"text\":\"Canary deploys route a small percent of traffic to new versions.\"},\n",
        "            {\"id\":\"OPS-MET-004\",\"text\":\"Task-aligned metrics beat generic scores for product fit.\"},\n",
        "            {\"id\":\"OPS-LAT-005\",\"text\":\"Latency and cost must be tracked alongside quality metrics.\"},\n",
        "        ],\n",
        "    },\n",
        "    \"inference_quant\": {\n",
        "        \"id\":\"inference_quant\",\n",
        "        \"name\":\"Inference & Quantization\",\n",
        "        \"domain\":\"ai\",\n",
        "        \"locale\":\"ANY\",\n",
        "        \"tags\":[\"vllm\",\"kv-cache\",\"quantization\",\"throughput\"],\n",
        "        \"banned_phrases\":[\"All of the above\",\"None of the above\"],\n",
        "        \"prompt_extra\":\"Make trade-offs explicit: speed, memory, quality. Prefer concrete options.\",\n",
        "        \"categories\":{\"Serving\":2,\"Caching\":2,\"Quantization\":2,\"Perf\":2},\n",
        "        \"facts\":[\n",
        "            {\"id\":\"INF-KV-001\",\"text\":\"KV cache stores past keys/values to avoid recomputation.\"},\n",
        "            {\"id\":\"INF-BS-002\",\"text\":\"Larger batch sizes improve throughput but can raise latency.\"},\n",
        "            {\"id\":\"INF-SPC-003\",\"text\":\"Speculative decoding drafts tokens then verifies them for speed.\"},\n",
        "            {\"id\":\"INF-QNT-004\",\"text\":\"Quantization shrinks weights (e.g., 8-bit) to save memory and speed up.\"},\n",
        "            {\"id\":\"INF-DIS-005\",\"text\":\"Distillation trains a smaller model to mimic a larger teacher.\"},\n",
        "        ],\n",
        "    },\n",
        "    \"responsible_ai_lite\": {\n",
        "        \"id\":\"responsible_ai_lite\",\n",
        "        \"name\":\"Responsible AI (Lite)\",\n",
        "        \"domain\":\"ai\",\n",
        "        \"locale\":\"ANY\",\n",
        "        \"tags\":[\"safety\",\"privacy\",\"governance\"],\n",
        "        \"banned_phrases\":[\"All of the above\",\"None of the above\"],\n",
        "        \"prompt_extra\":\"Neutral, non-legal tone. No promises of compliance.\",\n",
        "        \"categories\":{\"Risks\":2,\"Privacy\":2,\"Guardrails\":2,\"Transparency\":2},\n",
        "        \"facts\":[\n",
        "            {\"id\":\"RAI-PRV-001\",\"text\":\"Minimize PII; avoid logging sensitive user inputs when unnecessary.\"},\n",
        "            {\"id\":\"RAI-RSK-002\",\"text\":\"Risk assessment considers impact, likelihood, and mitigations.\"},\n",
        "            {\"id\":\"RAI-GRD-003\",\"text\":\"Guardrails reduce unsafe outputs but do not guarantee safety.\"},\n",
        "            {\"id\":\"RAI-DAT-004\",\"text\":\"Data retention should match purpose and regulatory requirements.\"},\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "# ----- Subtopics per pack (used in Standards mode) -----\n",
        "SUBTOPICS: Dict[str, List[str]] = {\n",
        "    \"llm_101\": [\n",
        "        \"Tokens\",\n",
        "        \"Context window limits\",\n",
        "        \"Sampling (temperature, top-p)\",\n",
        "        \"Embeddings basics\",\n",
        "        \"Prompt vs Fine-tune\",\n",
        "        \"Tool / function calling\",\n",
        "        \"Hallucinations & grounding\",\n",
        "        \"System prompts\",\n",
        "        \"Chain-of-thought & privacy\",\n",
        "        \"Evaluation basics\",\n",
        "    ],\n",
        "    \"rag_essentials\": [\n",
        "        \"Chunking strategies\",\n",
        "        \"Overlap / stride\",\n",
        "        \"Dense vs. BM25\",\n",
        "        \"Hybrid retrieval\",\n",
        "        \"Reranking models\",\n",
        "        \"Query rewriting / expansion\",\n",
        "        \"Deduping near-duplicates\",\n",
        "        \"Freshness / recency\",\n",
        "        \"Citation patterns\",\n",
        "        \"Metrics (P@k / MRR / nDCG)\",\n",
        "        \"k tuning & noise\",\n",
        "    ],\n",
        "    \"prompt_patterns\": [\n",
        "        \"Zero-shot\",\n",
        "        \"Few-shot (style steering)\",\n",
        "        \"Structured outputs / JSON\",\n",
        "        \"Error handling & retries\",\n",
        "        \"Role prompting\",\n",
        "        \"Self-consistency\",\n",
        "        \"Delimiters & format guards\",\n",
        "        \"Compression / summarize-then-answer\",\n",
        "        \"Tool-calling scaffolds\",\n",
        "    ],\n",
        "    \"llmops_evals\": [\n",
        "        \"Golden set curation\",\n",
        "        \"Offline eval pipelines\",\n",
        "        \"Prompt regression tests\",\n",
        "        \"Canary / LB experiments\",\n",
        "        \"Observability (quality / latency / cost)\",\n",
        "        \"Drift & guardrail monitoring\",\n",
        "        \"Red-teaming & failure taxonomies\",\n",
        "        \"Experiment tracking\",\n",
        "    ],\n",
        "    \"inference_quant\": [\n",
        "        \"Serving stacks (vLLM / TGI)\",\n",
        "        \"KV-cache management\",\n",
        "        \"Batching / dynamic batching\",\n",
        "        \"Speculative decoding\",\n",
        "        \"Parallelism (TP / PP)\",\n",
        "        \"Prompt caching\",\n",
        "        \"Quant methods (INT8 / FP8 / AWQ / GPTQ)\",\n",
        "        \"Distillation\",\n",
        "        \"Throughput / latency trade-offs\",\n",
        "    ],\n",
        "    \"responsible_ai_lite\": [\n",
        "        \"Privacy & PII minimization\",\n",
        "        \"Data retention\",\n",
        "        \"Transparency / model cards\",\n",
        "        \"Human oversight & fallback\",\n",
        "        \"Bias assessment & mitigation\",\n",
        "        \"Safety filters & rate-limits\",\n",
        "        \"Incident response\",\n",
        "        \"Access control & audit logging\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# ---- Facet registries (diversity prompts) ----\n",
        "DEFAULT_FACETS = [\n",
        "    \"definition/terminology\", \"core mechanism\", \"use cases\", \"trade-offs\",\n",
        "    \"metrics & evaluation\", \"failure modes\", \"edge cases\",\n",
        "    \"best practices\", \"privacy/safety\", \"mini scenario\"\n",
        "]\n",
        "\n",
        "FACETS_BY_PACK: Dict[str, List[str]] = {\n",
        "    \"llm_101\": [\n",
        "        \"tokens vs words\", \"context window budgeting\", \"temperature vs top-p\",\n",
        "        \"embeddings use cases\", \"prompting vs fine-tuning\",\n",
        "        \"tool/function calling contracts\", \"system prompt scope\",\n",
        "        \"hallucination diagnosis\", \"basic evaluation signals\"\n",
        "    ],\n",
        "    \"rag_essentials\": [\n",
        "        \"chunk size & overlap\", \"BM25 vs dense retrieval\", \"hybrid weighting\",\n",
        "        \"reranker precision@k\", \"query rewrite/expansion\", \"deduping & clustering\",\n",
        "        \"freshness filters\", \"citation granularity\", \"k tuning & noise\",\n",
        "        \"eval metrics (MRR/nDCG/Recall@k)\"\n",
        "    ],\n",
        "    \"prompt_patterns\": [\n",
        "        \"zero-shot clarity\", \"few-shot selection\", \"JSON schemas/structure\",\n",
        "        \"error handling & retries\", \"role/tone control\", \"self-consistency\",\n",
        "        \"delimiters & format guards\", \"compression (summarize-then-answer)\",\n",
        "        \"tool-calling argument shaping\"\n",
        "    ],\n",
        "    \"llmops_evals\": [\n",
        "        \"golden set design\", \"offline eval pipeline\", \"prompt regression gates\",\n",
        "        \"canary rollout % & guardrails\", \"quality/latency/cost SLOs\",\n",
        "        \"drift monitoring\", \"red-teaming & failure taxonomies\",\n",
        "        \"experiment tracking\"\n",
        "    ],\n",
        "    \"inference_quant\": [\n",
        "        \"serving stack (vLLM/TGI)\", \"KV-cache limits\", \"batching strategies\",\n",
        "        \"speculative decoding\", \"TP/PP schemes\", \"prompt caching\",\n",
        "        \"quant (INT8/FP8/AWQ/GPTQ) trade-offs\", \"distillation\",\n",
        "        \"throughput vs latency tuning\"\n",
        "    ],\n",
        "    \"responsible_ai_lite\": [\n",
        "        \"risk assessment (impact/likelihood)\", \"privacy & PII minimization\",\n",
        "        \"transparency/model cards\", \"human oversight & fallback\",\n",
        "        \"guardrails (input/output)\", \"incident response\",\n",
        "        \"access control & audit logging\", \"data retention policy\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Optional: subtopic-specific overrides (only where you see repetition)\n",
        "FACETS_BY_SUBTOPIC: Dict[str, Dict[str, List[str]]] = {\n",
        "    \"responsible_ai_lite\": {\n",
        "        \"Privacy & PII minimization\": [\n",
        "            \"data minimization vs purpose\",\n",
        "            \"collection vs processing trade-offs\",\n",
        "            \"anonymization vs pseudonymization\",\n",
        "            \"access control & least privilege\",\n",
        "            \"retention & deletion triggers\",\n",
        "            \"consent scope vs necessity\",\n",
        "            \"PII in logs/telemetry\",\n",
        "            \"privacy-by-default examples\",\n",
        "            \"edge-case PII (free text/images)\",\n",
        "            \"privacy incident playbook\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "def facets_for(pack_id: str, subtopic_label: str) -> List[str]:\n",
        "    # subtopic → pack → default\n",
        "    if pack_id and subtopic_label:\n",
        "        sub_map = FACETS_BY_SUBTOPIC.get(pack_id, {})\n",
        "        if subtopic_label in sub_map:\n",
        "            return sub_map[subtopic_label]\n",
        "    if pack_id and pack_id in FACETS_BY_PACK:\n",
        "        return FACETS_BY_PACK[pack_id]\n",
        "    return DEFAULT_FACETS\n",
        "\n",
        "def subtopics_for_pack(pack_id: str) -> List[str]:\n",
        "    return SUBTOPICS.get(pack_id, [])\n",
        "\n",
        "# Quick topic presets → map to packs\n",
        "TOPIC_PRESETS = [\n",
        "    {\"label\":\"LLM Fundamentals\",     \"pack\":\"llm_101\"},\n",
        "    {\"label\":\"RAG Essentials\",       \"pack\":\"rag_essentials\"},\n",
        "    {\"label\":\"Prompt Patterns\",      \"pack\":\"prompt_patterns\"},\n",
        "    {\"label\":\"LLMOps & Evaluation\",  \"pack\":\"llmops_evals\"},\n",
        "    {\"label\":\"Inference & Quant\",    \"pack\":\"inference_quant\"},\n",
        "    {\"label\":\"Responsible AI (Lite)\",\"pack\":\"responsible_ai_lite\"},\n",
        "]\n",
        "TOPIC_PRESET_CHOICES = [\"— choose —\"] + [t[\"label\"] for t in TOPIC_PRESETS]\n",
        "PACK_CHOICES = [\"\"] + list(PACKS.keys())  # \"\" means freeform\n",
        "\n",
        "# ---------------- Standards registry (expanded, curated per pack) ----------------\n",
        "# Names are high-level guides only. We paraphrase—no verbatim text; not legal advice.\n",
        "\n",
        "_COMMON_STANDARDS = {\n",
        "    \"Consolidated (default)\": {\n",
        "        \"id\": \"consolidated\",\n",
        "        \"name\": \"Consolidated (NIST AI RMF + ISO/IEC 23894 + OECD principles)\",\n",
        "        \"guide\": \"Blend widely used AI standards; use plain, consistent terms. Paraphrase only.\"\n",
        "    },\n",
        "    \"NIST AI RMF 1.0\": {\n",
        "        \"id\": \"nist_ai_rmf_1_0\",\n",
        "        \"name\": \"NIST AI Risk Management Framework 1.0\",\n",
        "        \"guide\": \"Emphasize Govern, Map, Measure, Manage; document context, risks, metrics.\"\n",
        "    },\n",
        "    \"ISO/IEC 23894:2023\": {\n",
        "        \"id\": \"iso_23894_2023\",\n",
        "        \"name\": \"ISO/IEC 23894:2023 (AI risk management)\",\n",
        "        \"guide\": \"Risk management lifecycle; controls as mitigations; governance & documentation.\"\n",
        "    },\n",
        "    \"ISO/IEC 42001:2023\": {\n",
        "        \"id\": \"iso_42001_2023\",\n",
        "        \"name\": \"ISO/IEC 42001:2023 (AI management system — AIMS)\",\n",
        "        \"guide\": \"Org-level processes, policy, competence, continual improvement for AI.\"\n",
        "    },\n",
        "    \"ISO/IEC 22989:2022\": {\n",
        "        \"id\": \"iso_22989_2022\",\n",
        "        \"name\": \"ISO/IEC 22989:2022 (AI concepts & terminology)\",\n",
        "        \"guide\": \"Prefer canonical definitions for core AI terms; avoid vendor marketing jargon.\"\n",
        "    },\n",
        "    \"ISO/IEC 23053:2022\": {\n",
        "        \"id\": \"iso_23053_2022\",\n",
        "        \"name\": \"ISO/IEC 23053:2022 (Framework for AI systems using ML)\",\n",
        "        \"guide\": \"Structure lifecycle: data, training, evaluation, deployment, operation, retirement.\"\n",
        "    },\n",
        "    \"ISO/IEC TR 24027:2021\": {\n",
        "        \"id\": \"iso_tr_24027_2021\",\n",
        "        \"name\": \"ISO/IEC TR 24027:2021 (Bias in AI systems)\",\n",
        "        \"guide\": \"Identify/mitigate data & model bias; discuss measurement limits and documentation.\"\n",
        "    },\n",
        "    \"ISO/IEC TR 24028:2020\": {\n",
        "        \"id\": \"iso_tr_24028_2020\",\n",
        "        \"name\": \"ISO/IEC TR 24028:2020 (Trustworthiness of AI)\",\n",
        "        \"guide\": \"High-level attributes: reliability, robustness, safety, security, privacy, accountability.\"\n",
        "    },\n",
        "    \"OECD AI Principles\": {\n",
        "        \"id\": \"oecd_ai\",\n",
        "        \"name\": \"OECD AI Principles\",\n",
        "        \"guide\": \"Human-centered values, robustness, transparency, accountability; high-level phrasing.\"\n",
        "    },\n",
        "    \"EU AI Act (lite summary)\": {\n",
        "        \"id\": \"eu_ai_act_lite\",\n",
        "        \"name\": \"EU AI Act (high-level)\",\n",
        "        \"guide\": \"Risk-based categories (unacceptable, high, limited, minimal). Paraphrase only.\"\n",
        "    },\n",
        "    # Engineering/quality sources (non-normative but useful)\n",
        "    \"ISO/IEC 25010 (quality model)\": {\n",
        "        \"id\": \"iso_25010\",\n",
        "        \"name\": \"ISO/IEC 25010 (software product quality)\",\n",
        "        \"guide\": \"Quality characteristics (reliability, performance, security, usability) in evals.\"\n",
        "    },\n",
        "    \"ISO/IEC/IEEE 29119 (software testing)\": {\n",
        "        \"id\": \"iso_ieee_29119\",\n",
        "        \"name\": \"ISO/IEC/IEEE 29119 (software testing)\",\n",
        "        \"guide\": \"Test design & evidence; traceability from requirements to test cases & results.\"\n",
        "    },\n",
        "    \"IEEE 1012 (V&V)\": {\n",
        "        \"id\": \"ieee_1012\",\n",
        "        \"name\": \"IEEE 1012 (Verification & Validation)\",\n",
        "        \"guide\": \"Independent V&V mindset; evidence for correctness and fitness for purpose.\"\n",
        "    },\n",
        "    \"MLPerf Inference (benchmark)\": {\n",
        "        \"id\": \"mlperf_inference\",\n",
        "        \"name\": \"MLPerf Inference (MLCommons benchmark)\",\n",
        "        \"guide\": \"Use standardized perf metrics (latency, throughput, accuracy) & reproducible runs. Treat as benchmarking guidance, not a formal standard.\"\n",
        "    },\n",
        "}\n",
        "\n",
        "# Curated menus per pack\n",
        "STANDARDS_BY_PACK: Dict[str, Dict[str, dict]] = {\n",
        "    \"llm_101\": {\n",
        "        \"Consolidated (default)\": _COMMON_STANDARDS[\"Consolidated (default)\"],\n",
        "        \"ISO/IEC 22989:2022\":     _COMMON_STANDARDS[\"ISO/IEC 22989:2022\"],\n",
        "        \"ISO/IEC 23053:2022\":     _COMMON_STANDARDS[\"ISO/IEC 23053:2022\"],\n",
        "        \"NIST AI RMF 1.0\":        _COMMON_STANDARDS[\"NIST AI RMF 1.0\"],\n",
        "        \"ISO/IEC 23894:2023\":     _COMMON_STANDARDS[\"ISO/IEC 23894:2023\"],\n",
        "    },\n",
        "    \"rag_essentials\": {\n",
        "        \"Consolidated (default)\": _COMMON_STANDARDS[\"Consolidated (default)\"],\n",
        "        \"ISO/IEC 23053:2022\":     _COMMON_STANDARDS[\"ISO/IEC 23053:2022\"],\n",
        "        \"ISO/IEC 22989:2022\":     _COMMON_STANDARDS[\"ISO/IEC 22989:2022\"],\n",
        "        \"ISO/IEC 23894:2023\":     _COMMON_STANDARDS[\"ISO/IEC 23894:2023\"],\n",
        "        \"ISO/IEC 25010 (quality model)\": _COMMON_STANDARDS[\"ISO/IEC 25010 (quality model)\"],\n",
        "    },\n",
        "    \"prompt_patterns\": {\n",
        "        \"Consolidated (default)\": _COMMON_STANDARDS[\"Consolidated (default)\"],\n",
        "        \"ISO/IEC 22989:2022\":     _COMMON_STANDARDS[\"ISO/IEC 22989:2022\"],\n",
        "        \"ISO/IEC TR 24027:2021\":  _COMMON_STANDARDS[\"ISO/IEC TR 24027:2021\"],\n",
        "        \"ISO/IEC TR 24028:2020\":  _COMMON_STANDARDS[\"ISO/IEC TR 24028:2020\"],\n",
        "        \"NIST AI RMF 1.0\":        _COMMON_STANDARDS[\"NIST AI RMF 1.0\"],\n",
        "    },\n",
        "    \"llmops_evals\": {\n",
        "        \"Consolidated (default)\": _COMMON_STANDARDS[\"Consolidated (default)\"],\n",
        "        \"ISO/IEC 25010 (quality model)\": _COMMON_STANDARDS[\"ISO/IEC 25010 (quality model)\"],\n",
        "        \"ISO/IEC/IEEE 29119 (software testing)\": _COMMON_STANDARDS[\"ISO/IEC/IEEE 29119 (software testing)\"],\n",
        "        \"IEEE 1012 (V&V)\":        _COMMON_STANDARDS[\"IEEE 1012 (V&V)\"],\n",
        "        \"NIST AI RMF 1.0\":        _COMMON_STANDARDS[\"NIST AI RMF 1.0\"],\n",
        "        \"ISO/IEC 23894:2023\":     _COMMON_STANDARDS[\"ISO/IEC 23894:2023\"],\n",
        "    },\n",
        "    \"inference_quant\": {\n",
        "        \"Consolidated (default)\": _COMMON_STANDARDS[\"Consolidated (default)\"],\n",
        "        \"ISO/IEC 23053:2022\":     _COMMON_STANDARDS[\"ISO/IEC 23053:2022\"],\n",
        "        \"ISO/IEC 22989:2022\":     _COMMON_STANDARDS[\"ISO/IEC 22989:2022\"],\n",
        "        \"MLPerf Inference (benchmark)\": _COMMON_STANDARDS[\"MLPerf Inference (benchmark)\"],\n",
        "    },\n",
        "    \"responsible_ai_lite\": {\n",
        "        \"Consolidated (default)\": _COMMON_STANDARDS[\"Consolidated (default)\"],\n",
        "        \"NIST AI RMF 1.0\":        _COMMON_STANDARDS[\"NIST AI RMF 1.0\"],\n",
        "        \"ISO/IEC 23894:2023\":     _COMMON_STANDARDS[\"ISO/IEC 23894:2023\"],\n",
        "        \"ISO/IEC 42001:2023\":     _COMMON_STANDARDS[\"ISO/IEC 42001:2023\"],\n",
        "        \"OECD AI Principles\":     _COMMON_STANDARDS[\"OECD AI Principles\"],\n",
        "        \"EU AI Act (lite summary)\": _COMMON_STANDARDS[\"EU AI Act (lite summary)\"],\n",
        "        \"ISO/IEC TR 24027:2021\":  _COMMON_STANDARDS[\"ISO/IEC TR 24027:2021\"],\n",
        "        \"ISO/IEC TR 24028:2020\":  _COMMON_STANDARDS[\"ISO/IEC TR 24028:2020\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "def standards_choices_for(pack_id: str) -> List[str]:\n",
        "    table = STANDARDS_BY_PACK.get(pack_id) or _COMMON_STANDARDS\n",
        "    return list(table.keys())\n",
        "\n",
        "def standards_meta(pack_id: str, choice: str) -> Optional[dict]:\n",
        "    table = STANDARDS_BY_PACK.get(pack_id) or _COMMON_STANDARDS\n",
        "    return table.get(choice) or None\n",
        "\n",
        "# Per-pack default standard for nicer UX\n",
        "DEFAULT_STANDARD_BY_PACK = {\n",
        "    \"llm_101\":             \"ISO/IEC 22989:2022\",\n",
        "    \"rag_essentials\":      \"ISO/IEC 23053:2022\",\n",
        "    \"prompt_patterns\":     \"ISO/IEC TR 24027:2021\",\n",
        "    \"llmops_evals\":        \"ISO/IEC 25010 (quality model)\",\n",
        "    \"inference_quant\":     \"MLPerf Inference (benchmark)\",\n",
        "    \"responsible_ai_lite\": \"NIST AI RMF 1.0\",\n",
        "}\n",
        "def default_standard_for(pack_id: str) -> str:\n",
        "    return DEFAULT_STANDARD_BY_PACK.get(pack_id, \"Consolidated (default)\")\n",
        "\n",
        "def compose_topic_display(preset_label: str, pack_id: str, subtopic: str, focus: str) -> str:\n",
        "    base = preset_label if (preset_label and preset_label != \"— choose —\") else (PACKS.get(pack_id, {}) or {}).get(\"name\", \"\")\n",
        "    title = base\n",
        "    if subtopic and subtopic != \"— none —\":\n",
        "        title = f\"{subtopic} ({base})\" if base else subtopic\n",
        "    if focus and focus.strip():\n",
        "        title = f\"{title} — focus: {focus.strip()}\" if title else f\"Focus: {focus.strip()}\"\n",
        "    return (title or \"\").strip()\n",
        "\n",
        "# --- token overlap helpers for packs ---\n",
        "_WORDS = re.compile(r\"[A-Za-z0-9_]+\")\n",
        "\n",
        "def _topic_tokens(s: str) -> set:\n",
        "    return set(w.lower() for w in _WORDS.findall(s or \"\"))\n",
        "\n",
        "def _facts_k(count: int, difficulty: int, strict: bool) -> int:\n",
        "    base = 10\n",
        "    base += 2 * min(count, 4)   # up to +8\n",
        "    if difficulty >= 7:\n",
        "        base += 2\n",
        "    if strict:\n",
        "        base += 2\n",
        "    return max(6, min(base, 14))  # cap much lower than before\n",
        "\n",
        "def select_facts_slice(topic: str, pack: dict, k: int = 10) -> list:\n",
        "    if not pack or not pack.get(\"facts\"):\n",
        "        return []\n",
        "    tt = _topic_tokens(topic)\n",
        "    scored = []\n",
        "    for f in pack[\"facts\"]:\n",
        "        ft = _topic_tokens(f[\"text\"])\n",
        "        score = len(tt & ft)\n",
        "        scored.append((score, f))\n",
        "    scored.sort(key=lambda x: (-x[0], x[1][\"id\"]))\n",
        "    out = [f for sc,f in scored[:k]]\n",
        "    if not out:\n",
        "        out = pack[\"facts\"][:min(k, len(pack[\"facts\"]))]  # fallback: first k\n",
        "    return out\n",
        "\n",
        "def build_source_alignment_addendum(pack_id: str, source_choice: str) -> str:\n",
        "    meta = standards_meta(pack_id, source_choice) if source_choice else None\n",
        "    if not meta or meta[\"id\"] == \"consolidated\":\n",
        "        return (\"STANDARDS ALIGNMENT: Blend overlapping principles from common AI standards \"\n",
        "                \"(e.g., NIST AI RMF, ISO/IEC 23894, OECD). Use consistent, plain terminology. \"\n",
        "                \"Do not copy wording; paraphrase at a high level.\")\n",
        "    return (\n",
        "        f\"STANDARDS ALIGNMENT — Base on {meta['name']}. {meta.get('guide','').strip()} \"\n",
        "        \"Do not quote or reproduce text verbatim; paraphrase succinctly.\"\n",
        "    )\n",
        "\n",
        "def build_standard_addendum(pack: dict, strict: bool, facts_slice: list, source_addendum: str) -> str:\n",
        "    if not pack:\n",
        "        return source_addendum.strip()\n",
        "    lines = []\n",
        "    if pack.get(\"prompt_extra\"):\n",
        "        lines.append(f\"GUIDE: {pack['prompt_extra']}\")\n",
        "    if pack.get(\"banned_phrases\"):\n",
        "        bp = \"; \".join(pack[\"banned_phrases\"])\n",
        "        lines.append(f\"BAN these phrases in stems/options: {bp}\")\n",
        "    if source_addendum:\n",
        "        lines.append(source_addendum)\n",
        "\n",
        "    if facts_slice:\n",
        "        show = facts_slice[:12] if strict else facts_slice[:8]  # cap visible facts\n",
        "        if strict:\n",
        "            lines.append(\"STRICT FACTS MODE — Use ONLY the facts below as your knowledge base; do not add external info.\")\n",
        "            lines.append(\"For each question include a 'citations' array of fact IDs used.\")\n",
        "            lines.append(\"Allowed facts:\")\n",
        "            for f in show:\n",
        "                lines.append(f\"[{f['id']}] {f['text']}\")\n",
        "        else:\n",
        "            lines.append(\"PREFER these facts where relevant (do not force citations):\")\n",
        "            for f in show:\n",
        "                lines.append(f\"- {f['text']}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def validate_strict_items(qitems: list, allowed_ids: set) -> list:\n",
        "    out = []\n",
        "    for q in qitems:\n",
        "        cites = q.get(\"citations\") or q.get(\"refs\") or []\n",
        "        if not isinstance(cites, list):\n",
        "            continue\n",
        "        cites = [c for c in cites if isinstance(c, str)]\n",
        "        if not any(c in allowed_ids for c in cites):\n",
        "            continue\n",
        "        out.append(q)\n",
        "    return out\n",
        "\n",
        "# ---------- text compaction ----------\n",
        "_FILLER = [\n",
        "    r\"\\bthat\\b\", r\"\\bvery\\b\", r\"\\bactually\\b\", r\"\\breally\\b\",\n",
        "    r\"\\bjust\\b\", r\"\\bkind of\\b\", r\"\\bsort of\\b\", r\"\\bin order to\\b\",\n",
        "]\n",
        "def _squash_spaces(s: str) -> str:\n",
        "    return _re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
        "def _kill_filler(s: str) -> str:\n",
        "    for pat in _FILLER:\n",
        "        s = _re.sub(pat, \"\", s, flags=_re.IGNORECASE)\n",
        "    return _squash_spaces(s)\n",
        "def _tighten_punct(s: str) -> str:\n",
        "    if not s: return s\n",
        "    s = _re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n",
        "    s = _re.sub(r\"\\(([^)]{0,20})\\)\", r\"\\1\", s)  # <-- fixed char class\n",
        "    s = _re.sub(r\"--+\", \"–\", s)\n",
        "    return _squash_spaces(s)\n",
        "\n",
        "def _smart_clip(s: str, n: int, add_ellipsis: bool = True) -> Tuple[str, bool]:\n",
        "    s = (s or \"\").strip()\n",
        "    if len(s) <= n:\n",
        "        return s, False\n",
        "    cut = s[:n+1]\n",
        "    m = _re.search(r\"[.;:!?]\\s+\\S*$\", cut)\n",
        "    if m:\n",
        "        out = cut[:m.start()].rstrip()\n",
        "        return (out + \"…\", True) if add_ellipsis else (out, True)\n",
        "    m = _re.search(r\"\\s+\\S*$\", cut)\n",
        "    if m:\n",
        "        out = cut[:m.start()].rstrip()\n",
        "        return (out + \"…\", True) if add_ellipsis else (out, True)\n",
        "    out = s[:n].rstrip()\n",
        "    return (out + \"…\", True) if add_ellipsis else (out, True)\n",
        "\n",
        "_ABBR_REPL = [\n",
        "    (r\"\\bapproximately\\b\", \"~\"), (r\"\\babout\\b\", \"~\"), (r\"\\baround\\b\", \"~\"),\n",
        "    (r\"\\bversus\\b\", \"vs\"), (r\"\\band\\b\", \"&\"), (r\"\\bpercent\\b\", \"%\"),\n",
        "    (r\"\\bper\\s+cent\\b\", \"%\"), (r\"\\byears?\\b\", \"yrs\"), (r\"\\bminutes?\\b\", \"min\"),\n",
        "    (r\"\\bhours?\\b\", \"h\"), (r\"\\bseconds?\\b\", \"s\"), (r\"\\bmillion\\b\", \"M\"),\n",
        "    (r\"\\bbillion\\b\", \"B\"), (r\"\\bUnited States\\b\", \"US\"), (r\"\\bUnited Kingdom\\b\", \"UK\"),\n",
        "    (r\"\\bkilometers per hour\\b\", \"km/h\"), (r\"\\bkilometres per hour\\b\", \"km/h\"), (r\"\\bmiles per hour\\b\", \"mph\"),\n",
        "]\n",
        "_MONTHS = {\"January\":\"Jan\",\"February\":\"Feb\",\"March\":\"Mar\",\"April\":\"Apr\",\"June\":\"Jun\",\n",
        "    \"July\":\"Jul\",\"August\":\"Aug\",\"September\":\"Sep\",\"October\":\"Oct\",\"November\":\"Nov\",\"December\":\"Dec\",\"May\":\"May\"}\n",
        "def _abbr_pass(s: str) -> str:\n",
        "    t = s\n",
        "    for pat, rep in _ABBR_REPL:\n",
        "        t = _re.sub(pat, rep, t, flags=_re.IGNORECASE)\n",
        "    for long, short in _MONTHS.items():\n",
        "        t = _re.sub(rf\"\\b{long}\\b\", short, t)\n",
        "    t = _re.sub(r\"\\b(\\d+)\\s+yrs?\\b\", r\"\\1 yrs\", t)\n",
        "    t = _re.sub(r\"\\b(\\d+)\\s+minutes?\\b\", r\"\\1 min\", t)\n",
        "    t = _re.sub(r\"\\b(\\d+)\\s+hours?\\b\", r\"\\1 h\", t)\n",
        "    return _squash_spaces(t)\n",
        "def _prune_clauses(s: str) -> str:\n",
        "    t = s\n",
        "    t = _re.sub(r\",\\s+(which|that|who|when|where)\\b.*$\", \"\", t, flags=_re.IGNORECASE)\n",
        "    t = _re.sub(r\"\\s+[–—-]\\s+.*$\", \"\", t)\n",
        "    t = _re.sub(r\":\\s+.*$\", \"\", t)\n",
        "    return _squash_spaces(t)\n",
        "def _prune_clauses_question(s: str) -> str:\n",
        "    t = s\n",
        "    t = _re.sub(r\",\\s+(which|that|who)\\s+(is|are|was|were)\\b.*$\", \"\", t, flags=_re.IGNORECASE)\n",
        "    t = _re.sub(r\",\\s+(when|where)\\s+(it|they)\\s+(is|are|was|were)\\b.*$\", \"\", t, flags=_re.IGNORECASE)\n",
        "    t = _re.sub(r\"\\s+[–—-]\\s+.*$\", \"\", t)\n",
        "    t = _re.sub(r\":\\s+.*$\", \"\", t)\n",
        "    return _squash_spaces(t)\n",
        "def compact_to(s: str, n: int, add_ellipsis: bool = True, *, prune_mode: str = \"generic\") -> Tuple[str, bool]:\n",
        "    before = s\n",
        "    t = _squash_spaces(s)\n",
        "    t = _tighten_punct(t)\n",
        "    t = _kill_filler(t)\n",
        "    if len(t) <= n:\n",
        "        return t, len(t) != len(before)\n",
        "    t2 = _abbr_pass(t)\n",
        "    if len(t2) <= n:\n",
        "        return t2, True\n",
        "    if prune_mode == \"question\":\n",
        "        t3 = _prune_clauses_question(t2)\n",
        "    elif prune_mode == \"generic\":\n",
        "        t3 = _prune_clauses(t2)\n",
        "    else:\n",
        "        t3 = t2\n",
        "    if len(t3) <= n:\n",
        "        return t3, True\n",
        "    return _smart_clip(t3, n, add_ellipsis=add_ellipsis)\n",
        "def _finish_sentence(t: str, *, is_question: bool) -> str:\n",
        "    import re\n",
        "    t = (t or \"\").strip()\n",
        "    t = re.sub(r\"(,\\s*(which|that|who|when|where)\\b.*)$\", \"\", t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r\"[\\s:;,\\-–—]+$\", \"\", t)\n",
        "    t = re.sub(r\"\\b(such as|including|like|for example|e\\.g\\.)\\s*$\", \"\", t, flags=re.IGNORECASE)\n",
        "    if is_question:\n",
        "        t = t.rstrip(\".\")\n",
        "        if not t.endswith(\"?\"):\n",
        "            t += \"?\"\n",
        "    else:\n",
        "        if not re.search(r\"[.!?]$\", t):\n",
        "            t += \".\"\n",
        "    return t\n",
        "def lint_item_lengths(q: str, options: List[str], exp: str, *, add_ellipsis: bool = False) -> Tuple[str, List[str], str, dict]:\n",
        "    flags = {\"Q\": False, \"A\": False, \"B\": False, \"C\": False, \"D\": False, \"EXP\": False}\n",
        "    q_raw = q or \"\"\n",
        "    q = _re.sub(r\"^\\s*(Which of the following|What of the following|Which statement).*?:\\s*\", \"\", q_raw, flags=_re.IGNORECASE)\n",
        "    q1, qc = compact_to(q, Q_MAX, add_ellipsis=add_ellipsis, prune_mode=\"question\")\n",
        "    flags[\"Q\"] = qc\n",
        "    q1 = _finish_sentence(q1, is_question=True)\n",
        "    looks_stub = (len(q1) < 24) and _re.match(r\"^(in|when|while|during|where)\\b\", q1.strip().lower())\n",
        "    if looks_stub:\n",
        "        q2, _ = compact_to(q_raw, Q_MAX, add_ellipsis=add_ellipsis, prune_mode=\"none\")\n",
        "        q2 = _finish_sentence(q2, is_question=True)\n",
        "        if len(q2) >= len(q1):\n",
        "            q1 = q2\n",
        "    q = q1\n",
        "    outs, letters = [], [\"A\",\"B\",\"C\",\"D\"]\n",
        "    for i, o in enumerate((options + [\"\"]*4)[:4]):\n",
        "        o2, oc = compact_to(o or \"\", OPT_MAX, add_ellipsis=add_ellipsis, prune_mode=\"generic\")\n",
        "        outs.append(o2); flags[letters[i]] = oc\n",
        "    exp1, ec = compact_to(exp or \"\", EXP_MAX, add_ellipsis=add_ellipsis, prune_mode=\"generic\")\n",
        "    flags[\"EXP\"] = ec\n",
        "    exp = _finish_sentence(exp1, is_question=False)\n",
        "    return q, outs, exp, flags\n",
        "\n",
        "# --------- JSON-ish cleanup ----------\n",
        "def _strip_code_fences(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    if s.startswith(\"```\"):\n",
        "        s = re.sub(r\"^```(?:json)?\", \"\", s.strip(), flags=re.IGNORECASE).strip()\n",
        "        s = re.sub(r\"```$\", \"\", s.strip()).strip()\n",
        "    return s\n",
        "def _remove_trailing_commas(s: str) -> str:\n",
        "    s = re.sub(r\",\\s*([}\\]])\", r\"\\1\", s); return s\n",
        "def _strip_json_comments(s: str) -> str:\n",
        "    s = re.sub(r\"(^|\\s)//.*?$\", r\"\\1\", s, flags=re.MULTILINE)\n",
        "    s = re.sub(r\"/\\*.*?\\*/\", \"\", s, flags=re.DOTALL)\n",
        "    return s\n",
        "def _normalize_quotes(s: str) -> str:\n",
        "    return (s or \"\").replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"').replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\")\n",
        "def _clean_jsonish(s: str) -> str:\n",
        "    s = _strip_code_fences(s); s = _normalize_quotes(s); s = _strip_json_comments(s); s = _remove_trailing_commas(s); return s.strip()\n",
        "def _extract_between_markers(s: str):\n",
        "    i = s.find(BEGIN_JSON); j = s.rfind(END_JSON)\n",
        "    if i != -1 and j != -1 and j > i: return s[i+len(BEGIN_JSON):j]\n",
        "    return None\n",
        "def _split_objects_by_brace(text: str) -> list:\n",
        "    body, objs, depth, in_str, esc, start = text, [], 0, False, False, None\n",
        "    for idx, ch in enumerate(body):\n",
        "        if in_str:\n",
        "            if esc: esc = False\n",
        "            elif ch == \"\\\\\": esc = True\n",
        "            elif ch == '\"': in_str = False\n",
        "        else:\n",
        "            if ch == '\"': in_str = True\n",
        "            elif ch == \"{\":\n",
        "                if depth == 0: start = idx\n",
        "                depth += 1\n",
        "            elif ch == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0 and start is not None:\n",
        "                    objs.append(body[start:idx+1]); start = None\n",
        "    return objs\n",
        "def _coerce_objectish(s: str) -> str:\n",
        "    t = (s or \"\").strip()\n",
        "    if not t: return t\n",
        "    if t.startswith(\"{\") or t.startswith(\"[\"): return t\n",
        "    if t.startswith('\"topic\"') or t.startswith('\"difficulty\"') or t.startswith('\"questions\"'):\n",
        "        t = t.rstrip()\n",
        "        if not t.endswith(\"}\"): t = t + \"}\"\n",
        "        return \"{\" + t\n",
        "    return t\n",
        "def _first_json_dict(s: str):\n",
        "    s = _clean_jsonish(s)\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        s2 = _coerce_objectish(s)\n",
        "        if s2 != s:\n",
        "            try: return json.loads(_clean_jsonish(s2))\n",
        "            except Exception: pass\n",
        "        if \"{\" in s and \"}\" in s:\n",
        "            chunk = s[s.find(\"{\"): s.rfind(\"}\")+1]\n",
        "            try: return json.loads(_clean_jsonish(chunk))\n",
        "            except Exception: return None\n",
        "        return None\n",
        "def parse_questions_from_model(s: str) -> list:\n",
        "    between = _extract_between_markers(s)\n",
        "    if between is not None: s = between\n",
        "    s = _coerce_objectish(s)\n",
        "    s_clean = _clean_jsonish(s)\n",
        "    data = _first_json_dict(s_clean)\n",
        "    if isinstance(data, dict):\n",
        "        if isinstance(data.get(\"questions\"), list): return data[\"questions\"]\n",
        "        for k in (\"items\", \"mcqs\", \"data\", \"list\", \"entries\"):\n",
        "            v = data.get(k)\n",
        "            if isinstance(v, list): return v\n",
        "    try:\n",
        "        arr = json.loads(s_clean)\n",
        "        if isinstance(arr, list): return arr\n",
        "    except Exception: pass\n",
        "    objs = _split_objects_by_brace(s_clean)\n",
        "    out = []\n",
        "    for obj in objs:\n",
        "        try:\n",
        "            d = json.loads(_clean_jsonish(obj))\n",
        "        except Exception:\n",
        "            try:\n",
        "                d = json.loads(_clean_jsonish(re.sub(r\"'\", '\"', obj)))\n",
        "            except Exception:\n",
        "                continue\n",
        "        if not isinstance(d, dict): continue\n",
        "        if \"question\" in d and (\"correct_answer\" in d or \"answer\" in d or \"correct\" in d or \"options\" in d):\n",
        "            if \"correct_answer\" not in d:\n",
        "                if \"answer\" in d: d[\"correct_answer\"] = d[\"answer\"]\n",
        "                elif \"correct\" in d: d[\"correct_answer\"] = d[\"correct\"]\n",
        "                elif \"options\" in d and isinstance(d.get(\"correct_index\"), int):\n",
        "                    opts = d.get(\"options\") or []\n",
        "                    ci = d.get(\"correct_index\")\n",
        "                    if 0 <= ci < len(opts):\n",
        "                        d[\"correct_answer\"] = opts[ci]\n",
        "                        d[\"distractors\"] = [o for i, o in enumerate(opts) if i != ci][:3]\n",
        "            out.append(d)\n",
        "    return out\n",
        "\n",
        "# ---------- Font + text helpers ----------\n",
        "FONT_CANDIDATES = [\n",
        "    \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\",\n",
        "    \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
        "    \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\",\n",
        "    \"/usr/share/fonts/truetype/liberation2/LiberationSans-Bold.ttf\",\n",
        "    \"/usr/share/fonts/truetype/freefont/FreeSansBold.ttf\",\n",
        "]\n",
        "def _resolve_font_path() -> str:\n",
        "    for p in FONT_CANDIDATES:\n",
        "        if Path(p).exists(): return p\n",
        "    try:\n",
        "        for p in Path(\"/usr/share/fonts\").rglob(\"*.ttf\"):\n",
        "            return str(p)\n",
        "    except Exception: pass\n",
        "    return \"\"\n",
        "DEFAULT_FONT_PATH = _resolve_font_path()\n",
        "DEFAULT_FONT_EXISTS = bool(DEFAULT_FONT_PATH)\n",
        "OUT_DIR = Path(\"/content/out/videos\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TMP_DIR = Path(\"/content/out/tmp\"); TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "def _font(pt: int) -> ImageFont.FreeTypeFont:\n",
        "    if DEFAULT_FONT_EXISTS: return ImageFont.truetype(DEFAULT_FONT_PATH, pt)\n",
        "    return ImageFont.load_default()\n",
        "def _text_wrap(draw: ImageDraw.ImageDraw, text: str, font: ImageFont.FreeTypeFont, max_w: int):\n",
        "    words = text.split()\n",
        "    lines, cur = [], \"\"\n",
        "    for w in words:\n",
        "        t = (cur + \" \" + w).strip()\n",
        "        bbox = draw.textbbox((0,0), t, font=font)\n",
        "        if bbox[2]-bbox[0] <= max_w or not cur:\n",
        "            cur = t\n",
        "        else:\n",
        "            lines.append(cur); cur = w\n",
        "    if cur: lines.append(cur)\n",
        "    if len(lines) >= 2 and len(lines[-1].split()) <= 2 and len(lines[-2].split()) > 2:\n",
        "        prev = lines[-2].split()\n",
        "        moved = prev.pop()\n",
        "        lines[-2] = \" \".join(prev)\n",
        "        lines[-1] = (moved + \" \" + lines[-1]).strip()\n",
        "    return lines\n",
        "def _draw_text_block(img, box, text, color, max_pt, min_pt, leading_ratio=0.30, stroke=0, stroke_color=(255,255,255), align=\"center\"):\n",
        "    x, y, w, h = box\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for pt in range(max_pt, min_pt-1, -2):\n",
        "        f = _font(pt)\n",
        "        lines = _text_wrap(draw, text, f, w)\n",
        "        line_heights = [draw.textbbox((0,0), ln, font=f)[3] - draw.textbbox((0,0), ln, font=f)[1] for ln in lines]\n",
        "        total_h = sum(line_heights) + int(pt * leading_ratio) * (len(lines)-1)\n",
        "        if total_h <= h:\n",
        "            cur_y = y + (h - total_h)//2\n",
        "            for ln in lines:\n",
        "                bbox = draw.textbbox((0,0), ln, font=f)\n",
        "                tw = bbox[2]-bbox[0]; th = bbox[3]-bbox[1]\n",
        "                if align == \"center\": tx = x + (w - tw)//2\n",
        "                elif align == \"left\": tx = x\n",
        "                else: tx = x + (w - tw)\n",
        "                if stroke > 0:\n",
        "                    draw.text((tx, cur_y), ln, font=f, fill=stroke_color, stroke_width=stroke, stroke_fill=stroke_color)\n",
        "                draw.text((tx, cur_y), ln, font=f, fill=color, stroke_width=stroke, stroke_fill=stroke_color)\n",
        "                cur_y += th + int(pt * leading_ratio)\n",
        "            return\n",
        "    f = _font(min_pt)\n",
        "    bbox = draw.textbbox((0,0), text, font=f); tw = bbox[2]-bbox[0]; th = bbox[3]-bbox[1]\n",
        "    tx = x + (w - tw)//2; ty = y + (h - th)//2\n",
        "    draw.text((tx, ty), text, font=f, fill=color, stroke_width=stroke, stroke_fill=stroke_color)\n",
        "\n",
        "# ---------- Slide renderers ----------\n",
        "def _badge(img: Image.Image):\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.rectangle([0,0,W,8], fill=GREEN)\n",
        "    pill_w, pill_h = 300, 72\n",
        "    px, py = W - pill_w - 36, 36\n",
        "    draw.rounded_rectangle([px, py, px+pill_w, py+pill_h], radius=20, fill=MINT)\n",
        "    _draw_text_block(img, (px+22, py+14, pill_w-44, pill_h-28), \"MyAI101\", DARK, 56, 32, align=\"left\")\n",
        "def _chip(img: Image.Image, y_center: int, text: str):\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    chip_w = W - 2*SAFE_X\n",
        "    x0, y0 = (W - chip_w)//2, y_center - CHIP_H//2\n",
        "    draw.rounded_rectangle([x0, y0, x0+chip_w, y0+CHIP_H], radius=18, fill=CHIP_BG)\n",
        "    draw.rectangle([x0, y0, x0+chip_w, y0+4], fill=GREEN)\n",
        "    _draw_text_block(img, (x0+CHIP_TX_PAD//2, y0+(CHIP_H-CHIP_TX_H)//2, chip_w-CHIP_TX_PAD, CHIP_TX_H),\n",
        "                     text, DARK, max_pt=54, min_pt=30, align=\"left\")\n",
        "def _question_slide(question: str, options: List[str]) -> Image.Image:\n",
        "    img = Image.new(\"RGB\", (W, H), BG)\n",
        "    _badge(img)\n",
        "    title_box = (SAFE_X, SAFE_TOP, W-2*SAFE_X, TITLE_BOX_H)\n",
        "    _draw_text_block(img, title_box, question, DARK, 84, 34, stroke=2, stroke_color=(255,255,255))\n",
        "    title_bottom = SAFE_TOP + TITLE_BOX_H\n",
        "    band_top = max(title_bottom + GAP_BELOW_TITLE_MIN, SAFE_TOP + int(H*0.34))\n",
        "    band_bot = min(H - SAFE_BOT, int(H*0.86))\n",
        "    n = len(options)\n",
        "    if n > 0:\n",
        "        band_h = band_bot - band_top\n",
        "        preferred = n*CHIP_H + (n-1)*CHIP_GAP\n",
        "        if preferred <= band_h:\n",
        "            ys = [band_top + CHIP_H//2 + i*(CHIP_H+CHIP_GAP) for i in range(n)]\n",
        "        else:\n",
        "            gap = max(18, int((band_h - n*CHIP_H) / max(1, n-1)))\n",
        "            ys = [band_top + CHIP_H//2 + i*(CHIP_H + gap) for i in range(n)]\n",
        "        for i, (opt, yc) in enumerate(zip(options, ys)):\n",
        "            _chip(img, yc, f\"{chr(65+i)}. {opt}\")\n",
        "    return img\n",
        "def _reveal_slide(correct: str, explanation: str) -> Image.Image:\n",
        "    img = Image.new(\"RGB\", (W, H), BG)\n",
        "    _badge(img)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    card_w, card_h = W - 2*SAFE_X, 140\n",
        "    x0, y0 = SAFE_X, int(H*0.34)\n",
        "    draw.rounded_rectangle([x0, y0, x0+card_w, y0+card_h], radius=20, fill=MINT)\n",
        "    _draw_text_block(img, (x0+20, y0+16, card_w-40, card_h-32), f\"Answer: {correct}\", DARK, 80, 40)\n",
        "    _draw_text_block(img, (SAFE_X, int(H*0.54), W-2*SAFE_X, int(H*0.28)), explanation, MID, 62, 32)\n",
        "    return img\n",
        "def _cta_slide() -> Image.Image:\n",
        "    img = Image.new(\"RGB\", (W, H), BG)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    _badge(img)\n",
        "    _draw_text_block(img, (SAFE_X, int(H*0.36), W-2*SAFE_X, int(H*0.22)), \"MyAI101\", DARK, 160, 84)\n",
        "    _draw_text_block(img, (SAFE_X, int(H*0.54), W-2*SAFE_X, int(H*0.16)),\n",
        "                     \"Daily AI literacy in 60 seconds\", MID, 72, 38)\n",
        "    bx, by = (W-CTA_BUTTON_W)//2, int(H*0.74)\n",
        "    draw.rounded_rectangle([bx, by, bx+CTA_BUTTON_W, by+CTA_BUTTON_H], radius=22, fill=GREEN)\n",
        "    _draw_text_block(img, (bx+18, by+10, CTA_BUTTON_W-36, CTA_BUTTON_H-20), \"Start Learning for Free\", (255,255,255), 54, 28)\n",
        "    return img\n",
        "\n",
        "# ---------- Local fallback generator ----------\n",
        "@dataclass\n",
        "class QuizItem:\n",
        "    topic: str\n",
        "    question: str\n",
        "    options: List[str]\n",
        "    answer_index: int\n",
        "    explanation: str\n",
        "\n",
        "EASY_TEMPLATES = [\n",
        "    \"Which of these is an example of {topic}?\",\n",
        "    \"What is {topic} mainly used for?\",\n",
        "    \"Which choice best matches {topic}?\",\n",
        "]\n",
        "MEDIUM_TEMPLATES = [\n",
        "    \"Which scenario best illustrates {topic} in practice?\",\n",
        "    \"Which statement about {topic} is correct?\",\n",
        "    \"What is a common use case of {topic}?\",\n",
        "]\n",
        "HARD_TEMPLATES = [\n",
        "    \"Which of the following is most accurate regarding {topic}?\",\n",
        "    \"In applied settings, which describes {topic} most precisely?\",\n",
        "    \"Which statement about {topic} reflects best practice?\",\n",
        "]\n",
        "EASY_DISTRACTORS = [\"Something unrelated\",\"A wrong idea\",\"Not quite right\",\"Another choice\",\"Sounds similar but isn't\"]\n",
        "MEDIUM_DISTRACTORS = [\"A partially correct statement\",\"A common misconception\",\"An unrelated technique\",\"A vague description\"]\n",
        "HARD_DISTRACTORS = [\"A subtle misconception\",\"A related but incorrect method\",\"An imprecise definition\",\"A misleading best practice\"]\n",
        "\n",
        "def _pick_template(difficulty: int) -> str:\n",
        "    return (random.choice(EASY_TEMPLATES) if difficulty<=3\n",
        "            else random.choice(MEDIUM_TEMPLATES) if difficulty<=7\n",
        "            else random.choice(HARD_TEMPLATES))\n",
        "\n",
        "def _generate_options_local(topic: str, difficulty: int) -> Tuple[List[str], int, str]:\n",
        "    topic_clean = topic.strip().rstrip(\"?.!\")\n",
        "    if difficulty <= 3:\n",
        "        correct, pool = f\"A simple example of {topic_clean}\", EASY_DISTRACTORS\n",
        "    elif difficulty <= 7:\n",
        "        correct, pool = f\"A practical use case of {topic_clean}\", MEDIUM_DISTRACTORS\n",
        "    else:\n",
        "        correct, pool = f\"A precise description of {topic_clean}\", HARD_DISTRACTORS\n",
        "    distractors = random.sample(pool, k=3)\n",
        "    options = distractors + [correct]\n",
        "    random.shuffle(options)\n",
        "    answer_index = options.index(correct)\n",
        "    explanation = (f\"The correct option describes {topic_clean} more appropriately than the others.\"\n",
        "                   if difficulty >= 4 else f\"It's the best match for {topic_clean}.\")\n",
        "    return options, answer_index, explanation\n",
        "\n",
        "def make_quiz_item_local(topic: str, difficulty: int) -> QuizItem:\n",
        "    q = _pick_template(difficulty).format(topic=topic)\n",
        "    options, answer_index, explanation = _generate_options_local(topic, difficulty)\n",
        "    return QuizItem(topic=topic, question=q, options=options, answer_index=answer_index, explanation=explanation)\n",
        "\n",
        "# ---------- Difficulty guide ----------\n",
        "DIFFICULTY_GUIDE = \"\"\"\n",
        "Map difficulty 1–10 to these constraints:\n",
        "1–2: kid-simple; one sentence; no jargon; obvious distractors.\n",
        "3–4: basic recognition; short phrasing; simple plausible distractors.\n",
        "5–6: intermediate conceptual; 1–2 sentences; plausible/related distractors.\n",
        "7–8: advanced application or edge cases; 2–3 sentences; subtle distractors.\n",
        "9–10: professional nuance; 2–3 concise sentences; highly plausible distractors with subtle traps.\n",
        "\"\"\"\n",
        "\n",
        "# Enforce variety across questions\n",
        "QUESTION_ARCHETYPES = [\n",
        "    \"Definition / concept check\",\n",
        "    \"Real-world scenario\",\n",
        "    \"Metric / measurement\",\n",
        "    \"Mitigation technique\",\n",
        "    \"Trade-off / decision\",\n",
        "    \"Failure diagnosis / debugging\",\n",
        "    \"Governance / policy / oversight\",\n",
        "    \"Counterfactual / what-if\",\n",
        "    \"Comparison / contrast\",\n",
        "    \"Process / ordering\"\n",
        "]\n",
        "\n",
        "# Common repetitive openings we want to forbid\n",
        "BANNED_OPENERS = [\n",
        "    \"what is a key\", \"what's a key\",\n",
        "    \"what is a crucial\", \"what's a crucial\",\n",
        "    \"what initial step\", \"which initial step\",\n",
        "    \"what is an initial step\", \"what’s an initial step\",\n",
        "    \"what is a primary step\", \"what’s a primary step\"\n",
        "]\n",
        "\n",
        "# ---------- Novelty / dedup helpers ----------\n",
        "_STOP = set(\"\"\"\n",
        "a an the of to in for on at by with from and or but if when while after before than as\n",
        "what which who whom whose this that these those there here how why is are was were be being been\n",
        "key crucial initial primary step best most common important effective\n",
        "\"\"\".split())\n",
        "def _tokens(s: str) -> set:\n",
        "    return {w for w in re.findall(r\"[a-z0-9]+\", (s or \"\").lower()) if len(w) > 2 and w not in _STOP}\n",
        "def _ngrams(s: str, n: int = 3) -> set:\n",
        "    t = re.sub(r\"\\s+\", \" \", (s or \"\").lower())\n",
        "    t = re.sub(r\"[^a-z0-9 ]\", \"\", t)\n",
        "    t = t.replace(\" \", \"_\")\n",
        "    return {t[i:i+n] for i in range(max(0, len(t)-n+1))}\n",
        "def _jaccard(a: set, b: set) -> float:\n",
        "    if not a and not b: return 1.0\n",
        "    if not a or not b: return 0.0\n",
        "    inter = len(a & b); union = len(a | b)\n",
        "    return inter / union if union else 0.0\n",
        "def too_similar(q1: str, q2: str, t_thr: float = 0.82, c_thr: float = 0.86) -> bool:\n",
        "    return max(_jaccard(_tokens(q1), _tokens(q2)), _jaccard(_ngrams(q1), _ngrams(q2))) >= max(t_thr, c_thr)\n",
        "\n",
        "# ---------- Prompt assembly (facet-aware + archetypes) ----------\n",
        "def build_prompt(topic: str, difficulty: int, count: int,\n",
        "                 standard_addendum: str,\n",
        "                 facet: str = \"\",\n",
        "                 begin_marker: str = BEGIN_JSON,\n",
        "                 end_marker: str   = END_JSON) -> str:\n",
        "    kinds_line = \", \".join(QUESTION_ARCHETYPES)\n",
        "    banned_line = \"; \".join(BANNED_OPENERS)\n",
        "\n",
        "    facet_block = \"\"\n",
        "    if facet:\n",
        "        facet_block = f\"\"\"\n",
        "DIVERSITY FACET (MANDATORY for this batch):\n",
        "- Focus specifically on: \"{facet}\".\n",
        "- Target THIS facet explicitly; avoid generic umbrella wording.\n",
        "\"\"\"\n",
        "\n",
        "    base = f\"\"\"\n",
        "You are an expert quiz writer for 1080×1920 SHORT videos.\n",
        "\n",
        "Produce exactly {count} distinct multiple-choice questions on the topic below.\n",
        "\n",
        "Topic: \"{topic}\"\n",
        "Difficulty (1-10): {difficulty}\n",
        "\n",
        "{DIFFICULTY_GUIDE.strip()}\n",
        "\n",
        "HARD LENGTH LIMITS (NEVER EXCEED):\n",
        "- question <= {Q_MAX} characters\n",
        "- each option <= {OPT_MAX} characters\n",
        "- rationale <= {EXP_MAX} characters\n",
        "\n",
        "STYLE:\n",
        "- Write the stem directly (no “Which of the following” preambles).\n",
        "- Everyday words. No parentheticals, citations, footnotes, or emojis.\n",
        "- Exactly 1 correct answer, 3 plausible distractors. No \"All/None of the above\".\n",
        "\n",
        "VARIETY RULES (STRICT):\n",
        "- Distribute questions across these archetypes and include a \"kind\" field per item:\n",
        "  [{kinds_line}]\n",
        "- Use each archetype at most once until all have been used; only then may you reuse.\n",
        "- Start each question with a DIFFERENT leading verb/phrase (e.g., Identify, Diagnose, Compare, Select, Quantify, Order, Spot, Predict, Explain, Choose).\n",
        "- Do not use repetitive templates such as “initial step”, “key step”, “crucial first step”.\n",
        "- Do NOT begin any question with: {banned_line}.\n",
        "- If an item’s archetype implies specifics, include them:\n",
        "  • Metric/measurement → name at least one concrete metric or signal.\n",
        "  • Trade-off/decision → contrast at least two options with a reason.\n",
        "  • Failure diagnosis → include a symptom and the likely cause.\n",
        "  • Process/ordering → ask for the first/next/most appropriate step.\n",
        "  • Scenario → give a short, realistic context (1–2 clauses).\n",
        "\n",
        "{facet_block.strip()}\n",
        "\n",
        "{standard_addendum.strip()}\n",
        "\n",
        "Return ONLY a single JSON object BETWEEN the markers below.\n",
        "Start your first character with the JSON after {BEGIN_JSON} and end before {END_JSON}.\n",
        "Do not include any text outside the markers. No markdown fences.\n",
        "\n",
        "{BEGIN_JSON}\n",
        "{{\n",
        "  \"topic\": \"{topic}\",\n",
        "  \"difficulty\": {difficulty},\n",
        "  \"questions\": [\n",
        "    {{\n",
        "      \"kind\": \"one of: {kinds_line}\",\n",
        "      \"question\": \"string (≤{Q_MAX})\",\n",
        "      \"correct_answer\": \"string (≤{OPT_MAX})\",\n",
        "      \"distractors\": [\"string (≤{OPT_MAX})\",\"string (≤{OPT_MAX})\",\"string (≤{OPT_MAX})\"],\n",
        "      \"rationale\": \"string (≤{EXP_MAX})\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "{END_JSON}\n",
        "\"\"\"\n",
        "    return base\n",
        "\n",
        "def _ensure_3_distractors(distractors: List[str], correct: str) -> List[str]:\n",
        "    seen, out = set(), []\n",
        "    corr = (correct or \"\").strip().lower()\n",
        "    for d in distractors or []:\n",
        "        d2 = (d or \"\").strip()\n",
        "        if not d2: continue\n",
        "        if d2.lower() == corr: continue\n",
        "        if d2.lower() in seen: continue\n",
        "        out.append(d2); seen.add(d2.lower())\n",
        "    while len(out) < 3:\n",
        "        out.append(f\"Alternative {len(out)+1}\")\n",
        "    return out[:3]\n",
        "def _shuffle_with_correct(correct: str, distractors: List[str]) -> Tuple[List[str], int]:\n",
        "    opts = (distractors or [])[:3] + [correct]\n",
        "    random.shuffle(opts)\n",
        "    idx = opts.index(correct)\n",
        "    return opts, idx\n",
        "\n",
        "# ---------- Gemini REST helpers ----------\n",
        "API_BASE = \"https://generativelanguage.googleapis.com/v1\"\n",
        "\n",
        "def list_models_v1(api_key: str) -> list:\n",
        "    r = requests.get(f\"{API_BASE}/models\", params={\"key\": api_key}, timeout=30)\n",
        "    if r.status_code >= 400:\n",
        "        raise RuntimeError(f\"REST {r.status_code}: {r.text[:200]}\")\n",
        "    return r.json().get(\"models\", []) or []\n",
        "\n",
        "def filter_generate_content_models(models: list) -> list:\n",
        "    out = []\n",
        "    for m in models:\n",
        "        methods = (m.get(\"supportedGenerationMethods\") or m.get(\"supported_generation_methods\") or [])\n",
        "        if \"generateContent\" in methods:\n",
        "            name = m.get(\"name\", \"\")\n",
        "            if name: out.append(name.split(\"/\")[-1])\n",
        "    return out\n",
        "\n",
        "def gemini_generate_v1(api_key: str, model: str, prompt: str,\n",
        "                       temperature: float, max_output_tokens: int) -> str:\n",
        "    url = f\"{API_BASE}/models/{model}:generateContent\"\n",
        "    params = {\"key\": api_key}\n",
        "    payload = {\n",
        "        \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": float(temperature),\n",
        "            \"topP\": 0.9,\n",
        "            \"topK\": 40,\n",
        "            \"maxOutputTokens\": int(max_output_tokens),\n",
        "            \"candidateCount\": 1,\n",
        "        },\n",
        "    }\n",
        "    r = requests.post(url, params=params, json=payload, timeout=60)\n",
        "    if r.status_code >= 400:\n",
        "        raise RuntimeError(f\"REST {r.status_code}: {r.text[:200]}\")\n",
        "    data = r.json()\n",
        "    cands = data.get(\"candidates\", [])\n",
        "    if not cands:\n",
        "        raise RuntimeError(f\"Empty candidates: {data}\")\n",
        "    cand0 = cands[0]\n",
        "    parts = (cand0.get(\"content\") or {}).get(\"parts\") or []\n",
        "    text = \"\".join(p.get(\"text\", \"\") for p in parts if isinstance(p, dict))\n",
        "    if not text:\n",
        "        raise RuntimeError(f\"No text in response. Raw: {json.dumps(data)[:400]}\")\n",
        "    return text\n",
        "\n",
        "def _shrink_prompt_blocks(p: str) -> str:\n",
        "    # Remove big fact lists if token pressure occurs (keep other guidance intact)\n",
        "    p = re.sub(r\"(?s)\\nPREFER these facts.*?(?=\\n{2,}|\" + re.escape(BEGIN_JSON) + r\")\", \"\\n\", p)\n",
        "    p = re.sub(r\"(?s)\\nSTRICT FACTS MODE.*?(?=\\n{2,}|\" + re.escape(BEGIN_JSON) + r\")\", \"\\n\", p)\n",
        "    return p\n",
        "\n",
        "def _safe_gemini_call(api_key, model, prompt, temperature, tok, tries: int = 4) -> str:\n",
        "    last = None\n",
        "    p = prompt\n",
        "    t = max(int(tok), 2000)  # start higher to leave room for internal reasoning\n",
        "    for _ in range(tries):\n",
        "        try:\n",
        "            return gemini_generate_v1(api_key, model, p, temperature, t)\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            msg = str(e)\n",
        "            if (\"MAX_TOKENS\" in msg) or (\"No text in response\" in msg):\n",
        "                if t < 3500:\n",
        "                    t = min(t + 500, 4096)  # give the model more headroom\n",
        "                else:\n",
        "                    p = _shrink_prompt_blocks(p)  # then shrink prompt facts\n",
        "                continue\n",
        "            time.sleep(0.2)  # small backoff for transient issues\n",
        "    raise last\n",
        "\n",
        "def _candidate_models_ordered(requested: str, available: list) -> list:\n",
        "    req = (requested or \"\").strip()\n",
        "    if req.endswith(\"-latest\"): req = req[:-7] + \"-001\"\n",
        "    order = []\n",
        "    if req: order.append(req)\n",
        "    for m in [\"gemini-2.5-flash\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-2.0-flash-exp\", \"gemini-pro\", \"gemini-1.0-pro\"]:\n",
        "        if m not in order: order.append(m)\n",
        "    avail = set(available)\n",
        "    return [m for m in order if m in avail]\n",
        "\n",
        "# ---------- AI question generator (REST) ----------\n",
        "def generate_questions_gemini(api_key: str, model_name: str, topic: str, difficulty: int, count: int,\n",
        "                              temperature: float = 0.6, max_output_tokens: int = 2800,\n",
        "                              pack_id: str = \"\", strict: bool = False,\n",
        "                              standard_choice: str = \"Consolidated (default)\",\n",
        "                              subtopic_label: str = \"\", focus_text: str = \"\"):\n",
        "\n",
        "    topic = (topic or \"\").strip()\n",
        "    difficulty = int(max(1, min(10, difficulty)))\n",
        "    count = int(max(1, min(10, count)))\n",
        "    items: List[QuizItem] = []\n",
        "    info_msgs: List[str] = []\n",
        "\n",
        "    pack = PACKS.get(pack_id) if pack_id else None\n",
        "    has_facts = bool(pack and pack.get(\"facts\"))\n",
        "    if strict and not has_facts:\n",
        "        strict = False\n",
        "        info_msgs.append(\"Strict facts disabled (no facts available for the selected pack).\")\n",
        "\n",
        "    # novelty memory\n",
        "    accepted_qs: List[str] = []\n",
        "\n",
        "    if not api_key:\n",
        "        info_msgs.append(\"No Gemini API key provided — using local generator.\")\n",
        "        for _ in range(count):\n",
        "            candidate = make_quiz_item_local(topic, difficulty)\n",
        "            if any(too_similar(candidate.question, q) for q in accepted_qs):\n",
        "                continue\n",
        "            accepted_qs.append(candidate.question)\n",
        "            items.append(candidate)\n",
        "            if len(items) >= count: break\n",
        "        # If still short, pad\n",
        "        while len(items) < count:\n",
        "            items.append(make_quiz_item_local(topic, difficulty))\n",
        "        return items, \" \".join(info_msgs)\n",
        "\n",
        "    # Discover models\n",
        "    try:\n",
        "        models_raw = list_models_v1(api_key)\n",
        "        available = filter_generate_content_models(models_raw)\n",
        "    except Exception as e:\n",
        "        available = []\n",
        "        info_msgs.append(f\"Model listing failed ({e}); attempting common defaults.\")\n",
        "\n",
        "    to_try = _candidate_models_ordered(\n",
        "        model_name,\n",
        "        available if available else [\"gemini-2.5-flash\",\"gemini-1.5-flash-001\",\"gemini-1.5-pro-001\",\"gemini-pro\",\"gemini-1.0-pro\"]\n",
        "    ) or [\"gemini-2.5-flash\",\"gemini-1.5-flash-001\",\"gemini-1.5-pro-001\",\"gemini-pro\",\"gemini-1.0-pro\"]\n",
        "\n",
        "    # Precompute facet cycle\n",
        "    facet_pool = facets_for(pack_id or \"\", subtopic_label or \"\")\n",
        "    if not facet_pool:\n",
        "        facet_pool = DEFAULT_FACETS[:]\n",
        "    random.shuffle(facet_pool)\n",
        "    facet_cycle = (facet_pool * ((count + len(facet_pool) - 1) // len(facet_pool)))[:count]\n",
        "\n",
        "    last_err = None\n",
        "    for m in to_try:\n",
        "        try:\n",
        "            remaining = count\n",
        "            chunk = 1  # one facet per prompt = maximum diversity\n",
        "\n",
        "            tok = max(max_output_tokens if max_output_tokens else 1600,\n",
        "                      1400 if difficulty >= 7 else 1200)\n",
        "\n",
        "            used_fact_ids = set()\n",
        "            k_base = _facts_k(count, difficulty, strict)\n",
        "            facet_idx = 0\n",
        "            attempts = 0\n",
        "            max_attempts = max(6, count * 4)\n",
        "\n",
        "            while remaining > 0 and attempts < max_attempts:\n",
        "                attempts += 1\n",
        "                take = min(chunk, remaining)\n",
        "\n",
        "                # facts slice (for strict or guidance)\n",
        "                if pack and has_facts:\n",
        "                    pool = select_facts_slice(topic, pack, k=k_base*2)\n",
        "                    chunk_slice = [f for f in pool if f[\"id\"] not in used_fact_ids][:k_base]\n",
        "                    if not chunk_slice:\n",
        "                        chunk_slice = pool[:k_base]\n",
        "                        used_fact_ids.clear()\n",
        "                    used_fact_ids |= {f[\"id\"] for f in chunk_slice}\n",
        "                else:\n",
        "                    chunk_slice = []\n",
        "\n",
        "                # standards + facet\n",
        "                source_addendum = build_source_alignment_addendum(pack_id or \"\", standard_choice or \"\")\n",
        "                addendum = build_standard_addendum(pack, strict, chunk_slice, source_addendum)\n",
        "                facet_now = facet_cycle[facet_idx % len(facet_cycle)] if facet_cycle else \"\"\n",
        "                facet_idx += 1\n",
        "\n",
        "                prompt = build_prompt(topic, difficulty, take, standard_addendum=addendum, facet=facet_now)\n",
        "\n",
        "                # SAFE CALL (with token headroom + prompt shrink under pressure)\n",
        "                text = _safe_gemini_call(api_key, m, prompt, temperature, tok)\n",
        "\n",
        "                qlist = parse_questions_from_model(text)\n",
        "                if not qlist:\n",
        "                    tok = min(tok + 300, 4096)\n",
        "                    text = _safe_gemini_call(api_key, m, prompt, temperature, tok)\n",
        "                    qlist = parse_questions_from_model(text)\n",
        "                    if not qlist:\n",
        "                        continue\n",
        "\n",
        "                if strict:\n",
        "                    qlist = validate_strict_items(qlist, {f[\"id\"] for f in chunk_slice})\n",
        "\n",
        "                # Add filtered, deduped items\n",
        "                added = 0\n",
        "                for q in qlist:\n",
        "                    qtext = (q.get(\"question\") or \"\").strip()\n",
        "                    correct = (q.get(\"correct_answer\") or q.get(\"answer\") or q.get(\"correct\") or \"\").strip()\n",
        "                    distractors = q.get(\"distractors\") or []\n",
        "                    rationale = (q.get(\"rationale\") or q.get(\"explanation\") or \"\").strip()\n",
        "                    if not qtext or not correct:\n",
        "                        continue\n",
        "                    # banned openers check\n",
        "                    low = qtext.lower().strip()\n",
        "                    if any(low.startswith(b) for b in BANNED_OPENERS):\n",
        "                        continue\n",
        "                    # novelty check\n",
        "                    if any(too_similar(qtext, prev) for prev in accepted_qs):\n",
        "                        continue\n",
        "\n",
        "                    distractors = _ensure_3_distractors(distractors, correct)\n",
        "                    options, answer_index = _shuffle_with_correct(correct, distractors)\n",
        "                    qtext, options, rationale, _ = lint_item_lengths(qtext, options, rationale, add_ellipsis=False)\n",
        "\n",
        "                    items.append(QuizItem(topic=topic, question=qtext, options=options,\n",
        "                                          answer_index=answer_index, explanation=rationale))\n",
        "                    accepted_qs.append(qtext)\n",
        "                    added += 1\n",
        "                    remaining -= 1\n",
        "                    if remaining <= 0:\n",
        "                        break\n",
        "\n",
        "                # If we didn't add anything this round, try a different facet immediately\n",
        "                if added == 0:\n",
        "                    continue\n",
        "\n",
        "                if m != model_name and f\"Using available model: {m}\" not in info_msgs:\n",
        "                    info_msgs.append(f\"Using available model: {m}\")\n",
        "\n",
        "            if strict and len(items) < count:\n",
        "                info_msgs.append(f\"Strict facts mode returned {len(items)}/{count}. Regenerate for more or disable Strict.\")\n",
        "\n",
        "            # If still short, attempt a quick local top-up, preserving novelty\n",
        "            tries = 0\n",
        "            while len(items) < count and tries < count*2:\n",
        "                tries += 1\n",
        "                cand = make_quiz_item_local(topic, difficulty)\n",
        "                if any(too_similar(cand.question, prev) for prev in accepted_qs):\n",
        "                    continue\n",
        "                items.append(cand); accepted_qs.append(cand.question)\n",
        "\n",
        "            return items, (\" \".join(info_msgs) if info_msgs else f\"Generated {len(items)} item(s) via Gemini.\")\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            continue\n",
        "\n",
        "    info_msgs.append(f\"Gemini error: {last_err}. Falling back to local generator.\")\n",
        "    # Fallback local (with novelty)\n",
        "    while len(items) < count:\n",
        "        cand = make_quiz_item_local(topic, difficulty)\n",
        "        if any(too_similar(cand.question, prev) for prev in accepted_qs):\n",
        "            continue\n",
        "        items.append(cand); accepted_qs.append(cand.question)\n",
        "    return items, \" \".join(info_msgs)\n",
        "\n",
        "# ---------- FFmpeg assembly ----------\n",
        "def _save_png(img: Image.Image, path: Path):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    img.save(str(path), format=\"PNG\", optimize=True)\n",
        "def _ffmpeg_build(out_path: Path, question_png: Path, reveal_png: Path, cta_png: Path,\n",
        "                  q_sec=6.0, r_sec=4.3, c_sec=2.9):\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\"-y\",\"-loglevel\",\"error\",\"-threads\",\"1\",\n",
        "        \"-loop\",\"1\",\"-t\",f\"{q_sec}\",\"-i\",str(question_png),\n",
        "        \"-loop\",\"1\",\"-t\",f\"{r_sec}\",\"-i\",str(reveal_png),\n",
        "        \"-loop\",\"1\",\"-t\",f\"{c_sec}\",\"-i\",str(cta_png),\n",
        "        \"-filter_complex\",\n",
        "        (\n",
        "          f\"[0:v]scale={W}:{H},setsar=1[v0];\"\n",
        "          f\"[1:v]scale={W}:{H},setsar=1[v1];\"\n",
        "          f\"[2:v]scale={W}:{H},setsar=1[v2];\"\n",
        "          f\"[v0][v1][v2]concat=n=3:v=1:a=0,format=yuv420p[v]\"\n",
        "        ),\n",
        "        \"-map\",\"[v]\",\"-r\", str(FPS),\n",
        "        \"-c:v\",\"libx264\",\"-preset\",\"veryfast\",\n",
        "        \"-b:v\", BITRATE,\"-maxrate\", BITRATE,\"-bufsize\", BITRATE,\n",
        "        \"-movflags\",\"+faststart\",\"-g\", str(FPS*2),\n",
        "        str(out_path)\n",
        "    ]\n",
        "    subprocess.check_call(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "\n",
        "def build_quiz_video(item: QuizItem, out_path: Path) -> Path:\n",
        "    q_img = _question_slide(item.question, item.options[:4])\n",
        "    r_img = _reveal_slide(item.options[item.answer_index], item.explanation)\n",
        "    c_img = _cta_slide()\n",
        "    base = TMP_DIR / f\"vid_{int(time.time()*1000)}_{random.randint(1000,9999)}\"\n",
        "    q_png, r_png, c_png = base.with_suffix(\".q.png\"), base.with_suffix(\".r.png\"), base.with_suffix(\".c.png\")\n",
        "    _save_png(q_img, q_png); _save_png(r_img, r_png); _save_png(c_img, c_png)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    _ffmpeg_build(out_path, q_png, r_png, c_png)\n",
        "    for p in [q_png, r_png, c_png]:\n",
        "        try: p.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "    return out_path\n",
        "\n",
        "def _slugify(text: str) -> str:\n",
        "    s = text.lower().strip().replace(\" \", \"-\")\n",
        "    allowed = set(string.ascii_lowercase + string.digits + \"-\")\n",
        "    s = \"\".join(ch for ch in s if ch in allowed)\n",
        "    return s or f\"item-{int(time.time())}\"\n",
        "\n",
        "# ---------- Gradio callbacks ----------\n",
        "def _seed_rng(): random.seed(time.time_ns() % (2**32 - 1))\n",
        "\n",
        "def generate_preview_ai(topic: str, difficulty: int, count: int, api_key: str, model_name: str,\n",
        "                        temperature: float = 0.6, max_output_tokens: int = 2800,\n",
        "                        pack_id: str = \"\", strict: bool = False, standard_choice: str = \"Consolidated (default)\",\n",
        "                        subtopic_label: str = \"\", focus_text: str = \"\"):\n",
        "    topic = (topic or \"\").strip()\n",
        "    if not topic:\n",
        "        return pd.DataFrame([]), \"Please enter a topic.\", [], \"\"\n",
        "    difficulty = int(max(1, min(10, difficulty)))\n",
        "    count = int(max(1, min(10, count)))\n",
        "    _seed_rng()\n",
        "\n",
        "    items, info_msg = generate_questions_gemini(\n",
        "        api_key, model_name, topic, difficulty, count,\n",
        "        temperature=temperature, max_output_tokens=max_output_tokens,\n",
        "        pack_id=pack_id, strict=strict, standard_choice=standard_choice,\n",
        "        subtopic_label=subtopic_label, focus_text=focus_text\n",
        "    )\n",
        "\n",
        "    rows = []\n",
        "    for i, it in enumerate(items, 1):\n",
        "        q2, opts2, exp2, flags = lint_item_lengths(it.question, it.options[:4], it.explanation, add_ellipsis=False)\n",
        "        q_show  = q2 + (CLIP_FLAG if flags[\"Q\"] else \"\")\n",
        "        a_show  = opts2[0] + (CLIP_FLAG if flags[\"A\"] else \"\")\n",
        "        b_show  = opts2[1] + (CLIP_FLAG if flags[\"B\"] else \"\")\n",
        "        c_show  = opts2[2] + (CLIP_FLAG if flags[\"C\"] else \"\")\n",
        "        d_show  = opts2[3] + (CLIP_FLAG if flags[\"D\"] else \"\")\n",
        "        exp_show= exp2 + (CLIP_FLAG if flags[\"EXP\"] else \"\")\n",
        "\n",
        "        rows.append({\n",
        "            \"#\": i, \"Question\": q_show,\n",
        "            \"A\": a_show, \"B\": b_show, \"C\": c_show, \"D\": d_show,\n",
        "            \"Correct\": [\"A\",\"B\",\"C\",\"D\"][it.answer_index],\n",
        "            \"Explanation\": exp_show,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    status = info_msg or f\"Generated {len(rows)} quiz item(s). Review below.\"\n",
        "    return df, status, rows, topic\n",
        "\n",
        "def confirm_and_produce(items_table, topic: str):\n",
        "    if items_table is None:\n",
        "        return \"No items to render. Please generate a preview first.\", []\n",
        "    if hasattr(items_table, \"to_dict\"):\n",
        "        rows = items_table.to_dict(orient=\"records\")\n",
        "    elif isinstance(items_table, list):\n",
        "        rows = items_table\n",
        "    else:\n",
        "        rows = []\n",
        "    if not rows:\n",
        "        return \"No items to render. Please generate a preview first.\", []\n",
        "\n",
        "    saved_paths = []\n",
        "    for idx, raw in enumerate(rows, 1):\n",
        "        def _clean(s):\n",
        "            return (s or \"\").replace(CLIP_FLAG, \"\").strip()\n",
        "\n",
        "        options = [_clean(raw.get(\"A\",\"\")), _clean(raw.get(\"B\",\"\")),\n",
        "                   _clean(raw.get(\"C\",\"\")), _clean(raw.get(\"D\",\"\"))]\n",
        "        correct_letter = str(raw.get(\"Correct\",\"A\")).strip().upper()\n",
        "        answer_index = {\"A\":0,\"B\":1,\"C\":2,\"D\":3}.get(correct_letter, 0)\n",
        "\n",
        "        q = _clean(raw.get(\"Question\",\"\"))\n",
        "        exp = _clean(raw.get(\"Explanation\",\"\"))\n",
        "\n",
        "        q, options, exp, _ = lint_item_lengths(q, options, exp, add_ellipsis=True)\n",
        "\n",
        "        qi = QuizItem(topic=topic or \"\", question=q, options=options,\n",
        "                      answer_index=answer_index, explanation=exp)\n",
        "\n",
        "        slug = _slugify(f\"{topic}-{idx}\")\n",
        "        out_path = OUT_DIR / f\"myai101_{slug}.mp4\"\n",
        "        try:\n",
        "            build_quiz_video(qi, out_path)\n",
        "            saved_paths.append(str(out_path))\n",
        "        except Exception as e:\n",
        "            saved_paths.append(f\"ERROR: {e}\")\n",
        "        gc.collect()\n",
        "\n",
        "    msg = f\"Done. Produced {len(saved_paths)} video(s).\"\n",
        "    files = [p for p in saved_paths if Path(p).suffix.lower()==\".mp4\" and Path(p).exists()]\n",
        "    return msg, files\n",
        "\n",
        "_empty_df = pd.DataFrame(columns=[\"#\", \"Question\", \"A\", \"B\", \"C\", \"D\", \"Correct\", \"Explanation\"])\n",
        "def _make_table():\n",
        "    try:\n",
        "        return gr.Dataframe(\n",
        "            headers=list(_empty_df.columns),\n",
        "            datatype=[\"number\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\"],\n",
        "            row_count=(1, \"dynamic\"),\n",
        "            col_count=(8, \"fixed\"),\n",
        "            wrap=True,\n",
        "            label=\"You can edit cells before confirming to tweak wording.\",\n",
        "            interactive=True,\n",
        "        )\n",
        "    except TypeError:\n",
        "        return gr.Dataframe(\n",
        "            value=_empty_df,\n",
        "            headers=list(_empty_df.columns),\n",
        "            wrap=True,\n",
        "            label=\"You can edit cells before confirming to tweak wording.\",\n",
        "            interactive=True,\n",
        "        )\n",
        "\n",
        "# ---------- UI ----------\n",
        "with gr.Blocks(title=\"MyAI101 — Quiz Video Maker (Gemini REST, No MoviePy, 1080x1920)\") as demo:\n",
        "    gr.Markdown(\"# MyAI101 — Quiz Video Maker (Gemini, **REST**, no MoviePy, 1080×1920)\")\n",
        "\n",
        "    # Topic (freeform; will be disabled in Standards mode)\n",
        "    with gr.Row():\n",
        "        topic_inp = gr.Textbox(label=\"Topic\", placeholder=\"e.g. Backpropagation, SSL certificates, Photosynthesis\")\n",
        "\n",
        "    with gr.Row():\n",
        "        diff_inp  = gr.Slider(1, 10, value=5, step=1, label=\"Difficulty (1 = child <10, 10 = professional)\")\n",
        "        count_inp = gr.Slider(1, 10, value=3, step=1, label=\"How many questions / videos to create\")\n",
        "\n",
        "    gr.Markdown(\"### AI Generation Settings (Gemini)\")\n",
        "    with gr.Row():\n",
        "        api_key_inp = gr.Textbox(label=\"Gemini API Key\", placeholder=\"Paste your Google AI Studio API key\", type=\"password\")\n",
        "        model_inp   = gr.Dropdown(choices=[\"gemini-2.5-flash\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\"], value=\"gemini-2.5-flash\", label=\"Model\")\n",
        "        list_btn    = gr.Button(\"🔎 List Available Models\")\n",
        "\n",
        "    gr.Markdown(\"### Standards & Topic Alignment\")\n",
        "    with gr.Row():\n",
        "        topic_preset_inp = gr.Dropdown(choices=TOPIC_PRESET_CHOICES, value=\"— choose —\", label=\"AI Topic (presets)\")\n",
        "        try:\n",
        "            pack_inp   = gr.Dropdown(choices=[\"\"] + list(PACKS.keys()), value=\"\", label=\"Pack (advanced)\")\n",
        "        except TypeError:\n",
        "            pack_inp   = gr.Dropdown(choices=[\"\"] + list(PACKS.keys()), value=\"\", label=\"Pack (advanced)\")\n",
        "        mode_inp   = gr.Radio([\"Freeform\", \"Standards\"], value=\"Standards\", label=\"Mode\")\n",
        "\n",
        "    # NEW: Subtopic + Focus (used in Standards mode; ignored in Freeform)\n",
        "    with gr.Row():\n",
        "        subtopic_inp = gr.Dropdown(choices=[\"— none —\"], value=\"— none —\", label=\"Subtopic (optional)\")\n",
        "        focus_inp    = gr.Textbox(label=\"Focus (optional)\", placeholder=\"e.g., KV cache limits on long contexts\")\n",
        "\n",
        "    with gr.Row():\n",
        "        std_choices = standards_choices_for(\"\")  # default\n",
        "        standards_inp = gr.Dropdown(choices=std_choices, value=\"Consolidated (default)\", label=\"Standards Source\")\n",
        "        strict_inp    = gr.Checkbox(label=\"Strict facts (require citations from pack)\", value=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        temp_inp    = gr.Slider(0.0, 1.0, value=0.6, step=0.1, label=\"Temperature (creativity)\")\n",
        "        max_tok_inp = gr.Slider(200, 4096, value=2800, step=100, label=\"Max output tokens\")\n",
        "\n",
        "    with gr.Row():\n",
        "        preview_btn = gr.Button(\"🧠 Generate Preview (AI)\")\n",
        "        regen_btn   = gr.Button(\"↻ Regenerate\")\n",
        "\n",
        "    gr.Markdown(\"### Preview: Questions & Answers\")\n",
        "    preview_state = gr.State([])\n",
        "    topic_state   = gr.State(\"\")\n",
        "\n",
        "    table  = _make_table()\n",
        "    status = gr.Markdown(visible=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        confirm_btn = gr.Button(\"✅ Confirm & Produce Videos\", variant=\"primary\")\n",
        "    out_msg   = gr.Markdown()\n",
        "    out_files = gr.Files(label=\"Rendered MP4s\")\n",
        "\n",
        "    # --- Callbacks (definitions) ---\n",
        "    def _on_list_models(api_key):\n",
        "        if not api_key:\n",
        "            return gr.update(), \"Enter API key first.\"\n",
        "        try:\n",
        "            mods = list_models_v1(api_key)\n",
        "            choices = filter_generate_content_models(mods)\n",
        "            if not choices:\n",
        "                return gr.update(choices=[], value=None), \"No generateContent-capable models found for this key.\"\n",
        "            default = (\"gemini-2.5-flash\" if \"gemini-2.5-flash\" in choices\n",
        "                       else \"gemini-1.5-flash-001\" if \"gemini-1.5-flash-001\" in choices\n",
        "                       else choices[0])\n",
        "            return gr.update(choices=choices, value=default), f\"Found {len(choices)} model(s).\"\n",
        "        except Exception as e:\n",
        "            return gr.update(), f\"Listing failed: {e}\"\n",
        "\n",
        "    def _on_topic_preset_change(preset_label, mode_value, focus_text):\n",
        "        # Auto-set pack when preset chosen\n",
        "        if preset_label and preset_label != \"— choose —\":\n",
        "            match = next((t for t in TOPIC_PRESETS if t[\"label\"] == preset_label), None)\n",
        "            new_pack = match[\"pack\"] if match else \"\"\n",
        "        else:\n",
        "            new_pack = \"\"\n",
        "\n",
        "        # Update standards choices for this pack; set default\n",
        "        choices = standards_choices_for(new_pack) if new_pack else standards_choices_for(\"\")\n",
        "        default_std = default_standard_for(new_pack) if new_pack else \"Consolidated (default)\"\n",
        "        if default_std not in choices and choices:\n",
        "            default_std = choices[0]\n",
        "\n",
        "        # Update subtopics for the selected pack\n",
        "        subs = [\"— none —\"] + (subtopics_for_pack(new_pack) if new_pack else [])\n",
        "\n",
        "        # Compose and possibly lock Topic\n",
        "        if mode_value == \"Standards\":\n",
        "            topic_val = compose_topic_display(preset_label, new_pack, \"— none —\", focus_text)\n",
        "            topic_upd = gr.update(value=topic_val, interactive=False)\n",
        "        else:\n",
        "            topic_upd = gr.update(interactive=True)  # keep user's text\n",
        "\n",
        "        return (\n",
        "            gr.update(value=new_pack),                   # pack_inp\n",
        "            gr.update(choices=choices, value=default_std),  # standards_inp\n",
        "            gr.update(choices=subs, value=\"— none —\"),   # subtopic_inp\n",
        "            topic_upd                                     # topic_inp\n",
        "        )\n",
        "\n",
        "    def _on_pack_change(pack_id, mode_value, preset_label, focus_text):\n",
        "        choices = standards_choices_for(pack_id) if pack_id else standards_choices_for(\"\")\n",
        "        default_std = default_standard_for(pack_id) if pack_id else \"Consolidated (default)\"\n",
        "        if default_std not in choices and choices:\n",
        "            default_std = choices[0]\n",
        "\n",
        "        subs = [\"— none —\"] + (subtopics_for_pack(pack_id) if pack_id else [])\n",
        "\n",
        "        if mode_value == \"Standards\":\n",
        "            topic_val = compose_topic_display(preset_label, pack_id, \"— none —\", focus_text)\n",
        "            topic_upd = gr.update(value=topic_val, interactive=False)\n",
        "        else:\n",
        "            topic_upd = gr.update(interactive=True)\n",
        "\n",
        "        return (\n",
        "            gr.update(choices=choices, value=default_std),  # standards_inp\n",
        "            gr.update(choices=subs, value=\"— none —\"),      # subtopic_inp\n",
        "            topic_upd                                       # topic_inp\n",
        "        )\n",
        "\n",
        "    def _on_subtopic_change(subtopic, mode_value, preset_label, pack_id, focus_text):\n",
        "        if mode_value == \"Standards\":\n",
        "            topic_val = compose_topic_display(preset_label, pack_id, subtopic, focus_text)\n",
        "            return gr.update(value=topic_val, interactive=False)\n",
        "        else:\n",
        "            return gr.update(interactive=True)\n",
        "\n",
        "    def _on_focus_change(focus_text, mode_value, preset_label, pack_id, subtopic):\n",
        "        if mode_value == \"Standards\":\n",
        "            topic_val = compose_topic_display(preset_label, pack_id, subtopic, focus_text)\n",
        "            return gr.update(value=topic_val, interactive=False)\n",
        "        else:\n",
        "            return gr.update(interactive=True)\n",
        "\n",
        "    def _on_mode_change(mode_value, current_pack, preset_label, subtopic, focus_text):\n",
        "        if mode_value == \"Freeform\":\n",
        "            # Enable Topic; disable Pack/Standards/Strict\n",
        "            return (\n",
        "                gr.update(interactive=False),                            # pack_inp\n",
        "                gr.update(interactive=False),                            # standards_inp\n",
        "                gr.update(value=False, interactive=False),               # strict_inp\n",
        "                gr.update(interactive=True),                             # topic_inp\n",
        "            )\n",
        "        else:\n",
        "            # Standards mode: enable Pack/Standards/Strict and lock Topic to composed\n",
        "            topic_val = compose_topic_display(preset_label, current_pack, subtopic, focus_text)\n",
        "            return (\n",
        "                gr.update(interactive=True, value=current_pack),         # pack_inp\n",
        "                gr.update(interactive=True),                             # standards_inp\n",
        "                gr.update(interactive=True),                             # strict_inp\n",
        "                gr.update(value=topic_val, interactive=False),           # topic_inp\n",
        "            )\n",
        "\n",
        "    def _compose_topic_for_preview(mode_value, freeform_topic, preset_label, pack_id, subtopic, focus_text) -> str:\n",
        "        if mode_value == \"Standards\":\n",
        "            t = compose_topic_display(preset_label, pack_id, subtopic, focus_text)\n",
        "            return t or freeform_topic or \"\"\n",
        "        return freeform_topic or \"\"\n",
        "\n",
        "    def _on_preview(topic_text, diff, count, api_key, model_name, temperature, max_tokens,\n",
        "                    mode_value, pack_id, strict_flag, std_choice, preset_label, subtopic, focus_text):\n",
        "        # Build final topic based on mode\n",
        "        final_topic = _compose_topic_for_preview(mode_value, topic_text, preset_label, pack_id, subtopic, focus_text)\n",
        "\n",
        "        # If Mode is Freeform, ignore pack & standards\n",
        "        use_pack = pack_id if (mode_value == \"Standards\" and pack_id) else \"\"\n",
        "        use_std  = std_choice if (mode_value == \"Standards\") else \"Consolidated (default)\"\n",
        "        use_strict = bool(strict_flag) and bool(use_pack) and bool(PACKS.get(use_pack, {}).get(\"facts\"))\n",
        "\n",
        "        df, msg, rows, used_topic = generate_preview_ai(\n",
        "            final_topic, int(diff), int(count), api_key, model_name, float(temperature), int(max_tokens),\n",
        "            pack_id=use_pack, strict=use_strict, standard_choice=use_std,\n",
        "            subtopic_label=subtopic if mode_value == \"Standards\" else \"\", focus_text=focus_text if mode_value == \"Standards\" else \"\"\n",
        "        )\n",
        "        if not hasattr(df, \"to_dict\"):\n",
        "            df = pd.DataFrame(df)\n",
        "        if use_strict:\n",
        "            msg = msg + \" (Strict facts ON)\"\n",
        "        elif mode_value == \"Standards\" and use_pack:\n",
        "            msg = msg + f\" (Aligned to {PACKS[use_pack]['name']} • {use_std})\"\n",
        "        return df, msg, rows, used_topic\n",
        "\n",
        "    def _on_confirm(current_table, topic_used):\n",
        "        return confirm_and_produce(current_table, topic_used)\n",
        "\n",
        "    # --- Wire events (inside Blocks) ---\n",
        "    list_btn.click(_on_list_models, inputs=[api_key_inp], outputs=[model_inp, status])\n",
        "\n",
        "    topic_preset_inp.change(\n",
        "        _on_topic_preset_change,\n",
        "        inputs=[topic_preset_inp, mode_inp, focus_inp],\n",
        "        outputs=[pack_inp, standards_inp, subtopic_inp, topic_inp]\n",
        "    )\n",
        "\n",
        "    pack_inp.change(\n",
        "        _on_pack_change,\n",
        "        inputs=[pack_inp, mode_inp, topic_preset_inp, focus_inp],\n",
        "        outputs=[standards_inp, subtopic_inp, topic_inp]\n",
        "    )\n",
        "\n",
        "    subtopic_inp.change(\n",
        "        _on_subtopic_change,\n",
        "        inputs=[subtopic_inp, mode_inp, topic_preset_inp, pack_inp, focus_inp],\n",
        "        outputs=[topic_inp]\n",
        "    )\n",
        "\n",
        "    focus_inp.change(\n",
        "        _on_focus_change,\n",
        "        inputs=[focus_inp, mode_inp, topic_preset_inp, pack_inp, subtopic_inp],\n",
        "        outputs=[topic_inp]\n",
        "    )\n",
        "\n",
        "    mode_inp.change(\n",
        "        _on_mode_change,\n",
        "        inputs=[mode_inp, pack_inp, topic_preset_inp, subtopic_inp, focus_inp],\n",
        "        outputs=[pack_inp, standards_inp, strict_inp, topic_inp]\n",
        "    )\n",
        "\n",
        "    preview_btn.click(_on_preview, inputs=[\n",
        "        topic_inp, diff_inp, count_inp, api_key_inp, model_inp, temp_inp, max_tok_inp,\n",
        "        mode_inp, pack_inp, strict_inp, standards_inp, topic_preset_inp, subtopic_inp, focus_inp\n",
        "    ], outputs=[table, status, preview_state, topic_state])\n",
        "\n",
        "    regen_btn.click(_on_preview, inputs=[\n",
        "        topic_inp, diff_inp, count_inp, api_key_inp, model_inp, temp_inp, max_tok_inp,\n",
        "        mode_inp, pack_inp, strict_inp, standards_inp, topic_preset_inp, subtopic_inp, focus_inp\n",
        "    ], outputs=[table, status, preview_state, topic_state])\n",
        "\n",
        "    confirm_btn.click(_on_confirm, inputs=[table, topic_state], outputs=[out_msg, out_files])\n",
        "\n",
        "# --- Launch (print URLs) ---\n",
        "gr.close_all(); gc.collect()\n",
        "res = demo.launch(share=True, inbrowser=False, inline=False, show_error=True, debug=True, prevent_thread_lock=True)\n",
        "try:\n",
        "    print(\"Local URL:\", getattr(res, \"local_url\", None) or res.local_url)\n",
        "    print(\"Public URL:\", getattr(res, \"share_url\", None) or res.share_url)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    }
  ]
}