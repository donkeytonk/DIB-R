{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOofwud0SAsAEdbTh0ngCz/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donkeytonk/DIB-R/blob/master/Video_Quiz_UI_Share_Link_v0_02_WORKING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW3gdQglzInu",
        "outputId": "33aba709-1e32-43e8-bbdf-64d95c28a89b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4c52e19071d0376ea6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://4c52e19071d0376ea6.gradio.live\n"
          ]
        }
      ],
      "source": [
        "# === ONE-CELL: MyAI101 Quiz Video Maker (1080x1920, Gemini-powered via REST, No MoviePy) ===\n",
        "# - Vertical HD (1080x1920 @ 24fps), ffmpeg only (threads=1)\n",
        "# - Gradio UI: Topic, Difficulty(1-10), Count(1-10), Gemini API Key, Model\n",
        "# - Uses Gemini **v1 REST** (no SDK). Button to list models your key actually has.\n",
        "# - Randomizes correct answer placement; editable preview table\n",
        "# - Renders MP4s to /content/out/videos; prints Local/Public URLs\n",
        "\n",
        "import os, sys, subprocess, random, string, time, gc, warnings, re, json\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Headless safety ---\n",
        "os.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"\n",
        "os.environ[\"XDG_RUNTIME_DIR\"]  = \"/tmp/runtime\"\n",
        "os.makedirs(\"/tmp/runtime\", exist_ok=True)\n",
        "\n",
        "# --- Deps ---\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                       \"pillow>=10.0.0\", \"gradio>=4.0.0\", \"pandas>=1.3\", \"requests>=2.31\"])\n",
        "subprocess.call([\"apt-get\", \"-y\", \"install\", \"-qq\",\n",
        "                 \"ffmpeg\", \"fonts-dejavu-core\", \"fonts-liberation\"])\n",
        "\n",
        "\n",
        "import requests\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# ---------- RENDER SETTINGS ----------\n",
        "W, H, FPS = 1080, 1920, 24\n",
        "BITRATE = \"3500k\"\n",
        "\n",
        "GREEN   = (16, 163, 74)\n",
        "MINT    = (209, 250, 229)\n",
        "DARK    = (15, 23, 42)\n",
        "MID     = (51, 65, 85)\n",
        "BG      = (255, 255, 255)\n",
        "CHIP_BG = (243, 244, 246)\n",
        "\n",
        "SAFE_X   = int(W * 0.09)\n",
        "SAFE_TOP = int(H * 0.08)\n",
        "SAFE_BOT = int(H * 0.10)\n",
        "\n",
        "TITLE_BOX_H         = int(H * 0.36)\n",
        "GAP_BELOW_TITLE_MIN = int(H * 0.10)\n",
        "\n",
        "CHIP_H      = 108\n",
        "CHIP_TX_H   = 80\n",
        "CHIP_TX_PAD = 90\n",
        "CHIP_GAP    = int(H * 0.085)\n",
        "\n",
        "CTA_BUTTON_W = 640\n",
        "CTA_BUTTON_H = 120\n",
        "\n",
        "# ---------- Font + text helpers ----------\n",
        "# Robust font resolver so we never fall back to the tiny bitmap font\n",
        "FONT_CANDIDATES = [\n",
        "    \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\",\n",
        "    \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
        "    \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\",\n",
        "    \"/usr/share/fonts/truetype/liberation2/LiberationSans-Bold.ttf\",\n",
        "    \"/usr/share/fonts/truetype/freefont/FreeSansBold.ttf\",\n",
        "]\n",
        "\n",
        "def _resolve_font_path() -> str:\n",
        "    for p in FONT_CANDIDATES:\n",
        "        if Path(p).exists():\n",
        "            return p\n",
        "    # last-ditch: grab the first TTF under /usr/share/fonts\n",
        "    try:\n",
        "        for p in Path(\"/usr/share/fonts\").rglob(\"*.ttf\"):\n",
        "            return str(p)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"  # will trigger bitmap fallback\n",
        "\n",
        "DEFAULT_FONT_PATH = _resolve_font_path()\n",
        "DEFAULT_FONT_EXISTS = bool(DEFAULT_FONT_PATH)\n",
        "\n",
        "OUT_DIR = Path(\"/content/out/videos\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TMP_DIR = Path(\"/content/out/tmp\"); TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _font(pt: int) -> ImageFont.FreeTypeFont:\n",
        "    if DEFAULT_FONT_EXISTS:\n",
        "        return ImageFont.truetype(DEFAULT_FONT_PATH, pt)\n",
        "    # fallback: bitmap (not ideal, but prevents crashes)\n",
        "    return ImageFont.load_default()\n",
        "\n",
        "def _text_wrap(draw: ImageDraw.ImageDraw, text: str, font: ImageFont.FreeTypeFont, max_w: int):\n",
        "    words = text.split()\n",
        "    lines, cur = [], \"\"\n",
        "    for w in words:\n",
        "        t = (cur + \" \" + w).strip()\n",
        "        bbox = draw.textbbox((0,0), t, font=font)\n",
        "        if bbox[2]-bbox[0] <= max_w or not cur:\n",
        "            cur = t\n",
        "        else:\n",
        "            lines.append(cur); cur = w\n",
        "    if cur: lines.append(cur)\n",
        "    # balance widow\n",
        "    if len(lines) >= 2 and len(lines[-1].split()) <= 2 and len(lines[-2].split()) > 2:\n",
        "        prev = lines[-2].split()\n",
        "        moved = prev.pop()\n",
        "        lines[-2] = \" \".join(prev)\n",
        "        lines[-1] = (moved + \" \" + lines[-1]).strip()\n",
        "    return lines\n",
        "\n",
        "\n",
        "def _draw_text_block(img, box, text, color, max_pt, min_pt, leading_ratio=0.30, stroke=0, stroke_color=(255,255,255), align=\"center\"):\n",
        "    x, y, w, h = box\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for pt in range(max_pt, min_pt-1, -2):\n",
        "        f = _font(pt)\n",
        "        lines = _text_wrap(draw, text, f, w)\n",
        "        line_heights = [draw.textbbox((0,0), ln, font=f)[3] - draw.textbbox((0,0), ln, font=f)[1] for ln in lines]\n",
        "        total_h = sum(line_heights) + int(pt * leading_ratio) * (len(lines)-1)\n",
        "        if total_h <= h:\n",
        "            cur_y = y + (h - total_h)//2\n",
        "            for ln in lines:\n",
        "                bbox = draw.textbbox((0,0), ln, font=f)\n",
        "                tw = bbox[2]-bbox[0]; th = bbox[3]-bbox[1]\n",
        "                if align == \"center\":\n",
        "                    tx = x + (w - tw)//2\n",
        "                elif align == \"left\":\n",
        "                    tx = x\n",
        "                else:\n",
        "                    tx = x + (w - tw)\n",
        "                if stroke > 0:\n",
        "                    draw.text((tx, cur_y), ln, font=f, fill=stroke_color, stroke_width=stroke, stroke_fill=stroke_color)\n",
        "                draw.text((tx, cur_y), ln, font=f, fill=color, stroke_width=stroke, stroke_fill=stroke_color)\n",
        "                cur_y += th + int(pt * leading_ratio)\n",
        "            return\n",
        "    # fallback single line\n",
        "    f = _font(min_pt)\n",
        "    bbox = draw.textbbox((0,0), text, font=f); tw = bbox[2]-bbox[0]; th = bbox[3]-bbox[1]\n",
        "    tx = x + (w - tw)//2; ty = y + (h - th)//2\n",
        "    draw.text((tx, ty), text, font=f, fill=color, stroke_width=stroke, stroke_fill=stroke_color)\n",
        "\n",
        "# ---------- Slide renderers ----------\n",
        "def _badge(img: Image.Image):\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.rectangle([0,0,W,8], fill=GREEN)\n",
        "    pill_w, pill_h = 300, 72\n",
        "    px, py = W - pill_w - 36, 36\n",
        "    draw.rounded_rectangle([px, py, px+pill_w, py+pill_h], radius=20, fill=MINT)\n",
        "    _draw_text_block(img, (px+22, py+14, pill_w-44, pill_h-28), \"MyAI101\", DARK, 56, 32, align=\"left\")\n",
        "\n",
        "def _chip(img: Image.Image, y_center: int, text: str):\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    chip_w = W - 2*SAFE_X\n",
        "    x0, y0 = (W - chip_w)//2, y_center - CHIP_H//2\n",
        "    draw.rounded_rectangle([x0, y0, x0+chip_w, y0+CHIP_H], radius=18, fill=CHIP_BG)\n",
        "    draw.rectangle([x0, y0, x0+chip_w, y0+4], fill=GREEN)\n",
        "    _draw_text_block(img, (x0+CHIP_TX_PAD//2, y0+(CHIP_H-CHIP_TX_H)//2, chip_w-CHIP_TX_PAD, CHIP_TX_H),\n",
        "                     text, DARK, max_pt=54, min_pt=30, align=\"left\")\n",
        "\n",
        "def _question_slide(question: str, options: List[str]) -> Image.Image:\n",
        "    img = Image.new(\"RGB\", (W, H), BG)\n",
        "    _badge(img)\n",
        "    title_box = (SAFE_X, SAFE_TOP, W-2*SAFE_X, TITLE_BOX_H)\n",
        "    _draw_text_block(img, title_box, question, DARK, 84, 34, stroke=2, stroke_color=(255,255,255))\n",
        "    title_bottom = SAFE_TOP + TITLE_BOX_H\n",
        "    band_top = max(title_bottom + GAP_BELOW_TITLE_MIN, SAFE_TOP + int(H*0.34))\n",
        "    band_bot = min(H - SAFE_BOT, int(H*0.86))\n",
        "    n = len(options)\n",
        "    if n > 0:\n",
        "        band_h = band_bot - band_top\n",
        "        preferred = n*CHIP_H + (n-1)*CHIP_GAP\n",
        "        if preferred <= band_h:\n",
        "            ys = [band_top + CHIP_H//2 + i*(CHIP_H+CHIP_GAP) for i in range(n)]\n",
        "        else:\n",
        "            gap = max(18, int((band_h - n*CHIP_H) / max(1, n-1)))\n",
        "            ys = [band_top + CHIP_H//2 + i*(CHIP_H + gap) for i in range(n)]\n",
        "        for i, (opt, yc) in enumerate(zip(options, ys)):\n",
        "            _chip(img, yc, f\"{chr(65+i)}. {opt}\")\n",
        "    return img\n",
        "\n",
        "def _reveal_slide(correct: str, explanation: str) -> Image.Image:\n",
        "    img = Image.new(\"RGB\", (W, H), BG)\n",
        "    _badge(img)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    card_w, card_h = W - 2*SAFE_X, 140\n",
        "    x0, y0 = SAFE_X, int(H*0.34)\n",
        "    draw.rounded_rectangle([x0, y0, x0+card_w, y0+card_h], radius=20, fill=MINT)\n",
        "    _draw_text_block(img, (x0+20, y0+16, card_w-40, card_h-32), f\"Answer: {correct}\", DARK, 80, 40)\n",
        "    _draw_text_block(img, (SAFE_X, int(H*0.54), W-2*SAFE_X, int(H*0.28)), explanation, MID, 62, 32)\n",
        "    return img\n",
        "\n",
        "def _cta_slide() -> Image.Image:\n",
        "    img = Image.new(\"RGB\", (W, H), BG)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    _badge(img)\n",
        "    _draw_text_block(img, (SAFE_X, int(H*0.36), W-2*SAFE_X, int(H*0.22)), \"MyAI101\", DARK, 160, 84)\n",
        "    _draw_text_block(img, (SAFE_X, int(H*0.54), W-2*SAFE_X, int(H*0.16)),\n",
        "                     \"Daily AI literacy in 60 seconds\", MID, 72, 38)\n",
        "    bx, by = (W-CTA_BUTTON_W)//2, int(H*0.74)\n",
        "    draw.rounded_rectangle([bx, by, bx+CTA_BUTTON_W, by+CTA_BUTTON_H], radius=22, fill=GREEN)\n",
        "    _draw_text_block(img, (bx+18, by+10, CTA_BUTTON_W-36, CTA_BUTTON_H-20), \"Start Learning for Free\", (255,255,255), 54, 28)\n",
        "    return img\n",
        "\n",
        "# ---------- Local fallback generator ----------\n",
        "@dataclass\n",
        "class QuizItem:\n",
        "    topic: str\n",
        "    question: str\n",
        "    options: List[str]\n",
        "    answer_index: int\n",
        "    explanation: str\n",
        "\n",
        "EASY_TEMPLATES = [\n",
        "    \"Which of these is an example of {topic}?\",\n",
        "    \"What is {topic} mainly used for?\",\n",
        "    \"Which choice best matches {topic}?\",\n",
        "]\n",
        "MEDIUM_TEMPLATES = [\n",
        "    \"Which scenario best illustrates {topic} in practice?\",\n",
        "    \"Which statement about {topic} is correct?\",\n",
        "    \"What is a common use case of {topic}?\",\n",
        "]\n",
        "HARD_TEMPLATES = [\n",
        "    \"Which of the following is most accurate regarding {topic}?\",\n",
        "    \"In applied settings, which describes {topic} most precisely?\",\n",
        "    \"Which statement about {topic} reflects best practice?\",\n",
        "]\n",
        "EASY_DISTRACTORS = [\"Something unrelated\",\"A wrong idea\",\"Not quite right\",\"Another choice\",\"Sounds similar but isn't\"]\n",
        "MEDIUM_DISTRACTORS = [\"A partially correct statement\",\"A common misconception\",\"An unrelated technique\",\"A vague description\"]\n",
        "HARD_DISTRACTORS = [\"A subtle misconception\",\"A related but incorrect method\",\"An imprecise definition\",\"A misleading best practice\"]\n",
        "\n",
        "def _pick_template(difficulty: int) -> str:\n",
        "    return (random.choice(EASY_TEMPLATES) if difficulty<=3\n",
        "            else random.choice(MEDIUM_TEMPLATES) if difficulty<=7\n",
        "            else random.choice(HARD_TEMPLATES))\n",
        "\n",
        "def _generate_options_local(topic: str, difficulty: int) -> Tuple[List[str], int, str]:\n",
        "    topic_clean = topic.strip().rstrip(\"?.!\")\n",
        "    if difficulty <= 3:\n",
        "        correct, pool = f\"A simple example of {topic_clean}\", EASY_DISTRACTORS\n",
        "    elif difficulty <= 7:\n",
        "        correct, pool = f\"A practical use case of {topic_clean}\", MEDIUM_DISTRACTORS\n",
        "    else:\n",
        "        correct, pool = f\"A precise description of {topic_clean}\", HARD_DISTRACTORS\n",
        "    distractors = random.sample(pool, k=3)\n",
        "    options = distractors + [correct]\n",
        "    random.shuffle(options)\n",
        "    answer_index = options.index(correct)\n",
        "    explanation = (f\"The correct option describes {topic_clean} more appropriately than the others.\"\n",
        "                   if difficulty >= 4 else f\"It's the best match for {topic_clean}.\")\n",
        "    return options, answer_index, explanation\n",
        "\n",
        "def make_quiz_item_local(topic: str, difficulty: int) -> QuizItem:\n",
        "    q = _pick_template(difficulty).format(topic=topic)\n",
        "    options, answer_index, explanation = _generate_options_local(topic, difficulty)\n",
        "    return QuizItem(topic=topic, question=q, options=options, answer_index=answer_index, explanation=explanation)\n",
        "\n",
        "# ---------- Gemini prompt ----------\n",
        "DIFFICULTY_GUIDE = \"\"\"\n",
        "Map difficulty 1â€“10 to these constraints:\n",
        "1â€“2: kid-simple; one sentence; no jargon; obvious distractors.\n",
        "3â€“4: basic recognition; short phrasing; simple plausible distractors.\n",
        "5â€“6: intermediate conceptual; 1â€“2 sentences; plausible/related distractors.\n",
        "7â€“8: advanced application or edge cases; 2â€“3 sentences; subtle distractors.\n",
        "9â€“10: professional nuance; 2â€“3 concise sentences; highly plausible distractors with subtle traps.\n",
        "\"\"\"\n",
        "\n",
        "GEMINI_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert quiz item writer. Produce {count} distinct multiple-choice questions on the topic below.\n",
        "Strictly follow the schema and return ONLY valid JSON (no markdown, no explanations outside the JSON).\n",
        "\n",
        "Topic: \"{topic}\"\n",
        "Difficulty (1-10): {difficulty}\n",
        "\n",
        "{difficulty_guide}\n",
        "\n",
        "Constraints:\n",
        "- Each question must be self-contained.\n",
        "- Avoid ambiguous wording; avoid \"All of the above\" / \"None of the above\".\n",
        "- Vary the angles: definition, example, scenario, misconception, comparison, application, edge case (as appropriate).\n",
        "- Provide exactly 3 distractors for each question.\n",
        "- Include a short rationale (<= 240 characters) explaining why the correct answer is correct.\n",
        "\n",
        "Output schema (JSON only):\n",
        "{{\n",
        "  \"topic\": \"string\",\n",
        "  \"difficulty\": {difficulty},\n",
        "  \"questions\": [\n",
        "    {{\n",
        "      \"question\": \"string\",\n",
        "      \"correct_answer\": \"string\",\n",
        "      \"distractors\": [\"string\",\"string\",\"string\"],\n",
        "      \"rationale\": \"string\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "def _strip_code_fences(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    if s.startswith(\"```\"):\n",
        "        s = re.sub(r\"^```(?:json)?\", \"\", s.strip(), flags=re.IGNORECASE).strip()\n",
        "        s = re.sub(r\"```$\", \"\", s.strip()).strip()\n",
        "    return s\n",
        "\n",
        "def _ensure_3_distractors(distractors: List[str], correct: str) -> List[str]:\n",
        "    seen, out = set(), []\n",
        "    for d in distractors:\n",
        "        d = (d or \"\").strip()\n",
        "        if not d: continue\n",
        "        if d.lower() == (correct or \"\").strip().lower(): continue\n",
        "        if d.lower() in seen: continue\n",
        "        out.append(d); seen.add(d.lower())\n",
        "    while len(out) < 3:\n",
        "        out.append(f\"Alternative {len(out)+1}\")\n",
        "    return out[:3]\n",
        "\n",
        "def _shuffle_with_correct(correct: str, distractors: List[str]) -> Tuple[List[str], int]:\n",
        "    opts = distractors[:3] + [correct]\n",
        "    random.shuffle(opts)\n",
        "    idx = opts.index(correct)\n",
        "    return opts, idx\n",
        "\n",
        "# ---------- Gemini REST helpers ----------\n",
        "API_BASE = \"https://generativelanguage.googleapis.com/v1\"\n",
        "\n",
        "def list_models_v1(api_key: str) -> list:\n",
        "    \"\"\"Return raw models list from v1.\"\"\"\n",
        "    r = requests.get(f\"{API_BASE}/models\", params={\"key\": api_key}, timeout=30)\n",
        "    if r.status_code >= 400:\n",
        "        raise RuntimeError(f\"REST {r.status_code}: {r.text[:200]}\")\n",
        "    return r.json().get(\"models\", []) or []\n",
        "\n",
        "def filter_generate_content_models(models: list) -> list:\n",
        "    \"\"\"Filter models that support generateContent, return trailing ids (no 'models/').\"\"\"\n",
        "    out = []\n",
        "    for m in models:\n",
        "        methods = (\n",
        "            m.get(\"supportedGenerationMethods\")\n",
        "            or m.get(\"supported_generation_methods\")\n",
        "            or []\n",
        "        )\n",
        "        if \"generateContent\" in methods:\n",
        "            name = m.get(\"name\", \"\")\n",
        "            if name:\n",
        "                out.append(name.split(\"/\")[-1])\n",
        "    return out\n",
        "\n",
        "def gemini_generate_v1(api_key: str, model: str, prompt: str,\n",
        "                       temperature: float, max_output_tokens: int) -> str:\n",
        "    \"\"\"POST /v1/models/{model}:generateContent and return first candidate text.\"\"\"\n",
        "    url = f\"{API_BASE}/models/{model}:generateContent\"\n",
        "    params = {\"key\": api_key}\n",
        "    payload = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"parts\": [{\"text\": prompt}],\n",
        "            }\n",
        "        ],\n",
        "        \"generationConfig\": {\n",
        "            \"temperature\": float(temperature),\n",
        "            \"topP\": 0.9,\n",
        "            \"topK\": 40,\n",
        "            \"maxOutputTokens\": int(max_output_tokens),\n",
        "            \"candidateCount\": 1,\n",
        "        },\n",
        "    }\n",
        "    r = requests.post(url, params=params, json=payload, timeout=60)\n",
        "    if r.status_code >= 400:\n",
        "        raise RuntimeError(f\"REST {r.status_code}: {r.text[:200]}\")\n",
        "    data = r.json()\n",
        "    try:\n",
        "        return data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Unexpected REST response shape: {data}\") from e\n",
        "\n",
        "def _candidate_models_ordered(requested: str, available: list) -> list:\n",
        "    \"\"\"Return an ordered list of model ids to try, prioritizing user's choice if present.\"\"\"\n",
        "    req = (requested or \"\").strip()\n",
        "    if req.endswith(\"-latest\"):\n",
        "        req = req[:-7] + \"-001\"\n",
        "    order = []\n",
        "    if req:\n",
        "        order.append(req)\n",
        "    for m in [\"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-2.0-flash-exp\", \"gemini-pro\", \"gemini-1.0-pro\"]:\n",
        "        if m not in order:\n",
        "            order.append(m)\n",
        "    avail = set(available)\n",
        "    return [m for m in order if m in avail]\n",
        "\n",
        "# ---------- AI question generator (REST) ----------\n",
        "def generate_questions_gemini(api_key: str, model_name: str, topic: str, difficulty: int, count: int,\n",
        "                              temperature: float = 0.6, max_output_tokens: int = 1200):\n",
        "    topic = (topic or \"\").strip()\n",
        "    difficulty = int(max(1, min(10, difficulty)))\n",
        "    count = int(max(1, min(10, count)))\n",
        "    items: List[QuizItem] = []\n",
        "    info_msgs: List[str] = []\n",
        "\n",
        "    if not api_key:\n",
        "        info_msgs.append(\"No Gemini API key provided â€” using local generator.\")\n",
        "        for _ in range(count):\n",
        "            items.append(make_quiz_item_local(topic, difficulty))\n",
        "        return items, \" \".join(info_msgs)\n",
        "\n",
        "    # Get models your key truly has, and filter by generateContent\n",
        "    try:\n",
        "        models_raw = list_models_v1(api_key)\n",
        "        available = filter_generate_content_models(models_raw)\n",
        "    except Exception as e:\n",
        "        available = []\n",
        "        info_msgs.append(f\"Model listing failed ({e}); attempting common defaults.\")\n",
        "\n",
        "    # Decide which to try\n",
        "    to_try = _candidate_models_ordered(model_name, available if available else [\n",
        "        \"gemini-1.5-flash-001\",\n",
        "        \"gemini-1.5-pro-001\",\n",
        "        \"gemini-pro\",\n",
        "        \"gemini-1.0-pro\",\n",
        "    ])\n",
        "    if not to_try:\n",
        "        to_try = [\"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-pro\", \"gemini-1.0-pro\"]\n",
        "\n",
        "    prompt = GEMINI_PROMPT_TEMPLATE.format(\n",
        "        count=count, topic=topic, difficulty=difficulty, difficulty_guide=DIFFICULTY_GUIDE\n",
        "    )\n",
        "\n",
        "    last_err = None\n",
        "    for m in to_try:\n",
        "        try:\n",
        "            text = gemini_generate_v1(api_key, m, prompt, temperature, max_output_tokens)\n",
        "            raw = _strip_code_fences(text)\n",
        "            data = json.loads(raw)\n",
        "            qlist = data.get(\"questions\", [])\n",
        "            if not isinstance(qlist, list) or not qlist:\n",
        "                raise ValueError(\"Gemini returned no questions.\")\n",
        "\n",
        "            if m != model_name:\n",
        "                info_msgs.append(f\"Using available model: {m}\")\n",
        "\n",
        "            for q in qlist[:count]:\n",
        "                qtext = (q.get(\"question\") or \"\").strip()\n",
        "                correct = (q.get(\"correct_answer\") or \"\").strip()\n",
        "                distractors = q.get(\"distractors\") or []\n",
        "                rationale = (q.get(\"rationale\") or \"\").strip()\n",
        "                if not qtext or not correct or not isinstance(distractors, list):\n",
        "                    raise ValueError(\"Missing fields in a question item.\")\n",
        "                distractors = _ensure_3_distractors(distractors, correct)\n",
        "                options, answer_index = _shuffle_with_correct(correct, distractors)\n",
        "\n",
        "                def _clip(s, n):\n",
        "                    s = (s or \"\").strip()\n",
        "                    return s if len(s) <= n else s[:n-1] + \"â€¦\"\n",
        "                qtext = _clip(qtext, 260)\n",
        "                options = [_clip(o, 140) for o in options]\n",
        "                rationale = _clip(rationale, 260)\n",
        "\n",
        "                items.append(QuizItem(\n",
        "                    topic=topic,\n",
        "                    question=qtext,\n",
        "                    options=options,\n",
        "                    answer_index=answer_index,\n",
        "                    explanation=rationale\n",
        "                ))\n",
        "\n",
        "            if len(items) < count:\n",
        "                missing = count - len(items)\n",
        "                info_msgs.append(f\"Gemini returned {len(items)} items; backfilled {missing} locally.\")\n",
        "                for _ in range(missing):\n",
        "                    items.append(make_quiz_item_local(topic, difficulty))\n",
        "\n",
        "            return items, (\" \".join(info_msgs) if info_msgs else f\"Generated {len(items)} item(s) via Gemini.\")\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            continue\n",
        "\n",
        "    info_msgs.append(f\"Gemini error: {last_err}. Falling back to local generator.\")\n",
        "    items = [make_quiz_item_local(topic, difficulty) for _ in range(count)]\n",
        "    return items, \" \".join(info_msgs)\n",
        "\n",
        "\n",
        "# ---------- FFmpeg assembly ----------\n",
        "def _save_png(img: Image.Image, path: Path):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    img.save(str(path), format=\"PNG\", optimize=True)\n",
        "\n",
        "def _ffmpeg_build(out_path: Path, question_png: Path, reveal_png: Path, cta_png: Path,\n",
        "                  q_sec=6.0, r_sec=4.3, c_sec=2.9):\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\"-y\",\n",
        "        \"-loglevel\",\"error\",\n",
        "        \"-threads\",\"1\",\n",
        "        \"-loop\",\"1\",\"-t\",f\"{q_sec}\",\"-i\",str(question_png),\n",
        "        \"-loop\",\"1\",\"-t\",f\"{r_sec}\",\"-i\",str(reveal_png),\n",
        "        \"-loop\",\"1\",\"-t\",f\"{c_sec}\",\"-i\",str(cta_png),\n",
        "        \"-filter_complex\",\n",
        "        (\n",
        "          f\"[0:v]scale={W}:{H},setsar=1[v0];\"\n",
        "          f\"[1:v]scale={W}:{H},setsar=1[v1];\"\n",
        "          f\"[2:v]scale={W}:{H},setsar=1[v2];\"\n",
        "          f\"[v0][v1][v2]concat=n=3:v=1:a=0,format=yuv420p[v]\"\n",
        "        ),\n",
        "        \"-map\",\"[v]\",\"-r\", str(FPS),\n",
        "        \"-c:v\",\"libx264\",\"-preset\",\"veryfast\",\n",
        "        \"-b:v\", BITRATE,\"-maxrate\", BITRATE,\"-bufsize\", BITRATE,\n",
        "        \"-movflags\",\"+faststart\",\"-g\", str(FPS*2),\n",
        "        str(out_path)\n",
        "    ]\n",
        "    subprocess.check_call(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "\n",
        "def build_quiz_video(item: QuizItem, out_path: Path) -> Path:\n",
        "    q_img = _question_slide(item.question, item.options[:4])\n",
        "    r_img = _reveal_slide(item.options[item.answer_index], item.explanation)\n",
        "    c_img = _cta_slide()\n",
        "    base = TMP_DIR / f\"vid_{int(time.time()*1000)}_{random.randint(1000,9999)}\"\n",
        "    q_png, r_png, c_png = base.with_suffix(\".q.png\"), base.with_suffix(\".r.png\"), base.with_suffix(\".c.png\")\n",
        "    _save_png(q_img, q_png); _save_png(r_img, r_png); _save_png(c_img, c_png)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    _ffmpeg_build(out_path, q_png, r_png, c_png)\n",
        "    for p in [q_png, r_png, c_png]:\n",
        "        try: p.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "    return out_path\n",
        "\n",
        "def _slugify(text: str) -> str:\n",
        "    s = text.lower().strip().replace(\" \", \"-\")\n",
        "    allowed = set(string.ascii_lowercase + string.digits + \"-\")\n",
        "    s = \"\".join(ch for ch in s if ch in allowed)\n",
        "    return s or f\"item-{int(time.time())}\"\n",
        "\n",
        "# ---------- Gradio callbacks ----------\n",
        "def _seed_rng(): random.seed(time.time_ns() % (2**32 - 1))\n",
        "\n",
        "def generate_preview_ai(topic: str, difficulty: int, count: int, api_key: str, model_name: str,\n",
        "                        temperature: float = 0.6, max_output_tokens: int = 1200):\n",
        "    topic = (topic or \"\").strip()\n",
        "    if not topic:\n",
        "        return pd.DataFrame([]), \"Please enter a topic.\", [], \"\"\n",
        "    difficulty = int(max(1, min(10, difficulty)))\n",
        "    count = int(max(1, min(10, count)))\n",
        "    _seed_rng()\n",
        "\n",
        "    items, info_msg = generate_questions_gemini(api_key, model_name, topic, difficulty, count,\n",
        "                                                temperature=temperature, max_output_tokens=max_output_tokens)\n",
        "\n",
        "    rows = []\n",
        "    for i, it in enumerate(items, 1):\n",
        "        rows.append({\n",
        "            \"#\": i, \"Question\": it.question,\n",
        "            \"A\": it.options[0], \"B\": it.options[1], \"C\": it.options[2], \"D\": it.options[3],\n",
        "            \"Correct\": [\"A\",\"B\",\"C\",\"D\"][it.answer_index],\n",
        "            \"Explanation\": it.explanation,\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    status = info_msg or f\"Generated {len(rows)} quiz item(s). Review below.\"\n",
        "    return df, status, rows, topic\n",
        "\n",
        "def confirm_and_produce(items_table, topic: str):\n",
        "    if items_table is None:\n",
        "        return \"No items to render. Please generate a preview first.\", []\n",
        "    if hasattr(items_table, \"to_dict\"):\n",
        "        rows = items_table.to_dict(orient=\"records\")\n",
        "    elif isinstance(items_table, list):\n",
        "        rows = items_table\n",
        "    else:\n",
        "        rows = []\n",
        "    if not rows:\n",
        "        return \"No items to render. Please generate a preview first.\", []\n",
        "\n",
        "    saved_paths = []\n",
        "    for idx, raw in enumerate(rows, 1):\n",
        "        options = [raw.get(\"A\",\"\"), raw.get(\"B\",\"\"), raw.get(\"C\",\"\"), raw.get(\"D\",\"\")]\n",
        "        correct_letter = str(raw.get(\"Correct\",\"A\")).strip().upper()\n",
        "        answer_index = {\"A\":0,\"B\":1,\"C\":2,\"D\":3}.get(correct_letter, 0)\n",
        "        qi = QuizItem(topic=topic or \"\", question=raw.get(\"Question\",\"\").strip(),\n",
        "                      options=options, answer_index=answer_index,\n",
        "                      explanation=(raw.get(\"Explanation\",\"\") or \"\").strip())\n",
        "        slug = _slugify(f\"{topic}-{idx}\")\n",
        "        out_path = OUT_DIR / f\"myai101_{slug}.mp4\"\n",
        "        try:\n",
        "            build_quiz_video(qi, out_path)\n",
        "            saved_paths.append(str(out_path))\n",
        "        except Exception as e:\n",
        "            saved_paths.append(f\"ERROR: {e}\")\n",
        "        gc.collect()\n",
        "\n",
        "    msg = f\"Done. Produced {len(saved_paths)} video(s).\"\n",
        "    files = [p for p in saved_paths if Path(p).suffix.lower()==\".mp4\" and Path(p).exists()]\n",
        "    return msg, files\n",
        "\n",
        "_empty_df = pd.DataFrame(columns=[\"#\", \"Question\", \"A\", \"B\", \"C\", \"D\", \"Correct\", \"Explanation\"])\n",
        "def _make_table():\n",
        "    try:\n",
        "        return gr.Dataframe(\n",
        "            headers=list(_empty_df.columns),\n",
        "            datatype=[\"number\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\",\"str\"],\n",
        "            row_count=(1, \"dynamic\"),\n",
        "            col_count=(8, \"fixed\"),\n",
        "            wrap=True,\n",
        "            label=\"You can edit cells before confirming to tweak wording.\",\n",
        "            interactive=True,\n",
        "        )\n",
        "    except TypeError:\n",
        "        return gr.Dataframe(\n",
        "            value=_empty_df,\n",
        "            headers=list(_empty_df.columns),\n",
        "            wrap=True,\n",
        "            label=\"You can edit cells before confirming to tweak wording.\",\n",
        "            interactive=True,\n",
        "        )\n",
        "\n",
        "# ---------- UI ----------\n",
        "with gr.Blocks(title=\"MyAI101 â€” Quiz Video Maker (Gemini REST, No MoviePy, 1080x1920)\") as demo:\n",
        "    gr.Markdown(\"# MyAI101 â€” Quiz Video Maker (Gemini, **REST**, no MoviePy, 1080Ã—1920)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        topic_inp = gr.Textbox(label=\"Topic\", placeholder=\"e.g. Backpropagation, SSL certificates, Photosynthesis\")\n",
        "    with gr.Row():\n",
        "        diff_inp  = gr.Slider(1, 10, value=5, step=1, label=\"Difficulty (1 = child <10, 10 = professional)\")\n",
        "        count_inp = gr.Slider(1, 10, value=3, step=1, label=\"How many questions / videos to create\")\n",
        "\n",
        "    gr.Markdown(\"### AI Generation Settings (Gemini)\")\n",
        "    with gr.Row():\n",
        "        api_key_inp = gr.Textbox(label=\"Gemini API Key\", placeholder=\"Paste your Google AI Studio API key\", type=\"password\")\n",
        "        model_inp   = gr.Dropdown(choices=[\"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\"], value=\"gemini-1.5-flash-001\", label=\"Model\")\n",
        "        list_btn    = gr.Button(\"ðŸ”Ž List Available Models\")\n",
        "\n",
        "    with gr.Row():\n",
        "        temp_inp    = gr.Slider(0.0, 1.0, value=0.6, step=0.1, label=\"Temperature (creativity)\")\n",
        "        max_tok_inp = gr.Slider(200, 2000, value=1200, step=100, label=\"Max output tokens\")\n",
        "\n",
        "    with gr.Row():\n",
        "        preview_btn = gr.Button(\"ðŸ§  Generate Preview (AI)\")\n",
        "        regen_btn   = gr.Button(\"â†» Regenerate\")\n",
        "\n",
        "    gr.Markdown(\"### Preview: Questions & Answers\")\n",
        "    preview_state = gr.State([])   # Python list (for safety across Gradio versions)\n",
        "    topic_state   = gr.State(\"\")   # store topic used for preview\n",
        "\n",
        "    table  = _make_table()\n",
        "    status = gr.Markdown(visible=True)\n",
        "\n",
        "    with gr.Row():\n",
        "        confirm_btn = gr.Button(\"âœ… Confirm & Produce Videos\", variant=\"primary\")\n",
        "    out_msg   = gr.Markdown()\n",
        "    out_files = gr.Files(label=\"Rendered MP4s\")\n",
        "\n",
        "    # Callbacks\n",
        "    def _on_list_models(api_key):\n",
        "        if not api_key:\n",
        "            return gr.update(), \"Enter API key first.\"\n",
        "        try:\n",
        "            mods = list_models_v1(api_key)\n",
        "            choices = filter_generate_content_models(mods)\n",
        "            if not choices:\n",
        "                return gr.update(choices=[], value=None), \"No generateContent-capable models found for this key.\"\n",
        "            # Prefer flash if present\n",
        "            default = \"gemini-1.5-flash-001\" if \"gemini-1.5-flash-001\" in choices else choices[0]\n",
        "            return gr.update(choices=choices, value=default), f\"Found {len(choices)} model(s).\"\n",
        "        except Exception as e:\n",
        "            return gr.update(), f\"Listing failed: {e}\"\n",
        "\n",
        "    def _on_preview(topic, diff, count, api_key, model_name, temperature, max_tokens):\n",
        "        df, msg, rows, used_topic = generate_preview_ai(topic, int(diff), int(count), api_key, model_name, float(temperature), int(max_tokens))\n",
        "        if not hasattr(df, \"to_dict\"):\n",
        "            df = pd.DataFrame(df)\n",
        "        return df, msg, rows, used_topic\n",
        "\n",
        "    def _on_confirm(current_table, topic_used):\n",
        "        return confirm_and_produce(current_table, topic_used)\n",
        "\n",
        "    list_btn.click(_on_list_models, inputs=[api_key_inp], outputs=[model_inp, status])\n",
        "    preview_btn.click(_on_preview, inputs=[topic_inp, diff_inp, count_inp, api_key_inp, model_inp, temp_inp, max_tok_inp], outputs=[table, status, preview_state, topic_state])\n",
        "    regen_btn.click(_on_preview,   inputs=[topic_inp, diff_inp, count_inp, api_key_inp, model_inp, temp_inp, max_tok_inp], outputs=[table, status, preview_state, topic_state])\n",
        "    confirm_btn.click(_on_confirm, inputs=[table, topic_state], outputs=[out_msg, out_files])\n",
        "\n",
        "# --- Launch (print URLs) ---\n",
        "gr.close_all(); gc.collect()\n",
        "res = demo.launch(share=True, inbrowser=False, inline=False, show_error=True, debug=True, prevent_thread_lock=True)\n",
        "try:\n",
        "    print(\"Local URL:\", getattr(res, \"local_url\", None) or res.local_url)\n",
        "    print(\"Public URL:\", getattr(res, \"share_url\", None) or res.share_url)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    }
  ]
}